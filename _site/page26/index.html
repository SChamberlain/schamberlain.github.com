<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://localhost:4000">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2013/05/rbison/">
        BISON USGS species occurrence data
      </a>
    </h1>

    <span class="post-date">27 May 2013</span>

    The USGS recently released a way to search for and get species occurrence records for the USA. The service is called [BISON](http://bison.usgs.ornl.gov/) (Biodiversity Information Serving Our Nation). The service has [a web interface](http://bison.usgs.ornl.gov/) for human interaction in a browser, and [two APIs](http://bison.usgs.ornl.gov/services.html) (application programming interface) to allow machines to interact with their database. One of the APIs allows you to search and retrieve data, and the other gives back maps as either a heatmap or a species occurrence map. The latter is more appropriate for working in a browser, so I'll leave that to the web app folks. 

The Core Science Analytics and Synthesis (CSAS) program of the US Geological Survey are responsible for BISON, and are the US node of the Global Biodiversity Information Facility (GBIF). BISON data is nested within that of GBIF, but has (or wil have?) additional data not in GBIF, as described on their *About* page:

> BISON has been initiated with the 110 million records GBIF makes available from the U.S. and is integrating millions more records from other sources each year

Have a look at their *Data providers* and *Statistics* tabs on the BISON website, which list where data comes from and how many searches and downloads have been done on each data provider.

We (rOpenSci) started an R package to interact with the BISON search API >> `rbison`. You may be thinking, but if the data in BISON is also in GBIF, why both making another R package for BISON? Good question. As I just said, BISON will have some data GBIF won't have. Also, the services (search API and map service) are different than those of GBIF. 

Check out the package on GitHub here [https://github.com/ropensci/rbison](https://github.com/ropensci/rbison). 

Here is a quick run through of some things you can do with `rbison`. 

***************

### Install ribson

{% highlight r %}
# Install rbison from GitHub using devtools, uncomment to install
# install.packages('devtools') library(devtools) install_github('rbison',
# 'ropensci')
library(rbison)
{% endhighlight %}


***************

### Search the BISON database for, of course, bison 
	

{% highlight r %}
# Do the search
out <- bison(species = "Bison bison", type = "scientific_name", start = 0, count = 10)

# Check that the returned object is the right class ('bison')
class(out)
{% endhighlight %}



{% highlight text %}
[1] "bison"
{% endhighlight %}


#### Get a summary of the data


{% highlight r %}
bison_data(out)
{% endhighlight %}



{% highlight text %}
  total observation fossil specimen unknown
1   761          30      4      709      18
{% endhighlight %}


#### Summary by counties (just the first 6 rows)


{% highlight r %}
head(bison_data(input = out, datatype = "counties"))
{% endhighlight %}



{% highlight text %}
  record_id total county_name      state
1     48295     7    Lipscomb      Texas
2     41025    15      Harney     Oregon
3     49017     8    Garfield       Utah
4     35031     2    McKinley New Mexico
5     56013     1     Fremont    Wyoming
6     40045     2       Ellis   Oklahoma
{% endhighlight %}


#### Summary of states


{% highlight r %}
bison_data(input = out, datatype = "states")
{% endhighlight %}



{% highlight text %}
      record_id total county_fips
1    Washington     1          53
2         Texas     8          48
3    New Mexico     8          35
4          Iowa     1          19
5       Montana     9          30
6       Wyoming   155          56
7        Oregon    15          41
8      Oklahoma    14          40
9        Kansas    10          20
10      Arizona     1          04
11       Alaska    29          02
12         Utah    16          49
13     Colorado    17          08
14     Nebraska     1          31
15 South Dakota    61          46
{% endhighlight %}


***************

### Map the results


{% highlight r %}
# Search for Ursus americanus (american black bear)
out <- bison(species = "Ursus americanus", type = "scientific_name", start = 0, 
    count = 200)

# Sweet, got some data
bison_data(out)
{% endhighlight %}



{% highlight text %}
  total observation fossil specimen literature unknown centroid
1  3792          59    125     3522         47      39       78
{% endhighlight %}


### Make some maps! Note that right now the county and state maps just plot the conterminous lower 48. The map of individual occurrences shows the lower 48 + Alaska


{% highlight r %}
# By county
bisonmap(out, tomap = "county")
{% endhighlight %}

![center](/public/img/2013-05-25-rbison/map11.png) 

{% highlight r %}

# By state
bisonmap(out, tomap = "state")
{% endhighlight %}

![center](/public/img/2013-05-25-rbison/map12.png) 

{% highlight r %}

# Individual locations
bisonmap(out)
{% endhighlight %}



{% highlight text %}
## Rendering map...plotting 199 points
{% endhighlight %}

![center](/public/img/2013-05-25-rbison/map13.png) 


*********
	
### When plotting occurrences, you can pass additional arguments into the `bisonmap` function.

#### For example, you can jitter the points


{% highlight r %}
bisonmap(input = out, geom = geom_jitter)
{% endhighlight %}



{% highlight text %}
## Rendering map...plotting 199 points
{% endhighlight %}

![center](/public/img/2013-05-25-rbison/map2.png) 


#### And you can specify by how much you want the points to jitter (here an extreme example to make it obvious)


{% highlight r %}
library(ggplot2)
bisonmap(input = out, geom = geom_jitter, jitter = position_jitter(width = 5))
{% endhighlight %}



{% highlight text %}
## Rendering map...plotting 199 points
{% endhighlight %}

![center](/public/img/2013-05-25-rbison/map3.png) 


*********

#### Let us know if you have any feature requests or find bugs at [our GitHub Issues page](https://github.com/ropensci/rbison/issues).

  </div>
  
  <div class="post">
    <h1>
      <a href="/2013/03/r-metadata/">
        Scholarly metadata in R
      </a>
    </h1>

    <span class="post-date">16 Mar 2013</span>

    Scholarly metadata - the meta-information surrounding articles - can be super useful.  Although metadata does not contain the full content of articles, it contains a lot of useful information, including title, authors, abstract, URL to the article, etc. 

One of the largest sources of metadata is provided via the Open Archives Initiative Protocol for Metadata Harvesting or [OAI-PMH](http://www.openarchives.org/OAI/openarchivesprotocol.html). Many publishers, provide their metadata through their own endpoint, and implement the standard OAI-PMH methods: [GetRecord](http://www.openarchives.org/OAI/openarchivesprotocol.html#GetRecord), [Identify](http://www.openarchives.org/OAI/openarchivesprotocol.html#Identify), [ListIdentifiers](http://www.openarchives.org/OAI/openarchivesprotocol.html#ListIdentifiers), [ListMetadataFormats](http://www.openarchives.org/OAI/openarchivesprotocol.html#ListMetadataFormats), [ListRecords](http://www.openarchives.org/OAI/openarchivesprotocol.html#ListRecords), and [ListSets](http://www.openarchives.org/OAI/openarchivesprotocol.html#ListSets). Many providers use OAI-PMH, including [DataCite](http://oai.datacite.org/), [Dryad](http://wiki.datadryad.org/Data_Access#OAI-PMH), and [PubMed](http://www.ncbi.nlm.nih.gov/pmc/tools/oai/).

Some data-/article-providers provide their metadata via their own APIs. For example, Nature Publishing Group provides their own metadata API [here](http://developers.nature.com/docs) in non OAI-PMH format; you can get PLoS metadata through their [search API](http://api.plos.org/), and the BHL (see below) provides their own custom metadata service.

In addition, CrossRef provides a number of metadata search services: [metadata search](http://search.labs.crossref.org/help/api) and [openurl](http://labs.crossref.org/openurl/). 

What about the other publishers? (please tell me if I'm wrong about these three) 

+ Springer has [a metadata API](http://dev.springer.com/docs), but it is terrible, soooo...
+ Elsevier, are you kidding? Well, they do have some sort of API service, but its a pain in the ass. 
+ Wiley, no better than Elsevier.

Note that metadata can live in other places:

+ Another package being developed by David Springate, [rpubmed](https://github.com/ropensci/rpubmed) can get PubMed metadata. 
+ Our wrapper to the Mendeley API, [RMendeley](https://github.com/ropensci/rmendeley), gets article metadata via Mendeley's database. 
+ Our wrapper to the Biodiversity Heritage Library API [here](http://www.biodiversitylibrary.org/api2/docs/docs.html) gets their metadata. 

No, you can't get metadata via Google Scholar - the don't allow scraping, and don't have expose their data via an API.

I have discussed this package [in a previous blog post](http://sckott.github.io/2012/09/rmetadata/), but have since worked on the code a bit, and thought it deserved a new post. 

You can see a tutorial for this package [here](http://ropensci.github.com/rmetadata/), and contribute to the code [here](https://github.com/ropensci/rmetadata).

***************

### Install rmetadata

{% highlight r %}
# install_github('rmetadata', 'ropensci') # uncomment to install
library(rmetadata)
{% endhighlight %}


***************

### Count OAI-PMH identifiers for a data provider.
	

{% highlight r %}
# For DataCite.
count_identifiers("datacite")

  provider   count
1 datacite 1216193
{% endhighlight %}


*********
	
### Lookup article info via CrossRef with DOI and get a citation.

#### As Bibtex
	

{% highlight r %}
print(crossref_citation("10.3998/3336451.0009.101"), style = "Bibtex")

@Article{,
  title = {In Google We Trust?},
  author = {Geoffrey Bilder},
  journal = {The Journal of Electronic Publishing},
  year = {2006},
  month = {01},
  volume = {9},
  doi = {10.3998/3336451.0009.101},
}
{% endhighlight %}

	
#### As regular text
	

{% highlight r %}
print(crossref_citation("10.3998/3336451.0009.101"), style = "text")

Bilder G (2006). "In Google We Trust?" _The Journal of Electronic
Publishing_, *9*. <URL:
http://dx.doi.org/10.3998/3336451.0009.101>.
{% endhighlight %}

	
*********
	
### Search the CrossRef Metatdata for DOIs using free form references.
	
#### Search with title, author, year, and journal
	

{% highlight r %}
crossref_search_free(query = "Piwowar Sharing Detailed Research Data Is Associated with Increased Citation Rate PLOS one 2007")

                                                                                             text
1 Piwowar Sharing Detailed Research Data Is Associated with Increased Citation Rate PLOS one 2007
  match                   doi score
1  TRUE 10.1038/npre.2007.361 4.905
{% endhighlight %}

	
#### Get a DOI and get the citation using \code{crossref_search}
	

{% highlight r %}
# Get a DOI for a paper
doi <- crossref_search_free(query = "Piwowar sharing data PLOS one")$doi

# Get the metadata
crossref_search(doi = doi)[, 1:3]

                           doi score normalizedScore
1 10.1371/journal.pone.0000308 18.19             100
{% endhighlight %}


*********
	
### Get a random set of DOI's through CrossRef.
	

{% highlight r %}
# Default search gets 20 random DOIs
crossref_r()

 [1] "10.4028/www.scientific.net/MSF.126-128.467"
 [2] "10.2139/ssrn.548523"                       
 [3] "10.1016/S0012-821X(02)00562-9"             
 [4] "10.1093/rsq/13.2-3.167"                    
 [5] "10.5772/55055"                             
 [6] "10.1515/BC.1999.050"                       
 [7] "10.1016/S0020-7292(98)90160-6"             
 [8] "10.1111/j.1439-0418.1985.tb02788.x"        
 [9] "10.1089/aid.2012.0115"                     
[10] "10.1016/0002-9378(95)90155-8"              
[11] "10.1001/jama.1949.02900490055028"          
[12] "10.1051/jphyscol:1989172"                  
[13] "10.1016/s0301-2115(03)00298-7"             
[14] "10.1007/BF02735292"                        
[15] "10.1016/0003-4916(65)90026-6"              
[16] "10.4156/jdcta.vol5.issue5.12"              
[17] "10.1007/s10904-009-9316-2"                 
[18] "10.1023/A:1021690001832"                   
[19] "10.1007/s12262-012-0724-0"                 
[20] "10.1007/bf02192860"
{% endhighlight %}



{% highlight r %}

# Specify you want journal articles only
crossref_r(type = "journal_article")

 [1] "10.1016/j.jacc.2011.09.055"                                 
 [2] "10.1002/dev.420170603"                                      
 [3] "10.4315/0362-028X.JFP-10-403"                               
 [4] "10.1016/S0925-4927(98)00016-X"                              
 [5] "10.1111/j.1933-1592.2002.tb00141.x"                         
 [6] "10.1541/ieejfms.127.629"                                    
 [7] "10.5539/enrr.v3n1p62"                                       
 [8] "10.1016/S0960-9776(96)90038-7"                              
 [9] "10.1016/0925-9635(94)05240-9"                               
[10] "10.1016/s0929-693x(97)86846-7"                              
[11] "10.1002/(SICI)1096-9071(199601)48:1<53::AID-JMV9>3.0.CO;2-K"
[12] "10.1016/s0267-7261(01)00016-1"                              
[13] "10.1111/j.1748-0361.2003.tb00575.x"                         
[14] "10.1097/00005721-197701000-00011"                           
[15] "10.1007/s00894-009-0593-z"                                  
[16] "10.1071/AR9830063"                                          
[17] "10.1186/gb-2009-10-4-r39"                                   
[18] "10.2165/00128415-201113540-00038"                           
[19] "10.1007/BF00522986"                                         
[20] "10.1080/19407963.2011.539385"
{% endhighlight %}

	
*********
	
### Search the CrossRef Metatdata API. 
	

{% highlight r %}
# Search for two different query terms
crossref_search(query = c("renear", "palmer"), rows = 4)[, 1:3]

                            doi score normalizedScore
1       10.1126/science.1157784 3.253             100
2  10.1002/meet.2009.1450460141 2.169              66
3 10.4242/BalisageVol3.Renear01 2.102              64
4 10.4242/BalisageVol5.Renear01 2.102              64
{% endhighlight %}



{% highlight r %}

# Get results for a certain year
crossref_search(query = c("renear", "palmer"), year = 2010)[, 1:3]

                                  doi  score normalizedScore
1            10.1002/meet.14504701218 1.0509             100
2            10.1002/meet.14504701240 1.0509             100
3           10.5270/OceanObs09.cwp.68 1.0442              99
4               10.1353/mpq.2010.0003 0.6890              65
5                  10.1353/mpq.0.0041 0.6890              65
6                  10.1353/mpq.0.0044 0.6890              65
7                  10.1353/mpq.0.0057 0.6890              65
8                    10.1386/fm.1.1.2 0.6890              65
9                    10.1386/fm.1.2.2 0.6890              65
10                   10.1386/fm.1.3.2 0.6890              65
11       10.1097/ALN.0b013e3181f09404 0.6090              57
12      10.1016/j.urology.2010.02.033 0.6090              57
13              10.1353/ect.2010.0025 0.6090              57
14               10.1117/2.4201001.04 0.6090              57
15 10.1111/j.1835-9310.1977.tb01159.x 0.6090              57
16    10.4067/S0717-69962010000100001 0.6090              57
17    10.4067/S0717-69962010000200001 0.6090              57
18           10.2105/AJPH.2009.191098 0.6029              57
19              10.1353/mpq.2010.0004 0.5167              49
20                 10.1353/mpq.0.0048 0.5167              49
{% endhighlight %}

	
*********
	
### Get a short DOI from shortdoi.org.
	

{% highlight r %}
# Geta a short DOI, just the short DOI returned
short_doi(doi = "10.1371/journal.pone.0042793")

[1] "10/f2bfz9"
{% endhighlight %}



{% highlight r %}

# Geta a short DOI, all data returned
short_doi(doi = "10.1371/journal.pone.0042793", justshort = FALSE)

$DOI
[1] "10.1371/journal.pone.0042793"

$ShortDOI
[1] "10/f2bfz9"

$IsNew
[1] FALSE
{% endhighlight %}

	
*********
	
### Get a record from a OAI-PMH data provider.
	

{% highlight r %}
# Single provider, one identifier
md_getrecord(provider = "pensoft", identifier = "10.3897/zookeys.1.10")

                                                                                                title
1 A new candidate for a Gondwanaland distribution in the Zodariidae (Araneae): Australutica in Africa
      creator date             type
1 Jocqué,Rudy 2008 Research Article
{% endhighlight %}



{% highlight r %}

# Single provider, multiple identifiers
md_getrecord(provider = "pensoft", identifier = c("10.3897/zookeys.1.10", "10.3897/zookeys.4.57"))

                                                                                                   title
1    A new candidate for a Gondwanaland distribution in the Zodariidae (Araneae): Australutica in Africa
2 Studies of Tiger Beetles. CLXXVIII. A new Lophyra (Lophyra) from Somaliland (Coleoptera, Cicindelidae)
        creator date             type
1   Jocqué,Rudy 2008 Research Article
2 Cassola,Fabio 2008 Research Article
{% endhighlight %}

	
*********

### List available metadata formats from various providers. 
	

{% highlight r %}
# List metadata formats for a provider
md_listmetadataformats(provider = "dryad")

  metadataPrefix
1         oai_dc
2            rdf
3            ore
4           mets
                                                       schema
1              http://www.openarchives.org/OAI/2.0/oai_dc.xsd
2                 http://www.openarchives.org/OAI/2.0/rdf.xsd
3 http://tweety.lanl.gov/public/schemas/2008-06/atom-tron.sch
4                  http://www.loc.gov/standards/mets/mets.xsd
                            metadataNamespace
1 http://www.openarchives.org/OAI/2.0/oai_dc/
2    http://www.openarchives.org/OAI/2.0/rdf/
3                 http://www.w3.org/2005/Atom
4                    http://www.loc.gov/METS/
{% endhighlight %}



{% highlight r %}

# List metadata formats for a specific identifier for a provider
md_listmetadataformats(provider = "pensoft", identifier = "10.3897/zookeys.1.10")

            identifier metadataPrefix
1 10.3897/zookeys.1.10         oai_dc
2 10.3897/zookeys.1.10           mods
                                             schema
1    http://www.openarchives.org/OAI/2.0/oai_dc.xsd
2 http://www.loc.gov/standards/mods/v3/mods-3-1.xsd
                            metadataNamespace
1 http://www.openarchives.org/OAI/2.0/oai_dc/
2                  http://www.loc.gov/mods/v3
{% endhighlight %}


*********

### Some plotting - mean number of authors per paper

Okay, so this isn't a super useful visualization, but you can surely think of something better. 


{% highlight r %}
library(ggplot2)
library(ggthemes)
library(reshape)


temp <- md_listrecords(provider = "pensoft", from = "2011-10-01", until = "2012-01-01")
temp2 <- ldply(temp)[, -1]
auths <- sapply(temp2$creator, function(x) length(strsplit(as.character(x), 
    ";")[[1]]))
toplot <- data.frame(authors = auths, articletype = temp2$type)
toplot_ <- ddply(toplot, .(articletype), summarise, authors = mean(authors))
toplot_$articletype <- reorder(toplot_$articletype, toplot_$authors)

ggplot(toplot_, aes(articletype, authors)) + theme_tufte(base_size = 16) + geom_bar(stat = "identity") + 
    coord_flip()
{% endhighlight %}

![center](/public/img/someplotting.png)

***************

#### Get the .Rmd file used to create this post [at my github account](https://github.com/sckott/sckott.github.com/tree/master/_drafts/2013-03-16-r-metadata.Rmd) - or [.md file](https://github.com/sckott/sckott.github.com/tree/master/_posts/2013-03-16-r-metadata.md).

#### Written in [Markdown](http://daringfireball.net/projects/markdown/), with help from [knitr](http://yihui.name/knitr/), and [knitcitations](https://github.com/cboettig/knitcitations).

  </div>
  
  <div class="post">
    <h1>
      <a href="/2013/03/ropensci-collaboration/">
        Visualizing rOpenSci collaboration
      </a>
    </h1>

    <span class="post-date">08 Mar 2013</span>

    We ([rOpenSci](http://ropensci.org/)) have been writing code for R packages for a couple years, so it is time to take a look back at the data. What data you ask? The commits data from GitHub ~ data that records who did what and when. 

Using the [Github commits API](http://developer.github.com/v3/repos/commits/) we can gather data on who commited code to a Github repository, and when they did it. Then we can visualize this hitorical record. 

***************

### Install some functions for interacting with the Github API via R

{% highlight r %}

install_github('sandbox', 'ropensci') 

library(sandbox)
library(httr)
library(ggplot2)
library(scales)
library(reshape2)
library(bipartite)
library(doMC)
library(plyr)
library(ggthemes)
library(picante)

# And authenticate - pops open a page in your default browser, then tells 
# you authentication was successful
github_auth()
{% endhighlight %}


***************

### Get all repos for an organization, here ropensci of course

{% highlight r %}
ropensci_repos <- github_allrepos(userorg = "ropensci")
{% endhighlight %}


***************

### Get commits broken down in to additions and deletions, though below we just collapse them to all commits

{% highlight r %}
registerDoMC(cores = 4)
github_commits_safe <- plyr::failwith(NULL, github_commits)
out <- llply(ropensci_repos, function(x) github_commits_safe("ropensci", x, 
    since = "2009-01-01T", limit = 500), .parallel = TRUE)
names(out) <- ropensci_repos
out2 <- compact(out)
outdf <- ldply(out2)
{% endhighlight %}


***************

### Plot commits by date and repo

{% highlight r %}
outdf_subset <- outdf[!outdf$.id %in% c("citeulike", "challenge", "docs", "ropensci-book", 
    "usecases", "textmine", "usgs", "ropenscitoolkit", "neotoma", "rEWDB", "rgauges", 
    "rodash", "ropensci.github.com", "ROAuth"), ]
outdf_subset$.id <- tolower(outdf_subset$.id)
outdf_subset <- ddply(outdf_subset, .(.id, date), summarise, value = sum(value))

mindates <- llply(unique(outdf_subset$.id), function(x) min(outdf_subset[outdf_subset$.id == 
    x, "date"]))
names(mindates) <- unique(outdf_subset$.id)
mindates <- sort(do.call(c, mindates))
outdf_subset$.id <- factor(outdf_subset$.id, levels = names(mindates))

ggplot(outdf_subset, aes(date, value, fill = .id)) + 
    geom_bar(stat = "identity", width = 0.5) + 
    geom_rangeframe(sides = "b", colour = "grey") + 
    theme_bw(base_size = 9) + 
    scale_x_date(labels = date_format("%Y"), breaks = date_breaks("year")) + 
    scale_y_log10() + 
    facet_grid(.id ~ .) + 
    labs(x = "", y = "") + 
    theme(axis.text.y = element_blank(), 
        axis.text.x = element_text(colour = "black"), 
        axis.ticks.y = element_blank(), 
        strip.text.y = element_text(angle = 0, size = 8, ), 
        strip.background = element_rect(size = 0), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        legend.text = element_text(size = 8), 
        legend.position = "none", 
        panel.border = element_blank())
{% endhighlight %}


![center](/public/img/commitsbydate.png) 


The plot above plots the sum of additions+deletions, and is sorted by the first commit date of reach repo, with the first being [treebase](https://github.com/ropensci/treeBASE), which wraps the [Treebase API](http://treebase.org/treebase-web/urlAPI.html), and the most recent being [rwbclimate](https://github.com/ropensci/rWBclimate), which wraps the [World Blank climate data API](http://data.worldbank.org/developers/climate-data-api). 

You can see that some repos have recieved commits more or less consistently over their life time, while others have seen a little development here and there. 

***************
w
### In addition, there are quite a few people that have committed code now to rOpenSci repos, calling for a network vizualization of course.

{% highlight r %}
outdf_network <- droplevels(outdf[!outdf$.id %in% c("citeulike", "challenge", 
    "docs", "ropensci-book", "usecases", "textmine", "usgs", "ropenscitoolkit", 
    "retriever", "rodash", "ropensci.github.com", "ROAuth", "rgauges", "sandbox", 
    "rfna", "rmetadata", "rhindawi", "rpmc", "rpensoft", "ritis"), ])
casted <- dcast(outdf_network, .id + date + name ~ variable, fun.aggregate = length, 
    value.var = "value")
names(casted)[1] <- "repo"
casted2 <- ddply(casted, .(repo, name), summarise, commits = sum(additions))
casted2 <- data.frame(repo = casted2$repo, weight = casted2$commits, name = casted2$name)
mat <- sample2matrix(casted2)
plotweb(sortweb(mat, sort.order = "dec"), method = "normal", text.rot = 90, 
    adj.high = c(-0.3, 0), adj.low = c(1, -0.3), y.width.low = 0.05, y.width.high = 0.05, 
    ybig = 0.09, labsize = 0.7)
{% endhighlight %}

![center](/public/img/collabnetwork.png) 


The plot above shows repos on one side and contributors on the other. Some folks (the core rOpenSci team: cboettig, karthikram, emhart, and schamberlain) have committed quite a lot to many packages. We also have amny awesome contributors to our packages (some contributors and repos have been removed for clarity). 

rOpenSci is truly a collaborative effort to develop tools for open science, so thanks to all our contributors - keep on forking, pull requesting, and commiting. 

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page27" class="older">Older</a>
  
  
    
      <a href="/page25" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
