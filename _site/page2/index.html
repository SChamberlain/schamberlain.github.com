<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://recology.info/">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2015/10/noaa-isd/">
        noaa - Integrated Surface Database data
      </a>
    </h1>

    <span class="post-date">21 Oct 2015</span>

    I've recently made some improvements to the functions that work with ISD 
(Integrated Surface Database) data.

__isd data__

* The `isd()` function now caches more intelligently. We now cache using 
`.rds` files via `saveRDS`/`readRDS`, whereas we used to use `.csv` files, 
which take up much more disk space, and we have to worry about not changing 
data formats on reading data back into an R session. This has the downside
that you can't just go directly to open up a cached file in your favorite 
spreadsheet viewer, but you can do that manually after reading in to R. 
* In addition, `isd()` now has a function `cleanup`, if `TRUE` after 
downloading the data file from NOAA's ftp server and processing, we delete 
the file. That's fine since we have the cached processed file. But you 
can choose not to cleanup the original data files.
* Data processing in `isd()` is improved as well. We convert key variables
to appropriate classes to be more useful. 

__isd stations__

* In `isd_stations()`, there's now a cached version of the station data in 
the package, or you can get optionally get fresh station data from NOAA's 
FTP server.
* There's a new function `isd_stations_search()` that uses the station data
to allow you to search for stations via either:
  * A bounding box
  * Radius froma point

## Install

For examples below, you'll need the development version:


```r
devtools::install_github("ropensci/rnoaa")
```

Load `rnoaa`


```r
library("rnoaa")
```

## ISD stations 

### Get stations

There's a cached version of the station data in the package, or you can get fresh
station data from NOAA's FTP server.


```r
stations <- isd_stations()
head(stations)
#>   usaf  wban station_name ctry state icao lat lon elev_m    begin      end
#> 1 7005 99999   CWOS 07005                  NA  NA     NA 20120127 20120127
#> 2 7011 99999   CWOS 07011                  NA  NA     NA 20111025 20121129
#> 3 7018 99999   WXPOD 7018                   0   0   7018 20110309 20130730
#> 4 7025 99999   CWOS 07025                  NA  NA     NA 20120127 20120127
#> 5 7026 99999   WXPOD 7026   AF              0   0   7026 20120713 20141120
#> 6 7034 99999   CWOS 07034                  NA  NA     NA 20121024 20121106
```

### Filter and visualize stations

In addition to getting the entire station data.frame, you can also search for stations,
either with a bounding box or within a radius from a point. First, the bounding box


```r
bbox <- c(-125.0, 38.4, -121.8, 40.9)
out <- isd_stations_search(bbox = bbox)
head(out)
#>     usaf  wban                          station_name ctry state icao
#> 1 720193 99999 LONNIE POOL FLD / WEAVERVILLE AIRPORT   US    CA KO54
#> 2 724834 99999                        POINT CABRILLO   US    CA     
#> 3 724953 99999                              RIO NIDO   US    CA     
#> 4 724957 23213                 SONOMA COUNTY AIRPORT   US    CA KSTS
#> 5 724957 99999                  C M SCHULZ SONOMA CO   US    CA KSTS
#> 6 724970 99999                  CHICO CALIFORNIA MAP   US    CA  CIC
#>   elev_m    begin      end      lon    lat
#> 1  716.0 20101030 20150831 -122.922 40.747
#> 2   20.0 19810906 19871007 -123.820 39.350
#> 3 -999.0 19891111 19900303 -122.917 38.517
#> 4   34.8 20000101 20150831 -122.810 38.504
#> 5   38.0 19430404 19991231 -122.817 38.517
#> 6   69.0 19420506 19760305 -121.850 39.783
```

Where is the bounding box? (you'll need [lawn](https://cran.rstudio.com/web/packages/lawn/), or you can vizualize some other way)


```r
library("lawn")
lawn::lawn_bbox_polygon(bbox) %>% view
```

![plot1](/public/img/2015-10-21-noaa-isd/bbox_area.png)

Vizualize station subset - yep, looks right


```r
library("leaflet")
leaflet(data = out) %>%
  addTiles() %>%
  addCircles()
```

![plot1](/public/img/2015-10-21-noaa-isd/bbox_result.png)

Next, search with a lat/lon coordinate, with a radius. That is, we search for stations
within X km from the coordinate.


```r
out <- isd_stations_search(lat = 38.4, lon = -123, radius = 250)
head(out)
#>     usaf  wban             station_name ctry state icao elev_m    begin
#> 1 690070 93217            FRITZSCHE AAF   US    CA KOAR   43.0 19600404
#> 2 720267 23224 AUBURN MUNICIPAL AIRPORT   US    CA KAUN  466.7 20060101
#> 3 720267 99999         AUBURN MUNICIPAL   US    CA KAUN  468.0 20040525
#> 4 720406 99999      GNOSS FIELD AIRPORT   US    CA KDVO    0.6 20071114
#> 5 720576   174       UNIVERSITY AIRPORT   US    CA KEDU   21.0 20130101
#> 6 720576 99999                    DAVIS   US    CA KEDU   21.0 20080721
#>        end      lon    lat
#> 1 19930831 -121.767 36.683
#> 2 20150831 -121.082 38.955
#> 3 20051231 -121.082 38.955
#> 4 20150831 -122.550 38.150
#> 5 20150831 -121.783 38.533
#> 6 20121231 -121.783 38.533
```

Again, compare search area to stations found

_search area_


```r
pt <- lawn::lawn_point(c(-123, 38.4))
lawn::lawn_buffer(pt, dist = 250) %>% view
```

![plot1](/public/img/2015-10-21-noaa-isd/circle_radius.png)

_stations found_


```r
leaflet(data = out) %>%
  addTiles() %>%
  addCircles()
```

![plot1](/public/img/2015-10-21-noaa-isd/lastplot.png)



## ISD data

### Get ISD data

Here, I get data for four stations.


```r
res1 <- isd(usaf="011690", wban="99999", year=1993)
res2 <- isd(usaf="172007", wban="99999", year=2015)
res3 <- isd(usaf="702700", wban="00489", year=2015)
res4 <- isd(usaf="109711", wban=99999, year=1970)
```

Then, combine data, with `rnoaa:::rbind.isd()`


```r
res_all <- rbind(res1, res2, res3, res4)
```

Add date time


```r
library("lubridate")
res_all$date_time <- ymd_hm(
  sprintf("%s %s", as.character(res_all$date), res_all$time)
)
```

Remove 999's (NOAA's way to indicate missing/no data)


```r
library("dplyr")
res_all <- res_all %>% filter(temperature < 900)
```

### Visualize ISD data


```r
library("ggplot2")
ggplot(res_all, aes(date_time, temperature)) +
  geom_line() + 
  facet_wrap(~usaf_station, scales = "free_x")
```

![img](/public/img/2015-10-21-noaa-isd/unnamed-chunk-12-1.png) 

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/10/open-source-metrics/">
        Metrics for open source projects
      </a>
    </h1>

    <span class="post-date">19 Oct 2015</span>

    Measuring use of open source software isn't always straightforward. The problem is especially acute for software targeted largely at academia, where usage is not measured just by software downloads, but also by citations.

Citations are a well-known pain point because the citation graph is privately held by iron doors (e.g., [Scopus][scopus], [Google Scholar][schol]). New ventures aim to open up citation data, but of course it's an immense amount of work, and so does not come quickly.

The following is a laundry list of metrics on software of which I am aware, and some of which I use in our [rOpenSci twice monthly updates][news].

I primarily develop software for the R language, so some of the metrics are specific to R, but many are not. In addition, we (rOpenSci) don't develop web apps, which may bring in an additional set of metrics not covered below.

I organize by source instead of type of data because some sources give multiple kinds of data - I note what kinds of data they give with <span class="label label-default">labels</span>.

## CRAN downloads

<span class="label label-warning">downloads</span>

- Link: [https://github.com/metacran/cranlogs.app](https://github.com/metacran/cranlogs.app)
- This is a REST API for CRAN downloads from the RStudio CRAN CDN. Note however, that the RStudio CDN is only one of many - there are other mirrors users can insall packages from, and are not included in this count. However, a significant portion of downloads probably come from the RStudio CDN.
- Other programming languages have similar support, e.g., [Ruby](http://guides.rubygems.org/rubygems-org-api/) and [Node](https://github.com/npm/download-counts).

## Lagotto

<small><span class="label label-success">citations</span>&nbsp;<span class="label label-info">github</span>&nbsp;<span class="label label-primary">social-media</span></small>

- Link: [http://software.lagotto.io/works](http://software.lagotto.io/works)
- Lagotto is a Rails application, developed by [Martin Fenner](https://github.com/mfenner), originally designed to collect and provide article level metrics for scientific publications at Public Library of Science. It is now used by many publishers, and there are installations of Lagotto targeting [datasets](http://mdc.lagotto.io/) and [software](http://software.lagotto.io/works).
- Discussion forum: [http://discuss.lagotto.io/](http://discuss.lagotto.io/)

## Depsy

<small><span class="label label-success">citations</span>&nbsp;<span class="label label-info">github</span></small>

- Link: [http://depsy.org](http://depsy.org)
- This is a nascent venture by the [ImpactStory team](https://impactstory.org/about) that seeks to uncover the impact of research software. As far as I can tell, they'll collect usage via software downloads and citations in the literature.

## Web Site Analytics

<small><span class="label label-danger">page-views</span></small>

- If you happen to have a website for your project, collecting analytics is a way to gauge views of the landing page, and any help/tutorial pages you may have. A good easy way to do this is a deploy a basic site on your `gh-pages` branch of your GitHub repo, and use the easily integrated Google Analytics.
- Whatever analytics you use, in my experience this mostly brings up links from google searches and blog posts that may mention your project
- Google Analytics beacon (for README views): [https://github.com/igrigorik/ga-beacon](https://github.com/igrigorik/ga-beacon). I haven't tried this yet, but seems promising.

## Auomated tracking: SSNMP

<small><span class="label label-success">citations</span>&nbsp;<span class="label label-info">github</span></small>

- Link: [http://scisoft-net-map.isri.cmu.edu](http://scisoft-net-map.isri.cmu.edu)
- Scientific Software Network Map Project
- This is a cool NSF funded project by Chris Bogart that tracks software usage via GitHub and citations in literature.  

## Google Scholar

<small><span class="label label-success">citations</span></small>

- Link: [https://scholar.google.com/](https://scholar.google.com/)
- Searching Google Scholar for software citations manually is fine at a small scale, but at a larger scale scraping is best. However, you're not legally supposed to do this, and Google will shut you down.
- Could try using g-scholar alerts as well, especially if new citations of your work are infrequent.
- If you have institutional access to Scopus/Web of Science, you could search those, but I don't push this as an option since it's available to so few.

## GitHub

<small><span class="label label-info">github</span></small>

- Links: [https://developer.github.com/v3/](https://developer.github.com/v3/)
- I keep a list of rOpenSci uses found in GitHub repos at [https://discuss.ropensci.org/t/use-of-some-ropensci-packages-on-github/137](https://discuss.ropensci.org/t/use-of-some-ropensci-packages-on-github/137)
- GitHub does collect traffic data on each repo (clones, downloads, page views), but they are not exposed in the API. I've bugged them a bit about this - hopefully we'll be able to get that dat in their API soon.
- Bitbucket/Gitlab - don't use them, but I assume they also provide some metrics via their APIs

## Other

- Support forums: Whether you use UserVoice, Discourse, Google Groups, Gitter, etc., depending on your viewpoint, these interactions could be counted as metrics of software usage. 
- Emails: I personally get a lot of emails asking for help with software I maintain. I imagine this is true for most software developers. Counting these could be another metric of software usage, although I never have counted mine.
- Social media: See Lagotto above, which tracks some social media outlets.
- Code coverage: There are many options now for code coverage, integrated with each Travis-CI build. A good option is [CodeCov](https://codecov.io). CodeCov gives percentage test coverage, which one could use as one measure of code quality.
- Reviews: There isn't a lot of code review going on that I'm aware of. Even if there was, I suppose this would just be a logical TRUE/FALSE.
- Cash money y'all: Grants/consulting income/etc. could be counted as a metric.
- Users: If you require users to create an account or similar before getting your software, you have a sense of number of users and perhaps their demographics.

## Promising

Some software metrics things on the horizon that look interesting:

* [Software Attribution for Geoscience Applications][saga] (SAGA)
* Crossref: They have [a very nice API][crapi], but they don't yet provide citation counts - but [they may soon][crmaybe].
* [njsmith/sempervirens](https://github.com/njsmith/sempervirens) - a prototype for _gathering anonymous, opt-in usage data for open scientific software_
* [Force11 Software Citation Working Group](https://github.com/force11/force11-scwg) - _...produce a consolidated set of citation principles in order to encourage broad adoption of a consistent policy for software citation across disciplines and venues_

## Missed?

I'm sure I missed things. Let me know.

[scopus]: http://www.scopus.com/
[schol]: https://scholar.google.com/
[saga]: https://geodynamics.org/cig/projects/saga/
[crapi]: https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md
[crmaybe]: https://github.com/CrossRef/rest-api-doc/issues/46
[neil]: https://youtu.be/jMH7FTGqQEE?t=1h3m41s
[wssspe3]: http://wssspe.researchcomputing.org.uk/wssspe3/
[news]: http://ropensci.github.io/biweekly/

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/10/analogsea-cran/">
        analogsea - an R client for the Digital Ocean API
      </a>
    </h1>

    <span class="post-date">02 Oct 2015</span>

    `analogsea` is now on CRAN. We started developing the pkg back in [May 2014][firstcomm], but just 
now getting the first version on CRAN. It's a collaboration with [Hadley][hadley] and [Winston Chang][chang].

Most of `analogsea` package is for interacting with the [Digital Ocean API](https://developers.digitalocean.com/documentation/v2/), including:

* Manage domains
* Manage ssh keys
* Get actions
* Manage images
* Manage droplets (servers)

A number of convenience functions are included for doing tasks (e.g., resizing 
a droplet) that aren't supported by Digital Ocean's API out of the box (i.e., 
there's no API route for it). 

In addition to wrapping their API routes, we provide other functionality, e.g.: 

* execute shell commands on a droplet (server)
* execute R commands on a droplet
* install R
* install RStudio server
* install Shiny server

Other functionality we're working on, not yet available:

* install OpenCPU
* use `packrat` to move projects from local to server, and vice versa

See also: two previous blog posts on this package [http://recology.info/2014/05/analogsea/](http://recology.info/2014/05/analogsea/) and [http://recology.info/2014/06/analogsea-v01/](http://recology.info/2014/06/analogsea-v01/)

## Install

Binaries are not yet on CRAN, but you can install from source.


```r
# install.packages("analogsea") # when binaries available
install.packages("analogsea", repos = "https://cran.r-project.org", type = "source")
```

Or install development version from GitHub


```r
devtools::install_github("sckott/analogsea")
```

Load `analogsea`


```r
library("analogsea")
```

## Etc.

As this post is mostly to announce that this pkg is on CRAN now, I won't go through examples, but instead point you to the package [README][readme] and [vignette][vign] in which we cover 
creating a Digital Ocean account, authenticating, and have many examples.

## Feedback

Let us know what you think. We'd love to hear about any problems, use cases, feature requests. 

[firstcomm]: https://github.com/sckott/analogsea/commit/b129164dd87969d2fc6bcf3b51576fe1da932fdb
[hadley]: http://had.co.nz/
[chang]: https://github.com/wch/
[readme]: https://github.com/sckott/analogsea/blob/master/README.md
[vign]: https://github.com/sckott/analogsea/blob/master/vignettes/doapi.Rmd

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page3" class="older">Older</a>
  
  
    
      <a href="/" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
