<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://recology.info/">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2015/04/the-new-way/">
        the new way - httsnap
      </a>
    </h1>

    <span class="post-date">29 Apr 2015</span>

    Inspired by `httpie`, a Python command line client as a sort of drop in replacement for `curl`, I am playing around with something similar-ish in R, at least in spirit. I started a little R pkg called `httsnap` with the following ideas:

* The web is increasingly a JSON world, so set `content-type` and `accept` headers to `applications/json` by default 
* The workflow follows logically, or at least should, from, _hey, I got this url_, to _i need to add some options_, to _execute request_
* Whenever possible, transform output to data.frame's - facilitating downstream manipulation via `dplyr`, etc.
* Do `GET` requests by default. Specify a different type if you don't want `GET`. Some functionality does GET by default, though in some cases you need to specify GET
* You can use non-standard evaluation to easily pass in query parameters without worrying about `&`'s, URL escaping, etc. (see `Query()`)
* Same for body params (see `Body()`)

## Install

Install and load `httsnap`


```r
devtools::install_github("sckott/httsnap")
```


```r
library("httsnap")
library("dplyr")
```

## Functions so far

* `Get` - GET request
* `Query` - add query parameters
* `Authenticate` - add authentication details
* `Progress` - add progress bar
* `Timeout` - add a timeout
* `User_agent` - add a user agent
* `Verbose` - give verbose output
* `Body` - add a body
* `h` - add headers by key-value pair

These are named to avoid conflict with `httr`

## Intro

A simple `GET` request


```r
"http://httpbin.org/get" %>%
  Get()
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

You'll notice that `Get()` doesn't just get the response, but also checks for whether it was a good response (the HTTP status code), and extracts the data. 

Or you can just pass the URL into the function itself


```r
Get("http://httpbin.org/get")
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

You can buid up options by calling functions via pipes, and see what the options look like


```r
"http://httpbin.org/get" %>%
  Progress() %>%
  Verbose()
#> <http request> 
#>   url: http://httpbin.org/get
#>   config: 
#> Config: 
#> List of 4
#>  $ noprogress      :FALSE
#>  $ progressfunction:function (...)  
#>  $ debugfunction   :function (...)  
#>  $ verbose         :TRUE
```

Then execute the GET request when you're ready


```r
"http://httpbin.org/get" %>%
  Progress() %>%
  Verbose() %>%
  Get()
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

## Example 1

Get scholarly article metadata from the Crossref API


```r
"http://api.crossref.org/works" %>%
  Query(query = "ecology") %>% 
  .$message %>% 
  .$items %>% 
  select(DOI, title, publisher)
#>                            DOI                title
#> 1          10.4996/fireecology         Fire Ecology
#> 2              10.5402/ecology         ISRN Ecology
#> 3                 10.1155/8641         ISRN Ecology
#> 4      10.1111/(issn)1526-100x  Restoration Ecology
#> 5        10.1007/248.1432-184x    Microbial Ecology
#> 6      10.1007/10144.1438-390x   Population Ecology
#> 7      10.1007/10452.1573-5125      Aquatic Ecology
#> 8      10.1007/10682.1573-8477 Evolutionary Ecology
#> 9      10.1007/10745.1572-9915        Human Ecology
#> 10     10.1007/10980.1572-9761    Landscape Ecology
#> 11     10.1007/11258.1573-5052        Plant Ecology
#> 12     10.1007/12080.1874-1746  Theoretical Ecology
#> 13     10.1111/(issn)1442-9993      Austral Ecology
#> 14     10.1111/(issn)1439-0485       Marine Ecology
#> 15     10.1111/(issn)1365-2435   Functional Ecology
#> 16     10.1111/(issn)1365-294x    Molecular Ecology
#> 17     10.1111/(issn)1461-0248      Ecology Letters
#> 18   10.1002/9780470979365.ch7  Behavioural Ecology
#> 19 10.1111/fec.2007.21.issue-5                     
#> 20     10.1111/rec.0.0.issue-0                     
#>                            publisher
#> 1       Association for Fire Ecology
#> 2     Hindawi Publishing Corporation
#> 3     Hindawi Publishing Corporation
#> 4                    Wiley-Blackwell
#> 5  Springer Science + Business Media
#> 6  Springer Science + Business Media
#> 7  Springer Science + Business Media
#> 8  Springer Science + Business Media
#> 9  Springer Science + Business Media
#> 10 Springer Science + Business Media
#> 11 Springer Science + Business Media
#> 12 Springer Science + Business Media
#> 13                   Wiley-Blackwell
#> 14                   Wiley-Blackwell
#> 15                   Wiley-Blackwell
#> 16                   Wiley-Blackwell
#> 17                   Wiley-Blackwell
#> 18                   Wiley-Blackwell
#> 19                   Wiley-Blackwell
#> 20                   Wiley-Blackwell
```

## Example 2

Get Public Library of Science article metadata via their API, make a histogram of number of tweets for each article


```r
"http://api.plos.org/search" %>%
  Query(q = "*:*", wt = "json", rows = 100, 
        fl = "id,journal,alm_twitterCount",  
        fq = 'alm_twitterCount:[100 TO 10000]') %>% 
  .$response %>% 
  .$docs %>% 
  .$alm_twitterCount %>% 
  hist()
```
 
![image](/public/img/2015-04-29-the-new-way/unnamed-chunk-9-1.png)

## Notes

Okay, so this isn't drastically different from what `httr` already does, but its early days. 

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/03/faster-solr/">
        Faster solr with csv
      </a>
    </h1>

    <span class="post-date">20 Mar 2015</span>

    With the [help of user input](https://github.com/ropensci/solr/issues/47), I've tweaked `solr` just a bit to make things faster using default setings. I imagine the main interface for people using the `solr` R client is via `solr_search()`, which used to have `wt=json` by default. Changing this to `wt=csv` gives better performance. And it sorta makes sense to use csv, as the point of using an R client is probably do get data eventually into a data.frame, so it makes sense to go csv format (Already in tabular format) if it's faster too.

## Install

Install and load `solr`


```r
devtools::install_github("ropensci/solr")
```


```r
library("solr")
library("microbenchmark")
```

## Setup

Define base url and fields to return


```r
url <- 'http://api.plos.org/search'
fields <- c('id','cross_published_journal_name','cross_published_journal_key',
            'cross_published_journal_eissn','pmid','pmcid','publisher','journal',
            'publication_date','article_type','article_type_facet','author',
            'author_facet','volume','issue','elocation_id','author_display',
            'competing_interest','copyright')
```

## json

The previous default for `solr_search()` used `json`


```r
solr_search(q='*:*', rows=10, fl=fields, base=url, wt = "json")
#> Source: local data frame [10 x 19]
#> 
#>                                                                    id
#> 1             10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4
#> 2       10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/title
#> 3    10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/abstract
#> 4  10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/references
#> 5        10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/body
#> 6             10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525
#> 7       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/title
#> 8    10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/abstract
#> 9  10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/references
#> 10       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/body
#> Variables not shown: cross_published_journal_name (chr),
#>   cross_published_journal_key (chr), cross_published_journal_eissn (chr),
#>   pmid (chr), pmcid (chr), publisher (chr), journal (chr),
#>   publication_date (chr), article_type (chr), article_type_facet (chr),
#>   author (chr), author_facet (chr), volume (int), issue (int),
#>   elocation_id (chr), author_display (chr), competing_interest (chr),
#>   copyright (chr)
```

## csv

The default `wt` setting is now `csv`


```r
solr_search(q='*:*', rows=10, fl=fields, base=url, wt = "json")
#> Source: local data frame [10 x 19]
#> 
#>                                                                    id
#> 1             10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4
#> 2       10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/title
#> 3    10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/abstract
#> 4  10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/references
#> 5        10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/body
#> 6             10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525
#> 7       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/title
#> 8    10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/abstract
#> 9  10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/references
#> 10       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/body
#> Variables not shown: cross_published_journal_name (chr),
#>   cross_published_journal_key (chr), cross_published_journal_eissn (chr),
#>   pmid (chr), pmcid (chr), publisher (chr), journal (chr),
#>   publication_date (chr), article_type (chr), article_type_facet (chr),
#>   author (chr), author_facet (chr), volume (int), issue (int),
#>   elocation_id (chr), author_display (chr), competing_interest (chr),
#>   copyright (chr)
```

## Compare times

When parsing to a data.frame (which `solr_search()` does by default), csv is quite a bit faster.


```r
microbenchmark(
  json = solr_search(q='*:*', rows=500, fl=fields, base=url, wt = "json", verbose = FALSE),
  csv = solr_search(q='*:*', rows=500, fl=fields, base=url, wt = "csv", verbose = FALSE), 
  times = 20
)
#> Unit: milliseconds
#>  expr      min       lq      mean    median        uq       max neval cld
#>  json 965.7043 1013.014 1124.1229 1086.3225 1227.9054 1441.8332    20   b
#>   csv 509.6573  520.089  541.5784  532.4546  548.0303  723.7575    20  a
```

## json vs xml vs csv

When getting raw data, csv is best, json next, then xml pulling up the rear.


```r
microbenchmark(
  json = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "json", verbose = FALSE, raw = TRUE),
  csv = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "csv", verbose = FALSE, raw = TRUE),
  xml = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "xml", verbose = FALSE, raw = TRUE),
  times = 10
)
#> Unit: milliseconds
#>  expr       min       lq      mean    median        uq       max neval cld
#>  json 1110.9515 1142.478 1198.9981 1169.0808 1195.5709 1518.7412    10  b 
#>   csv  801.6871  802.516  826.0655  819.1532  835.0512  873.4266    10 a  
#>   xml 1507.1111 1554.002 1618.5963 1617.5208 1671.0026 1740.4448    10   c
```

## Notes

Note that `wt=csv` is only available in `solr_search()` and `solr_all()` because csv writer 
only returns the docs element in csv, dropping other elements, including facets, mlt, groups, 
stats, etc. 

Also, note the http client used in `solr` is `httr`, which passes in a gzip compression header by default, so as long as the server serving up the Solr data has compression turned on, that's all set.

Another way I've sped things up is if you use `wt=json` then parse to a data.frame, it uses `dplyr` which sped things up considerably.

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/03/couch-dataframes/">
        PUT dataframes on your couch
      </a>
    </h1>

    <span class="post-date">12 Mar 2015</span>

    It would be nice to easily push each row or column of a data.frame into CouchDB instead of having to prepare them yourself into JSON, then push in to couch. I recently added ability to push data.frame's into couch using the normal `PUT /{db}` method, and added support for the couch bulk API.

## Install


```r
install.packages("devtools")
devtools::install_github("sckott/sofa")
```


```r
library("sofa")
```

## PUT /db

You can write directly from a data.frame, either by rows or columns. First, rows:


```
#> $ok
#> [1] TRUE
```

Create a database


```r
db_create(dbname="mtcarsdb")
#> $ok
#> [1] TRUE
```


```r
out <- doc_create(mtcars, dbname="mtcarsdb", how="rows")
out[1:2]
#> $`Mazda RX4`
#> $`Mazda RX4`$ok
#> [1] TRUE
#> 
#> $`Mazda RX4`$id
#> [1] "0063109bfb1c15765854cbc9525c3a7a"
#> 
#> $`Mazda RX4`$rev
#> [1] "1-3946941c894a874697554e3e6d9bc176"
#> 
#> 
#> $`Mazda RX4 Wag`
#> $`Mazda RX4 Wag`$ok
#> [1] TRUE
#> 
#> $`Mazda RX4 Wag`$id
#> [1] "0063109bfb1c15765854cbc9525c461d"
#> 
#> $`Mazda RX4 Wag`$rev
#> [1] "1-273ff17a938cb956cba21051ab428b95"
```

Then by columns


```r
out <- doc_create(mtcars, dbname="mtcarsdb", how="columns")
out[1:2]
#> $mpg
#> $mpg$ok
#> [1] TRUE
#> 
#> $mpg$id
#> [1] "0063109bfb1c15765854cbc9525d4f1f"
#> 
#> $mpg$rev
#> [1] "1-4b83d0ef53a28849a872d47ad03fef9a"
#> 
#> 
#> $cyl
#> $cyl$ok
#> [1] TRUE
#> 
#> $cyl$id
#> [1] "0063109bfb1c15765854cbc9525d57d3"
#> 
#> $cyl$rev
#> [1] "1-c21bfa5425c67743f0826fd4b44b0dbf"
```

## Bulk API

The bulk API will/should be faster for larger data.frames


```
#> $ok
#> [1] TRUE
```

We'll use part of the diamonds dataset


```r
library("ggplot2")
dat <- diamonds[1:20000,]
```

Create a database


```r
db_create(dbname="bulktest")
#> $ok
#> [1] TRUE
```

Load by row (could instead do each column, see `how` parameter), printing the time it takes


```r
system.time(out <- bulk_create(dat, dbname="bulktest"))
#>    user  system elapsed 
#>  16.832   6.039  24.432
```

The returned data is the same as with `doc_create()`


```r
out[1:2]
#> [[1]]
#> [[1]]$ok
#> [1] TRUE
#> 
#> [[1]]$id
#> [1] "0063109bfb1c15765854cbc9525d8b8d"
#> 
#> [[1]]$rev
#> [1] "1-f407fe4935af7fd17c101f13d3c81679"
#> 
#> 
#> [[2]]
#> [[2]]$ok
#> [1] TRUE
#> 
#> [[2]]$id
#> [1] "0063109bfb1c15765854cbc9525d998b"
#> 
#> [[2]]$rev
#> [1] "1-cf8b9a9dcdc026052a663d6fef8a36fe"
```

So that's 20,000 rows in not that much time, not bad.

### not dataframes

You can also pass in lists or vectors of json as character strings, e.g., 

_lists_


```
#> $ok
#> [1] TRUE
```


```r
row.names(mtcars) <- NULL # get rid of row.names
lst <- parse_df(mtcars, tojson=FALSE)
db_create(dbname="bulkfromlist")
#> $ok
#> [1] TRUE
out <- bulk_create(lst, dbname="bulkfromlist")
out[1:2]
#> [[1]]
#> [[1]]$ok
#> [1] TRUE
#> 
#> [[1]]$id
#> [1] "ba70c46d73707662b1e204a90fcd9bb8"
#> 
#> [[1]]$rev
#> [1] "1-3946941c894a874697554e3e6d9bc176"
#> 
#> 
#> [[2]]
#> [[2]]$ok
#> [1] TRUE
#> 
#> [[2]]$id
#> [1] "ba70c46d73707662b1e204a90fcda9f6"
#> 
#> [[2]]$rev
#> [1] "1-273ff17a938cb956cba21051ab428b95"
```

_json_


```
#> $ok
#> [1] TRUE
```


```r
strs <- as.character(parse_df(mtcars, "columns"))
db_create(dbname="bulkfromchr")
#> $ok
#> [1] TRUE
out <- bulk_create(strs, dbname="bulkfromchr")
out[1:2]
#> [[1]]
#> [[1]]$ok
#> [1] TRUE
#> 
#> [[1]]$id
#> [1] "ba70c46d73707662b1e204a90fce8c20"
#> 
#> [[1]]$rev
#> [1] "1-4b83d0ef53a28849a872d47ad03fef9a"
#> 
#> 
#> [[2]]
#> [[2]]$ok
#> [1] TRUE
#> 
#> [[2]]$id
#> [1] "ba70c46d73707662b1e204a90fce9bc1"
#> 
#> [[2]]$rev
#> [1] "1-c21bfa5425c67743f0826fd4b44b0dbf"
```

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page9" class="older">Older</a>
  
  
    
      <a href="/page7" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
