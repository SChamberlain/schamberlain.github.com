<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://localhost:4000">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2013/06/coffeehouse/">
        Coffeehouse - an aggregator for blog posts about data, data management, etc.
      </a>
    </h1>

    <span class="post-date">18 Jun 2013</span>

    Have you heard of [DataONE](http://www.dataone.org/)? It stands for the Data Observation Network for Earth, and I am involved in the [Community Education and Engagement working group](http://www.dataone.org/working_groups/community-education-and-engagement) at DataONE. We try to communicate about data, data management, and similar things to scientists and other DataONE *stakeholders*. 

At our last meeting, we decided to start a blog aggregator to pull in to one place blog posts about data, data management, and related topics. Those reading this blog have likely heard of [R-Bloggers](http://www.r-bloggers.com/) - and there are many more aggregator blogs. We are calling this blog aggregator **Coffeehouse** - as it's sort of a place to gather to talk/read about ideas. Check it out [here][coffee]. If you blog about data management think about adding your blog to Coffeehouse - go to the [*Add your blog*][addblog] page to do so. A screenshot:

![](/public/img/coffeehouse.png)

********************

The blogs already added to Coffeehouse:

<i class="icon-coffee"></i> <a href="http://dataconservancy.org/blog/" target="_blank">Data Conservancy</a><br>
<i class="icon-coffee"></i> <a href="http://datapub.cdlib.org/" target="_blank">Data Pub</a><br>
<i class="icon-coffee"></i> <a href="http://www.datacite.org/" target="_blank">DataCite</a><br>
<i class="icon-coffee"></i> <a href="http://researchremix.wordpress.com/" target="_blank">Research Remix</a><br>
<i class="icon-coffee"></i> <a href="http://blogs.loc.gov/digitalpreservation/" target="_blank">The Signal: Digital Preservation</a>

********************

The tech/styling details:

+ As is obvious we are using Wordpress.org, with the Magazine Basic theme.
+ We don't accept comments - when someone clicks on the comments button it sends them back to the original post. This is on purpose so that the authors of the post get the comments on their own site.
+ On the top of each post there is an alert to tell you the post is syndicated, and gives a link to the original post. You can close this alert if it's annoying to you.
+ Style - we have strived to use clean and simple styling to make for a nice reading experience. A cluttered website makes reading painful. And using the [Twitter Bootstrap WP plugin][boot]
+ Icons: done using the [FontAwesome Wordpress Plugin][fawp].
+ Aggregating posts is done using the [FeedWordPress plugin][fwp].
+ The add your blog form: using the [Nina forms plugin][ninja]
+ Analytics: using the [Gauges WP plugin][gauges]

That's it. Let us know if you have any thoughts/comments.

[coffee]: https://coffeehouse.dataone.org/
[fawp]: https://github.com/rachelbaker/Font-Awesome-WordPress-Plugin
[addblog]: https://coffeehouse.dataone.org/add-your-blog/
[fwp]: http://feedwordpress.radgeek.com/
[ninja]: http://wpninjas.com/ninja-forms/
[boot]: http://www.icontrolwp.com/our-wordpress-plugins/wordpress-twitter-bootstrap-css-plugin-home/
[gauges]: http://wordpress.org/plugins/gauges/

  </div>
  
  <div class="post">
    <h1>
      <a href="/2013/06/couch/">
        Stashing and playing with raw data locally from the web
      </a>
    </h1>

    <span class="post-date">17 Jun 2013</span>

    It is getting easier to get data directly into R from the web. Often R packages that retrieve data from the web return useful R data structures to users like a data.frame. This is a good thing of course to make things user friendly. 

However, what if you want to drill down into the data that's returned from a query to a database in R?  What if you want to get that nice data.frame in R, but you think you may want to look at the raw data later? The raw data from web queries are often JSON or XML data. This type of data, especially JSON, can be easily stored in schemaless so-called NoSQL databases, and queried later. 

A brief aside: What are JSON and XML? This is what JSON looks like (ps if you ever wonder if your JSON is correct, go [here](http://jsonlint.com/)):

{% highlight bash %}
{
  "name": "joe",
  "hobby": "codemonkey",
  "lives": [
      {
          "city": "San Jose",
          "state": "CA"
      }
  ]
}
{% endhighlight %}

This is what XML looks like:

{% highlight bash %}
<?xml version="1.0" encoding="UTF-8" ?>
  <name>joe</name>
	<hobby>codemonkey</hobby>
	<lives>
		<city>San Jose</city>
		<state>CA</state>
	</lives>
{% endhighlight %}

But don't worry if it looks complicated - the project I talk about below, sofa, tries to make the interface to JSON and XML easy. Web APIs almost always return either JSON or XML, so this is the raw data.

So here's the use case I imagine, or workflow: 

+ Query a database on the web, and choose to write the raw data to a local database.
+ Do whatever you want with the output R object - analyze, visualize, etc.
+ Now you want to go back and search through some of the raw data. But, that query took an hour. Since you wrote it to a local database, you can search the data. 
+ If you hadn't written it locally, you would have to make a new web call. 

Note that if you are doing calls to web APIs that get small amounts of data you don't need to worry too much as you can easily just do the call again. 

I've started an R package to interact with the NoSQL database [CouchDB][couch]. CouchDB is a schemaless database that speaks JSON, so you can store JSON and get back JSON (don't worry XML, we got you covered - we can just wrap the XML in JSON before putting it in CouchDB). What's especially cool is you can interact with CouchDB via [a RESTful API][restapi]. CouchDB doesn't have full text search built in (though you can build map-reduce *Views*, basically preset queries on the database), so I added functions (and docs to help) to interact with the [CouchDB River plugin][couchriver] for [Elasticsearch][elastic], which provides powerful full text search via an API interface. But nevermind the tech details - all this just means you can search on the full text of your stored data. 

There are plenty of databases you can interact with from R, so why CouchDB? For one, it makes a lot of sense to write to a NoSQL database since this blog post is dealing with a use case writing JSON, which isn't a good fit for databases like MySQL, SQLite, PostgreSQL, etc. ([though postgres allows you to write JSON][postgres]). It didn't have to be CouchDB, but at least to me it seems relatively easy to install, you can interact with it via an HTTP API (if you're into that, which I am), and it has a nice web interface (navigate to [http://localhost:5984/_utils/](http://localhost:5984/_utils/) after starting `couchdb`).

Is this for the casual R user? Probably not. But, I imagine there are R users out there that want some more flexibility when it comes to interacting with web data in R. It is nice and tidy to get back an R data.frame from a web call, but having the raw data at your fingertips could be super powerful. I'll describe using an R package to interact with a web database with `sofa` baked in, and discuss a bit about the functions within `sofa`.

***************

### First start CouchDB in your terminal

You can do this from anywhere in your directory. See [here](http://couchdb.apache.org/) for instructions on how to install CouchDB.

```bash
couchdb
```

### Then start elasticsearch in your terminal

See [here](https://github.com/sckott/sofa) for instructions on how to install Elasticsearch and the River CouchDB plugin.

```bash
cd /usr/local/elasticsearch
bin/elasticsearch -f
```

***************

### Install sofa

{% highlight r %}
# Uncomment these lines if you don't have these packages installed
# install.packages('devtools') library(devtools) install_github('sofa',
# 'schamberlain') install_github('alm', 'ropensci', ref='couch')
library(sofa)
library(alm)
{% endhighlight %}


***************

### Simultaneously write data to CouchDB along with API calls using the alm package to get altmetrics data on PLoS papers. Ping to make sure CouchDB is on

{% highlight r %}
sofa_ping()
{% endhighlight %}



{% highlight text %}
  couchdb   version 
"Welcome"   "1.2.1" 
{% endhighlight %}

***************

### Create a new database

{% highlight r %}
sofa_createdb(dbname = "alm_db")
{% endhighlight %}



{% highlight text %}
  ok 
TRUE 
{% endhighlight %}

***************

### Write couchdb database name to options

{% highlight r %}
options(couch_db_name = "alm_db")
{% endhighlight %}


***************

### List the databases

{% highlight r %}
sofa_listdbs()
{% endhighlight %}



{% highlight text %}
 [1] "_replicator"                "_users"                    
 [3] "alm_couchdb"                "alm_db"                    
 [5] "dudedb"                     "example"                   
 [7] "poop"                       "rplos_db"                  
 [9] "shit"                       "shitty"                    
[11] "shitty2"                    "test_suite_db"             
[13] "test_suite_db/with_slashes" "test_suite_reports"        
[15] "testr2couch"                "twitter_db"                
{% endhighlight %}

***************

### Search for altmetrics normally, w/o writing to a database

{% highlight r %}
head(alm(doi = "10.1371/journal.pone.0029797"))
{% endhighlight %}



{% highlight text %}
          .id pdf html shares groups comments likes citations total
1   bloglines  NA   NA     NA     NA       NA    NA         0     0
2   citeulike  NA   NA      1     NA       NA    NA        NA     1
3    connotea  NA   NA     NA     NA       NA    NA         0     0
4    crossref  NA   NA     NA     NA       NA    NA         6     6
5      nature  NA   NA     NA     NA       NA    NA         4     4
6 postgenomic  NA   NA     NA     NA       NA    NA         0     0
{% endhighlight %}


***************

### Search for altmetrics normally, while writing to a database

{% highlight r %}
head(alm(doi = "10.1371/journal.pone.0029797", write2couch = TRUE))
{% endhighlight %}



{% highlight text %}
          .id pdf html shares groups comments likes citations total
1   bloglines  NA   NA     NA     NA       NA    NA         0     0
2   citeulike  NA   NA      1     NA       NA    NA        NA     1
3    connotea  NA   NA     NA     NA       NA    NA         0     0
4    crossref  NA   NA     NA     NA       NA    NA         6     6
5      nature  NA   NA     NA     NA       NA    NA         4     4
6 postgenomic  NA   NA     NA     NA       NA    NA         0     0
{% endhighlight %}


***************

### Make lots of calls, and write them simultaneously

{% highlight r %}
# install_github('rplos', 'ropensci')
library(rplos)
dois <- searchplos(terms = "evolution", fields = "id", limit = 100)
out <- alm(doi = as.character(dois[, 1]), write2couch = TRUE)
lapply(out[1:2], head)
{% endhighlight %}



{% highlight text %}
$`01`
          .id pdf html shares groups comments likes citations total
1   bloglines  NA   NA     NA     NA       NA    NA         0     0
2   citeulike  NA   NA      0     NA       NA    NA        NA     0
3    connotea  NA   NA     NA     NA       NA    NA         0     0
4    crossref  NA   NA     NA     NA       NA    NA         0     0
5      nature  NA   NA     NA     NA       NA    NA         0     0
6 postgenomic  NA   NA     NA     NA       NA    NA         0     0

$`02`
          .id pdf html shares groups comments likes citations total
1   bloglines  NA   NA     NA     NA       NA    NA         0     0
2   citeulike  NA   NA      1     NA       NA    NA        NA     1
3    connotea  NA   NA     NA     NA       NA    NA         0     0
4    crossref  NA   NA     NA     NA       NA    NA         2     2
5      nature  NA   NA     NA     NA       NA    NA         0     0
6 postgenomic  NA   NA     NA     NA       NA    NA         0     0
{% endhighlight %}


***************

### Writing data to CouchDB does take a bit longer

{% highlight r %}
system.time(alm(doi = as.character(dois[, 1])[1:60], write2couch = FALSE))
{% endhighlight %}



{% highlight text %}
   user  system elapsed 
  1.739   0.016   4.554 
{% endhighlight %}



{% highlight r %}
system.time(alm(doi = as.character(dois[, 1])[1:60], write2couch = TRUE))
{% endhighlight %}



{% highlight text %}
   user  system elapsed 
  3.579   0.062   6.460 
{% endhighlight %}


***************

### Search using elasticsearch
#### tell elasticsearch to start indexing your database

{% highlight r %}
elastic_start(dbname = "alm_db")
{% endhighlight %}



{% highlight text %}
$ok
[1] TRUE
{% endhighlight %}


***************

#### Search your database

{% highlight r %}
out <- elastic_search(dbname = "alm_db", q = "twitter", parse_ = TRUE)
out$hits$total
{% endhighlight %}



{% highlight text %}
[1] 679
{% endhighlight %}


***************

### Using views 

#### Write a view - here letting key be the default of null

{% highlight r %}
sofa_view_put(dbname = "alm_db", design_name = "myview", value = "doc.baseurl")
{% endhighlight %}



{% highlight text %}
$ok
[1] TRUE

$id
[1] "_design/myview"

$rev
[1] "1-e7c17cff1b96e4595c3781da53e16ad8"
{% endhighlight %}


***************

#### Get info on your new view

{% highlight r %}
sofa_view_get(dbname = "alm_db", design_name = "myview")
{% endhighlight %}



{% highlight text %}
$`_id`
[1] "_design/myview"

$`_rev`
[1] "1-e7c17cff1b96e4595c3781da53e16ad8"

$views
$views$foo
                                    map 
"function(doc){emit(null,doc.baseurl)}" 
{% endhighlight %}


***************

#### Get data using a view

{% highlight r %}
out <- sofa_view_search(dbname = "alm_db", design_name = "myview")
length(out$rows)  # 160 results
{% endhighlight %}



{% highlight text %}
[1] 161
{% endhighlight %}



{% highlight r %}
sapply(out$rows, function(x) x$value)[1:5]  # the values, just the API call URLs
{% endhighlight %}



{% highlight text %}
[1] "http://alm.plos.org/api/v3/articles"
[2] "http://alm.plos.org/api/v3/articles"
[3] "http://alm.plos.org/api/v3/articles"
[4] "http://alm.plos.org/api/v3/articles"
[5] "http://alm.plos.org/api/v3/articles"
{% endhighlight %}


***************

#### Delete the view

{% highlight r %}
sofa_view_del(dbname = "alm_db", design_name = "myview")
{% endhighlight %}



{% highlight text %}
[1] ""
{% endhighlight %}


***************

## What now? 

Well, if no one uses this, then probably nothing. Though, if people think this could be useful:

+ It would be cool to make easy hooks into any package making web calls to allow users to write data to CouchDB if they choose to, sort of like the example above with rplos.
+ Perhaps automate some of the setup for CouchDB for users, making system calls so they don't have to.
+ Performance: As shown above, simultaneously writing data to CouchDB takes longer than not doing so - removing this time difference will make writing to couch more palatable.

## What do you think?

What is your reaction to this post?  Do you have a need for this sort of tool?  Do you have similar use cases that could be addressed with `sofa`?

[couchriver]: https://github.com/elasticsearch/elasticsearch-river-couchdb/blob/master/README.md
[elastic]: http://www.elasticsearch.org/
[restapi]: http://docs.couchdb.org/en/latest/api-basics.html
[couch]: http://couchdb.apache.org/
[r4couch]: (https://github.com/wactbprot/R4CouchDB)
[postgres]: http://wiki.postgresql.org/wiki/What's_new_in_PostgreSQL_9.2#JSON_datatype

  </div>
  
  <div class="post">
    <h1>
      <a href="/2013/06/fylopic/">
        Fylopic, an R wrapper to Phylopic
      </a>
    </h1>

    <span class="post-date">01 Jun 2013</span>

    ## What is PhyloPic?

PhyloPic is an awesome new service - I'll let the creator, [Mike Keesey](http://tmkeesey.net/), explain what it is (paraphrasing here): 

> PhyloPic stores silhouette images of organisms, and each image is associated with taxonomic names, and stores the taxonomy of all taxa, allowing searching by taxonomic names. Anyone can submit silhouettes to PhyloPic. 

What is a silhouette?  It's like this:

![A silhouette](http://phylopic.org/assets/images/submissions/bedd622a-4de2-4067-8c70-4aa44326d229.128.png)

*by Gareth Monger*


What makes PhyloPic not just awesome, but super awesome? All or most images are licensed under [Creative Commons licenses](http://creativecommons.org/). This means you can use the silhouettes without having to ask or pay - just attribute. 

***************

## What is fylopic?

The idea behind Fylopic is to create modular bits and pieces (i.e., functions) to allow you to add silhouettes to not only ggplot2 plots, but base plots as well. That is, you can simply load fylopic in your R session, and add some silhouettes to your phylogeny, or your barchart, etc. - that is, `fylopic` is meant to be a helper in your workflow to add in silhouettes to visualizations. 

Some people prefer base plots while others prefer ggplot2 plots (me!), so it would be nice to have both options. Phylogenies at the moment render faster in base plots. I don't yet have implementations for base plots, but will come soon, or you can send a pull request to add it. 

One interesting use case could be to be able to get a set of silhouettes, then get a phylogeny for taxa associatd with the silhouettess using the NCBI taxonomy, but it's not easily available yet (though I may be able to use [Ben Morris' phylocommons](https://github.com/bendmorris/phylocommons) soon. This isn't doable yet, so in the example below the function `make_phylo` creates a phylogeny using `ape::rcoal`.

You could also do the reverse -> you have a phylogeny and then you could search Phylopic for silhouettes. 

***************

## Info

Check out the Phylopic website [here](http://phylopic.org/), and Phylopic API developer documentation [here](http://phylopic.org/api/). 

Also check out Ben Morris' Python wrapper to Phylopic [here](https://github.com/bendmorris/python-phylopic). 

***************

## What can you do with fylopic?

***************

#### Install fylopic

{% highlight r %}
install.packages("devtools")
library(devtools)
install_github("fylopic", "sckott")
{% endhighlight %}



{% highlight r %}
library(fylopic, quietly = TRUE)
{% endhighlight %}


***************

#### Plot a phylogeny with silhouettes at the tips

Here, I search for names based on keyword *Homo sapiens* - which returns many matche codes. With those results we search for any silhouettes associated with those codes. Then we download images. Finally, make a phylogeny with the silhouettes at the tips. Note that in this eample the phylogeny is just a random coalescent tree made using `ape::rcoal` - obviously, in the real world you'd want to do something more useful. 


{% highlight r %}
## search on Homo sapiens
searchres <- search_text(text = "Homo sapiens", options = "names")

### which returns UUIDs
searchres[1:3]
{% endhighlight %}



{% highlight text %}
[1] "74aea16b-666b-497a-b2cb-72201ad75a8e"
[2] "1ee65cf3-53db-4a52-9960-a9f7093d845d"
[3] "cc9ad8ee-3a82-4add-8d50-bc78f4ff6956"
{% endhighlight %}



{% highlight r %}

## search for images based on the UUIds
output <- search_images(uuid = searchres, options = c("pngFiles", "credit", 
    "canonicalName"))

### we got eight matches
output
{% endhighlight %}



{% highlight text %}
$`15444b9c-f17f-4d6e-89b5-5990096bcfb0`
$`15444b9c-f17f-4d6e-89b5-5990096bcfb0`$supertaxa
[1] "e547cd01-7dd1-495b-8239-52cf9971a609"
[2] "bd88f674-6976-4cb2-a46e-e6a12a8ba463"


$`fedf0e5f-f20a-442c-accf-eb84a3af8c6b`
$`fedf0e5f-f20a-442c-accf-eb84a3af8c6b`$supertaxa
[1] "e547cd01-7dd1-495b-8239-52cf9971a609"
[2] "bd88f674-6976-4cb2-a46e-e6a12a8ba463"


$`a88d3a4c-44d3-409e-87b6-516bd188c709`
$`a88d3a4c-44d3-409e-87b6-516bd188c709`$supertaxa
[1] "e547cd01-7dd1-495b-8239-52cf9971a609"
[2] "bd88f674-6976-4cb2-a46e-e6a12a8ba463"


$`d88164ec-3152-444b-b41c-4757a344a764`
$`d88164ec-3152-444b-b41c-4757a344a764`$supertaxa
[1] "9c6af553-390c-4bdd-baeb-6992cbc540b1"


$`da5eaeb7-1ed2-4b2e-ad4a-49993881d706`
$`da5eaeb7-1ed2-4b2e-ad4a-49993881d706`$supertaxa
[1] "9c6af553-390c-4bdd-baeb-6992cbc540b1"
{% endhighlight %}



{% highlight r %}

## download images
myobjs <- get_image(uuids = output, size = "128")

## make the phylogeny
make_phylo(pngobj = myobjs)
{% endhighlight %}

![center](/public/img/2013-06-01-fylopic/unnamed-chunk-1.png) 


***************

#### Plot a silhouette behind a plot

Notice in the below example that you can use normal `ggplot2` syntax, and simply add another layer (`add_phylopic` from `fylopic`) to the plot.


{% highlight r %}
library(ggplot2)
img <- get_image("27356f15-3cf8-47e8-ab41-71c6260b2724", size = "512")[[1]]
qplot(x = Sepal.Length, y = Sepal.Width, data = iris, geom = "point") + add_phylopic(img)
{% endhighlight %}

![center](/public/img/2013-06-01-fylopic/unnamed-chunk-2.png) 


***************

## What's next?

This is a side project, so if anyone has interest in helping please do contribute code, report bugs, request features, etc. 

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page26" class="older">Older</a>
  
  
    
      <a href="/page24" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
