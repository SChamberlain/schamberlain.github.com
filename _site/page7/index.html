<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://recology.info/">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/recology_"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2015/02/secure-elasticsearch/">
        note to self, secure elasticsearch
      </a>
    </h1>

    <span class="post-date">26 Feb 2015</span>

    Recently I spun up a box on a cloud hosting provider planning to make a tens of thousdands of queries to an Elasticsearch instance on the same box. I could have done this on my own machine, but didn't want to take up compute resources.

I installed R and Elasticsearch on the box, then went about doing my thang.

A day later when things were still running, the hosting provider sent me a message that apparently my box had been serving up a DDoS attack.

This was incredibly surprising, as I don't even know how to do such a thing.

After some digging it seems that the culprit was likely Elasticsearch, as a number of tutorials/blog posts state that Elaticsearch is insecure by default, so if it's exposed on a public port, someone can hack in. I had only used Elasticsearch locally on my own machine, so I hadn't thought about security. Here's a few resources for security help:

* [DigitalOcean tutorial no. 1][do1]
* [DigitalOcean tutorial no. 2][do2]
* [Blog post on securing ES][saskia]
* [SO answer on securing ES][so]

Trying to narrow down the various pieces of advice for securing Elasticsearch, here's a list:

* Use `iptables` (or rather [nftables][nftables]?) to firewall the box
* Whitelist certain trusted IPs 
* Use the [`elasticsearch-http-basic`][esbasic] plugin, adds basic username/pwd login
* Remove public access: use `network.bind_host: localhost` and `script.disable_dynamic: true` in the `elasticsearch.yml` config file [from][do1]

Elasticsearch provides a new feature for security that's built into Elasticsearch, [Shield](http://www.elasticsearch.org/overview/shield/), but I believe it's only available to enterprise customers. Boo. 

[do1]: https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-on-an-ubuntu-vps
[do2]: https://www.digitalocean.com/community/tutorials/elasticsearch-fluentd-and-kibana-open-source-log-search-and-visualization
[esbasic]: https://github.com/Asquera/elasticsearch-http-basic
[saskia]: http://saskia-vola.com/install-secure-elasticsearch-1-x-digital-ocean/
[so]: http://stackoverflow.com/questions/26006373/how-to-secure-a-digital-ocean-elasticsearch-cluster
[nftables]: http://en.wikipedia.org/wiki/Nftables

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/02/package-dev/">
        Package development
      </a>
    </h1>

    <span class="post-date">14 Feb 2015</span>

    Someone asked recently about tips for package development workflow to optimize a successful submission to CRAN.

The ultimate guide is probably [Hadley's book on package development](http://r-pkgs.had.co.nz/), but here's more of a bulleted list of some things I do.

## Use RStudio

Choice of text editor/IDE is always contentious, but for R package development, RStudio makes it so easy, including keyboard shortcuts for lots of steps that you need to make development faster. See the [cheatsheet](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts).

## Documentation and roxygen2

You can always write your manual files (`.Rd`) files by hand, but to avoid mistakes including missing and duplicate parameter definitions, and other things, simply write documentation alongside your code with [roxygen2](http://cran.r-project.org/web/packages/roxygen2/index.html). The RStudio IDE includes a keyboard shortcut (shift+cmd+D on Mac) to generate manual files from your roxygen documentation. 

When you run either `R CMD CHECK` in your terminal or `devtools::check()` or simply using keyboard shortcuts in RStudio, you may notice problems with documentation, upon which you can make fixes, quickly re-document the whole package, then run check again. Iterating on this process is very easy with RStudio keyboard shortcuts. 

## Examples

CRAN checks now actually run code examples wrapped in `\donttest`. So if you have examples that may throw warnings or errors on purpose or accident, make sure to wrap them in `\dontrun`. Ripley used to complain that at least some examples in the package should run on check, but I haven't heard this complaint in a while.

## First submission of the package?

If it is your first submission of the package:

* Include the sentence in your submission _I have read and agree to the the CRAN policies at http://cran.r-project.org/web/packages/policies.html_

## Code

CRAN maintainers generally don't look at code in my experience, but they may in the case of some examples or tests not passing on submission. 

## Tests

If you have tests in your package, and you should, think about whether your tests are likely to fail in some scenarios. For example, I mostly write packages that work with web APIs, all of which are not under my control, meaning they could fail at any time, causing tests to fail on CRAN (CRAN runs check once per day I think). 

If you do have tests may fail, think about ignoring tests all together on CRAN. If you do this, it's especially important to use continuous integration on your own. For example, use [Travis-CI](https://travis-ci.org/), which will run check on your package on each change. If you have a package that works with web APIs, it's important to check your package functionality even when you aren't changing it since the resource your package works with can change. So run tests e.g. once per day - you can [do something like we do using a bit of Ruby code](https://github.com/ropensci/travis-restarts).

## Continuous integration

I just talked about this above, but a few more thoughts. This is a relatively easy thing to do, its free, and at least I think it greatly pays off once set up. In addition, you can do more than just test code, and run checks. You can deploy artifacts to various places. That is, for example, you could at the end of a build on Travis-CI, push a binary of the package to Dropbox, or Amazon S3. A few good options that I've used:

* [Travis-CI](https://travis-ci.org/) ([R integration](https://github.com/craigcitro/r-travis)) - Linux/unix builds
* [Appveyor](http://www.appveyor.com/) ([R integration](https://github.com/krlmlr/r-appveyor)) - Windows builds

There are other options, but I haven't used them...

## DESCRIPTION file

In addition to following [CRAN's guidelines](http://cran.r-project.org/doc/manuals/R-exts.html#The-DESCRIPTION-file) (and search _description_ in the [CRAN policies](http://cran.r-project.org/web/packages/policies.html)), some tips for some of the parts of the file.

* Title: must be sentence case, no period at end
* Description: Don't use the phrase _This package_
* Watch out for _possibly mis-spelled words_ warnings on check. They will reject your package for very minor mis-spellings.

## Use cran-comments.md file

Hadley supports this in `devtools`. Put a file named `cran-comments.md` in the root of your package. In this file, include the comments you would submit for the package (e.g., I agree to cran policies...this package passed all checks...etc). Rembmer to put `cran-comments.md` in the `.Rbuildignore` file in the root of your package so that `R CMD CHECK` doesn't complain. When you use the `devtools::release()` function, it will look for this file, and automatically throw in your submission comments. Doing this and using `release()` means you don't have to worry about Brian Ripley complaining about rich text emails.

## CRAN policy changes 

If you're on Twitter, watch the `#rstats` hashtag to be more likely to notice any upcoming changes in package submission policies. Also can follow Dirk's [CRAN policy watch repo](https://github.com/eddelbuettel/crp). 

## Other things

* [Hilary Parker's blog post](http://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/)
* [Hadley's book on package development](http://r-pkgs.had.co.nz/)

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/01/httping/">
        httping - ping and time http requests
      </a>
    </h1>

    <span class="post-date">30 Jan 2015</span>

    I've been working on a little thing called `httping` - a small R package that started as a pkg to Ping urls and time requests. It's a port of the Ruby gem [httping](https://github.com/jpignata/httping). The `httr` package is in `Depends` in this package, so its functions can be called directly, without having to load `httr` explicitly yourself.

In addition to timing requests, I've been tinkering with how to make http requests, with curl options accepting and returning the same object so they can be chained together, and then that object passed to a http verb like `GET`. Maybe this is a bad idea, but maybe not.

## Installation

Install:

One non-CRAN dep (`httpcode`) needs to be installed first.


```r
install.packages("devtools")
devtools::install_github("sckott/httpcode")
devtools::install_github("sckott/httping")
```

Then load package


```r
library("httping")
```

## Time requests

The idea with `time()` is to provide easy to use and understand information on how long http requests take to run. You should be able to pass in any `httr` verbs (`GET()`, `POST()`, etc.) to `time()`. `time()` repeats whatever http request you pass to it by default 10 times, but you can set the number of times to repeat in the `count` parameter. In addition, the `flood` parameter controls whether there is a delay between requests or not, and `delay` controls length of the delay. 

A `GET` request


```r
GET("http://google.com") %>% time(count=3)
#> 29.392 kb - http://www.google.com/ code:200 time(ms):92.444
#> 29.392 kb - http://www.google.com/ code:200 time(ms):82.127
#> 29.392 kb - http://www.google.com/ code:200 time(ms):85.587
#> <http time>
#>   Avg. min (ms):  82.127
#>   Avg. max (ms):  92.444
#>   Avg. mean (ms): 86.71933
```

A `POST` request


```r
POST("http://httpbin.org/post", body = "A simple text string") %>% time(count=3)
#> 10.144 kb - http://httpbin.org/post code:200 time(ms):267.574
#> 10.144 kb - http://httpbin.org/post code:200 time(ms):113.309
#> 10.144 kb - http://httpbin.org/post code:200 time(ms):99.938
#> <http time>
#>   Avg. min (ms):  99.938
#>   Avg. max (ms):  267.574
#>   Avg. mean (ms): 160.2737
```

The return object is a list with slots for all the `httr` response objects, the times for each request, and the average times. The number of requests, and the delay between requests are included as attributes.


```r
res <- GET("http://google.com") %>% time(count=3)
#> 29.392 kb - http://www.google.com/ code:200 time(ms):82.086
#> 29.392 kb - http://www.google.com/ code:200 time(ms):78.15
#> 29.392 kb - http://www.google.com/ code:200 time(ms):79.763
```


```r
attributes(res)
#> $names
#> [1] "times"    "averages" "request" 
#> 
#> $count
#> [1] 3
#> 
#> $delay
#> [1] 0.5
#> 
#> $class
#> [1] "http_time"
```

Or print a summary of a response, gives more detail


```r
res %>% summary()
#> <http time, averages (min max mean)>
#>   Total (s):           78.15 82.086 79.99967
#>   Tedirect (s):        26.695 34.319 29.80633
#>   Namelookup time (s): 0.025 0.03 0.028
#>   Connect (s):         0.028 0.034 0.032
#>   Pretransfer (s):     0.069 0.081 0.07633333
#>   Starttransfer (s):   45.44 49.326 47.95867
```

Messages are printed using `cat`, so you can suppress those using `verbose=FALSE`, like


```r
GET("http://google.com") %>% time(count=3, verbose = FALSE)
#> <http time>
#>   Avg. min (ms):  86.12
#>   Avg. max (ms):  94.035
#>   Avg. mean (ms): 89.12467
```


## Ping an endpoint

The idea with `ping()` is to simply return the http status code along with a message describing what that code means. That's it.

This function is a bit different, accepts a url as first parameter, but can accept any args passed on to `httr` verb functions, like `GET`, `POST`,  etc.


```r
"http://google.com" %>% ping()
#> <http ping> 200
#>   Message: OK
#>   Description: Request fulfilled, document follows
```

Or pass in additional arguments to modify request


```r
"http://google.com" %>% ping(config=verbose())
#> -> GET / HTTP/1.1
#> -> User-Agent: curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1
#> -> Host: google.com
#> -> Accept-Encoding: gzip
...cutoff
```

## Even simpler verbs

`httr` is already easy, but `Get()`:

* Allows use of an intuitive chaining workflow
* Parses data for you using `httr` built in format guesser, which should work in most cases

A simple GET request


```r
"http://httpbin.org/get" %>%
  Get()
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

You can buid up options by calling functions


```r
"http://httpbin.org/get" %>%
  Progress() %>%
  Verbose()
#> <http request> 
#>   url: http://httpbin.org/get
#>   config: 
#> Config: 
#> List of 4
#>  $ noprogress      :FALSE
#>  $ progressfunction:function (...)  
#>  $ debugfunction   :function (...)  
#>  $ verbose         :TRUE
```

Then eventually execute the GET request


```r
"http://httpbin.org/get" %>%
  Verbose() %>%
  Progress() %>%
  Get()
#> -> GET /get HTTP/1.1
#> -> User-Agent: curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1
#> -> Host: httpbin.org
#> -> Accept-Encoding: gzip
#> -> Accept: application/json, text/xml, application/xml, */*
#> -> 
#> <- HTTP/1.1 200 OK
#> <- Server: nginx
#> <- Date: Fri, 30 Jan 2015 17:38:58 GMT
#> <- Content-Type: application/json
#> <- Content-Length: 288
#> <- Connection: keep-alive
#> <- Access-Control-Allow-Origin: *
#> <- Access-Control-Allow-Credentials: true
#> <- 
#>   |=======================================| 100%
#> 
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.5 httr/0.6.1"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
#> 
```

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page8" class="older">Older</a>
  
  
    
      <a href="/page6" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
