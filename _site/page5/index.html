<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://localhost:4000">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2015/11/crossref-clients/">
        Crossref programmatic clients
      </a>
    </h1>

    <span class="post-date">30 Nov 2015</span>

    I gave two talks recently at the annual [Crossref meeting][crmeeting], one of which was a somewhat technical overview of programmatic clients for Crossref APIs. Check out the talk [here](https://crossref.wistia.com/medias/8rh0jm5eda). I talked about the motivation for working with Crossref data by writing code/etc. rather than going the GUI route, then went over the various clients, with brief examples.

We (rOpenSci) have been working on the R client [rcrossref][rcrossref] for a while now, but I'm also working on the Python and Ruby clients for Crossref. In addition, the Ruby client has a CLI client inside. The Javascript client is worked on independently by [ScienceAI](https://science.ai/).

The R, Ruby, and Python clients are useable but not feature complete yet, and would benefit from lots of users surfacing bugs and highlighting nice to have features.

The main Crossref API used in all the clients is documented at [api.crossref.org](https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md).

I've tried to make the APIs similar-ish across clients. Functions in each client match the main Crossref search API (api.crossref.org) routes:

* `/works`
* `/members`
* `/funders`
* `/journals`
* `/types`
* `/licenses`

Other methods in all three clients:

* Get DOI minting agency
  * Uses api.crossref.org API
* Get random DOIs
  * Uses api.crossref.org API
* Content negotiation
  * Documented at [http://www.crosscite.org/cn](http://www.crosscite.org/cn)
* Get full text
  * other clients in each language will focus on this use case
* Get citation count
  * Uses service at [http://www.crossref.org/openurl](http://www.crossref.org/openurl) - though this functionality may be in the api.crossref.org API at some point

The following shows how to install, and then examples from each client for a few use cases.

## Installation

### Python

```sh
pip install habanero
```

### Ruby

```sh
gem install serrano
```

### R

Inside R:

```R
install.packages("rcrossref")
```

### Javascript

```sh
npm install crossref
```

I won't do any examples with the js library, as I don't maintain it.

## Use case: get ORCID IDs for authors

Python

```python
from habanero import Crossref
cr = Crossref()
res = cr.works(filter = {'has_orcid': True}, limit = 10)
res2 = [ [ z.get('ORCID') for z in x['author'] ] for x in res.result['message']['items'] ]
filter(None, reduce(lambda x, y: x+y, res2))
```

```python
[u'http://orcid.org/0000-0003-4087-8021',
 u'http://orcid.org/0000-0002-2076-5452',
 u'http://orcid.org/0000-0003-4087-8021',
 u'http://orcid.org/0000-0002-2076-5452',
 u'http://orcid.org/0000-0003-1710-1580',
 u'http://orcid.org/0000-0003-1710-1580',
 u'http://orcid.org/0000-0003-4637-238X',
 u'http://orcid.org/0000-0003-4637-238X',
 u'http://orcid.org/0000-0003-4637-238X',
 u'http://orcid.org/0000-0003-4637-238X',
 u'http://orcid.org/0000-0003-4637-238X',
 u'http://orcid.org/0000-0003-2510-4271']
```

Ruby

```ruby
require 'serrano'
res = Serrano.works(filter: {'has_orcid': true}, limit: 10)
res2 = res['message']['items'].collect { |x| x['author'].collect { |z| z['ORCID'] } }
res2.flatten.compact
```


```ruby
=> ["http://orcid.org/0000-0003-4087-8021",
 "http://orcid.org/0000-0002-2076-5452",
 "http://orcid.org/0000-0003-4087-8021",
 "http://orcid.org/0000-0002-2076-5452",
 "http://orcid.org/0000-0003-1710-1580",
 "http://orcid.org/0000-0003-1710-1580",
 "http://orcid.org/0000-0003-4637-238X",
 "http://orcid.org/0000-0003-4637-238X",
 "http://orcid.org/0000-0003-4637-238X",
 "http://orcid.org/0000-0003-4637-238X",
 "http://orcid.org/0000-0003-4637-238X",
 "http://orcid.org/0000-0003-2510-4271"]
```

R

```R
library("rcrossref")
res <- cr_works(filter=c(has_orcid=TRUE), limit = 10)
orcids <- unlist(lapply(res$data$author, function(z) z$ORCID))
Filter(function(x) !is.na(x), orcids)
```

```R
 [1] "http://orcid.org/0000-0003-4087-8021"
 [2] "http://orcid.org/0000-0002-2076-5452"
 [3] "http://orcid.org/0000-0003-4087-8021"
 [4] "http://orcid.org/0000-0002-2076-5452"
 [5] "http://orcid.org/0000-0003-1710-1580"
 [6] "http://orcid.org/0000-0003-1710-1580"
 [7] "http://orcid.org/0000-0003-4637-238X"
 [8] "http://orcid.org/0000-0003-4637-238X"
 [9] "http://orcid.org/0000-0003-4637-238X"
[10] "http://orcid.org/0000-0003-4637-238X"
[11] "http://orcid.org/0000-0003-4637-238X"
[12] "http://orcid.org/0000-0003-2510-4271"
```

CLI

```sh
serrano works --filter=has_orcid:true --json --limit=12 | jq '.message.items[].author[].ORCID | select(. != null)'
```

```sh
"http://orcid.org/0000-0003-4087-8021"
"http://orcid.org/0000-0002-2076-5452"
"http://orcid.org/0000-0003-4087-8021"
"http://orcid.org/0000-0002-2076-5452"
"http://orcid.org/0000-0003-1710-1580"
"http://orcid.org/0000-0003-1710-1580"
"http://orcid.org/0000-0003-4637-238X"
"http://orcid.org/0000-0003-4637-238X"
"http://orcid.org/0000-0003-4637-238X"
"http://orcid.org/0000-0003-4637-238X"
"http://orcid.org/0000-0003-4637-238X"
"http://orcid.org/0000-0003-2510-4271"
"http://orcid.org/0000-0001-9408-8207"
"http://orcid.org/0000-0002-2076-5452"
```

## Use case: content negotation

Python

```python
from habanero import cn
cn.content_negotiation(ids = '10.1126/science.169.3946.635', format = "text")
```


```python
u'Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635\xe2\x80\x93641. doi:10.1126/science.169.3946.635\n'
```

Ruby

```ruby
require 'serrano'
Serrano.content_negotiation(ids: '10.1126/science.169.3946.635', format: "text")
```

```ruby
=> ["Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635\xE2\x80\x93641. doi:10.1126/science.169.3946.635\n"]
```

R

```r
library("rcrossref")
cr_cn(dois="10.1126/science.169.3946.635", "text")
```

```r
[1] "Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635–641. doi:10.1126/science.169.3946.635"
```

CLI

```sh
serrano contneg 10.1890/13-0590.1 --format=text
```

```sh
Murtaugh, P. A. (2014).  In defense of P values . Ecology, 95(3), 611–617. doi:10.1890/13-0590.1
```

## More

There are definitely issues with data in the Crossref search API, some of which I cover in my talks. However, it is still the best place to go for scholarly metadata.

Let us know of other use cases - there are others not covered here for brevity sake.

There are lots of examples in the docs for each client. If you can think of any doc improvements file an issue.

If you find any bugs, please do file an issue.

[rcrossref]: https://github.com/ropensci/rcrossref
[habanero]: https://github.com/sckott/habanero
[serrano]: https://github.com/sckott/serrano
[crmeeting]: http://www.crossref.org/annualmeeting/agenda.html

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/11/pygbif/">
        pygbif - GBIF client for Python
      </a>
    </h1>

    <span class="post-date">12 Nov 2015</span>

    I maintain an R client for the GBIF API, at [rgbif][rgbif]. Been working on it for a few years, and recently been thinking that there should be a nice low level client for Python as well. I didn't see one searching Github, etc. so I started working on one recently: [pygbif][pygbif]

It's up on [pypi][pypi].

There's not much in `pygbif` yet - I wanted to get something up to start getting some users to more quickly make the library useful to people.

There's three modules, with a few methods each:

* species
    * `name_backbone()`
    * `name_suggest()`
* registry
    * `nodes()`
    * `dataset_metrics()`
    * `datasets()`
* occurrences
    * `search()`
    * `get()`
    * `get_verbatim()`
    * `get_fragment()`
    * `count()`
    * `count_basisofrecord()`
    * `count_year()`
    * `count_datasets()`
    * `count_countries()`
    * `count_publishingcountries()`
    * `count_schema()`

Here's a quick intro ([in a Jupyter notebook][notebook]):

### Install

```python
pip install pygbif
```

### Registry/datasets

```python
from pygbif import registry
registry.dataset_metrics(uuid='3f8a1297-3259-4700-91fc-acc4170b27ce')
```

```python
{u'colCoveragePct': 79,
 u'colMatchingCount': 24335,
 u'countByConstituent': {},
 u'countByIssue': {u'BACKBONE_MATCH_FUZZY': 573,
  u'BACKBONE_MATCH_NONE': 1306,
  u'VERNACULAR_NAME_INVALID': 7777},
 u'countByKingdom': {u'ANIMALIA': 30,
  u'FUNGI': 3,
  u'INCERTAE_SEDIS': 26,
  u'PLANTAE': 10997,
  u'PROTOZOA': 1},
 ...
}
```

### Taxonomic names

```python
from pygbif import species
species.name_suggest(q='Puma concolor', limit = 1)
```

```python
{'data': [{u'canonicalName': u'Puma concolor',
   u'class': u'Mammalia',
   u'classKey': 359,
   u'family': u'Felidae',
   u'familyKey': 9703,
   u'genus': u'Puma',
   u'genusKey': 2435098,
   u'key': 2435099,
   u'kingdom': u'Animalia',
   u'kingdomKey': 1,
   u'nubKey': 2435099,
   u'order': u'Carnivora',
   u'orderKey': 732,
   u'parent': u'Puma',
   u'parentKey': 2435098,
   u'phylum': u'Chordata',
   u'phylumKey': 44,
   u'rank': u'SPECIES',
   u'species': u'Puma concolor',
   u'speciesKey': 2435099}],
 'hierarchy': [{u'1': u'Animalia',
   u'2435098': u'Puma',
   u'359': u'Mammalia',
   u'44': u'Chordata',
   u'732': u'Carnivora',
   u'9703': u'Felidae'}]}
```

### Occurrence data

Search

```python
from pygbif import occurrences
res = occurrences.search(taxonKey = 3329049, limit = 10)
[ x['phylum'] for x in res['results'] ]
```

```python
[u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota',
 u'Basidiomycota']
```

Fetch specific occurrences

```python
occurrences.get(key = 252408386)
```

```python
{u'basisOfRecord': u'OBSERVATION',
 u'catalogNumber': u'70875196',
 u'collectionCode': u'7472',
 u'continent': u'EUROPE',
 u'country': u'United Kingdom',
 u'countryCode': u'GB',
 u'datasetKey': u'26a49731-9457-45b2-9105-1b96063deb26',
 u'day': 30,
...
}
```

Occurrence counts API

```python
occurrences.count(isGeoreferenced = True)
```

```python
500283031
```

### feedback

Would love any feedback...

[rgbif]: https://github.com/ropensci/rgbif
[pygbif]: https://github.com/sckott/pygbif
[pypi]: https://pypi.python.org/pypi/pygbif/0.1.1
[notebook]: https://github.com/sckott/pygbif/blob/master/demos/pygbif-intro.ipynb

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/10/noaa-isd/">
        noaa - Integrated Surface Database data
      </a>
    </h1>

    <span class="post-date">21 Oct 2015</span>

    I've recently made some improvements to the functions that work with ISD 
(Integrated Surface Database) data.

__isd data__

* The `isd()` function now caches more intelligently. We now cache using 
`.rds` files via `saveRDS`/`readRDS`, whereas we used to use `.csv` files, 
which take up much more disk space, and we have to worry about not changing 
data formats on reading data back into an R session. This has the downside
that you can't just go directly to open up a cached file in your favorite 
spreadsheet viewer, but you can do that manually after reading in to R. 
* In addition, `isd()` now has a function `cleanup`, if `TRUE` after 
downloading the data file from NOAA's ftp server and processing, we delete 
the file. That's fine since we have the cached processed file. But you 
can choose not to cleanup the original data files.
* Data processing in `isd()` is improved as well. We convert key variables
to appropriate classes to be more useful. 

__isd stations__

* In `isd_stations()`, there's now a cached version of the station data in 
the package, or you can get optionally get fresh station data from NOAA's 
FTP server.
* There's a new function `isd_stations_search()` that uses the station data
to allow you to search for stations via either:
  * A bounding box
  * Radius froma point

## Install

For examples below, you'll need the development version:


```r
devtools::install_github("ropensci/rnoaa")
```

Load `rnoaa`


```r
library("rnoaa")
```

## ISD stations 

### Get stations

There's a cached version of the station data in the package, or you can get fresh
station data from NOAA's FTP server.


```r
stations <- isd_stations()
head(stations)
#>   usaf  wban station_name ctry state icao lat lon elev_m    begin      end
#> 1 7005 99999   CWOS 07005                  NA  NA     NA 20120127 20120127
#> 2 7011 99999   CWOS 07011                  NA  NA     NA 20111025 20121129
#> 3 7018 99999   WXPOD 7018                   0   0   7018 20110309 20130730
#> 4 7025 99999   CWOS 07025                  NA  NA     NA 20120127 20120127
#> 5 7026 99999   WXPOD 7026   AF              0   0   7026 20120713 20141120
#> 6 7034 99999   CWOS 07034                  NA  NA     NA 20121024 20121106
```

### Filter and visualize stations

In addition to getting the entire station data.frame, you can also search for stations,
either with a bounding box or within a radius from a point. First, the bounding box


```r
bbox <- c(-125.0, 38.4, -121.8, 40.9)
out <- isd_stations_search(bbox = bbox)
head(out)
#>     usaf  wban                          station_name ctry state icao
#> 1 720193 99999 LONNIE POOL FLD / WEAVERVILLE AIRPORT   US    CA KO54
#> 2 724834 99999                        POINT CABRILLO   US    CA     
#> 3 724953 99999                              RIO NIDO   US    CA     
#> 4 724957 23213                 SONOMA COUNTY AIRPORT   US    CA KSTS
#> 5 724957 99999                  C M SCHULZ SONOMA CO   US    CA KSTS
#> 6 724970 99999                  CHICO CALIFORNIA MAP   US    CA  CIC
#>   elev_m    begin      end      lon    lat
#> 1  716.0 20101030 20150831 -122.922 40.747
#> 2   20.0 19810906 19871007 -123.820 39.350
#> 3 -999.0 19891111 19900303 -122.917 38.517
#> 4   34.8 20000101 20150831 -122.810 38.504
#> 5   38.0 19430404 19991231 -122.817 38.517
#> 6   69.0 19420506 19760305 -121.850 39.783
```

Where is the bounding box? (you'll need [lawn](https://cran.rstudio.com/web/packages/lawn/), or you can vizualize some other way)


```r
library("lawn")
lawn::lawn_bbox_polygon(bbox) %>% view
```

![plot1](/public/img/2015-10-21-noaa-isd/bbox_area.png)

Vizualize station subset - yep, looks right


```r
library("leaflet")
leaflet(data = out) %>%
  addTiles() %>%
  addCircles()
```

![plot1](/public/img/2015-10-21-noaa-isd/bbox_result.png)

Next, search with a lat/lon coordinate, with a radius. That is, we search for stations
within X km from the coordinate.


```r
out <- isd_stations_search(lat = 38.4, lon = -123, radius = 250)
head(out)
#>     usaf  wban             station_name ctry state icao elev_m    begin
#> 1 690070 93217            FRITZSCHE AAF   US    CA KOAR   43.0 19600404
#> 2 720267 23224 AUBURN MUNICIPAL AIRPORT   US    CA KAUN  466.7 20060101
#> 3 720267 99999         AUBURN MUNICIPAL   US    CA KAUN  468.0 20040525
#> 4 720406 99999      GNOSS FIELD AIRPORT   US    CA KDVO    0.6 20071114
#> 5 720576   174       UNIVERSITY AIRPORT   US    CA KEDU   21.0 20130101
#> 6 720576 99999                    DAVIS   US    CA KEDU   21.0 20080721
#>        end      lon    lat
#> 1 19930831 -121.767 36.683
#> 2 20150831 -121.082 38.955
#> 3 20051231 -121.082 38.955
#> 4 20150831 -122.550 38.150
#> 5 20150831 -121.783 38.533
#> 6 20121231 -121.783 38.533
```

Again, compare search area to stations found

_search area_


```r
pt <- lawn::lawn_point(c(-123, 38.4))
lawn::lawn_buffer(pt, dist = 250) %>% view
```

![plot1](/public/img/2015-10-21-noaa-isd/circle_radius.png)

_stations found_


```r
leaflet(data = out) %>%
  addTiles() %>%
  addCircles()
```

![plot1](/public/img/2015-10-21-noaa-isd/lastplot.png)



## ISD data

### Get ISD data

Here, I get data for four stations.


```r
res1 <- isd(usaf="011690", wban="99999", year=1993)
res2 <- isd(usaf="172007", wban="99999", year=2015)
res3 <- isd(usaf="702700", wban="00489", year=2015)
res4 <- isd(usaf="109711", wban=99999, year=1970)
```

Then, combine data, with `rnoaa:::rbind.isd()`


```r
res_all <- rbind(res1, res2, res3, res4)
```

Add date time


```r
library("lubridate")
res_all$date_time <- ymd_hm(
  sprintf("%s %s", as.character(res_all$date), res_all$time)
)
```

Remove 999's (NOAA's way to indicate missing/no data)


```r
library("dplyr")
res_all <- res_all %>% filter(temperature < 900)
```

### Visualize ISD data


```r
library("ggplot2")
ggplot(res_all, aes(date_time, temperature)) +
  geom_line() + 
  facet_wrap(~usaf_station, scales = "free_x")
```

![img](/public/img/2015-10-21-noaa-isd/unnamed-chunk-12-1.png) 

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page6" class="older">Older</a>
  
  
    
      <a href="/page4" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
