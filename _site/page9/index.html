<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://localhost:4000">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2015/04/geojson-io/">
        geojsonio - a new package to do geojson things
      </a>
    </h1>

    <span class="post-date">30 Apr 2015</span>

    `geojsonio` converts geographic data to GeoJSON and TopoJSON formats - though the focus is mostly on GeoJSON 

For those not familiar GeoJSON it is a format for encoding a variety of geographic data structures. GeoJSON supports the following geometry types: Point, LineString, Polygon, MultiPoint, MultiLineString, MultiPolygon, and GeometryCollection. These geometry types are also found in [well known text (WKT)](http://en.wikipedia.org/wiki/Well-known_text), and have equivalents in R's spatial classes. Read the [spec](http://geojson.org/geojson-spec.html) for more detailed information. 

Other great geojson resources:

* GeoJSON lint - lint your geojson - [http://geojsonlint.com/](http://geojsonlint.com/)
* GeoJSON.io - make maps with geojson input or draw maps and get geojson - [http://geojson.io/](http://geojson.io/)

Functions in this package are organized first around what you're working with or want to get, geojson or topojson, then convert to or read from various formats:

* `geojson_list()` - convert to GeoJSON as R list format
* `geojson_json()` - convert to GeoJSON as json
* `geojson_read()`/`topojson_read()` - read a GeoJSON/TopoJSON file from file path or URL
* `geojson_write()` - write a GeoJSON file locally (no write TopoJSON yet)

Each of the above functions have methods for various objects/classes, including `numeric`, `data.frame`, `list`, `SpatialPolygons`, `SpatialLines`, `SpatialPoints`, etc. (including the classes in `rgeos`)

Use cases for this package include (but not limited to, obs.) the following:

* Get data in GeoJSON json format, and you want to get it into a list in R.
* Get data into GeoJSON format to use downstream to make a interactive map
  * in R (e.g., with [leaflet](https://github.com/rstudio/leaflet))
  * or in another context (e.g., using javascript with mapbox/leaflet)
* Data is in a data.frame/matrix/list and you want to make GeoJSON format data.
* Data is in one of the many spatial classes (e.g., `SpatialPoints`) and you want GeoJSON
* You need to add styling to your data - can do with this package for certain data types.
* You want to check that your GeoJSON data is valid - two ways to do it in geojsonio.
* Combine objects together (e.g., a point and a line), either from two `geo_list` objects, or two `json` objects. See `?geojson-add`

## Install

See the github repo for notes about dependencies [https://github.com/ropensci/geojsonio#install](https://github.com/ropensci/geojsonio#install). 

CRAN version or the dev version from GitHub


```r
install.packages("geojsonio")
devtools::install_github("sckott/geojsonio")
```


```r
library("geojsonio")
```

## GeoJSON

### Convert various formats to geojson

From a `numeric` vector of length 2

as _json_


```r
geojson_json(c(32.45, -99.74))
#> {"type":"FeatureCollection","features":[{"type":"Feature","geometry":{"type":"Point","coordinates":[32.45,-99.74]},"properties":{}}]}
```

as a __list__


```r
geojson_list(c(32.45, -99.74))
#> $type
#> [1] "FeatureCollection"
#> 
#> $features
#> $features[[1]]
#> $features[[1]]$type
#> [1] "Feature"
#> 
#> $features[[1]]$geometry
#> $features[[1]]$geometry$type
...
```

From a `data.frame`

as __json__


```r
geojson_json(us_cities[1:2, ], lat = 'lat', lon = 'long')
#> {"type":"FeatureCollection","features":[{"type":"Feature","geometry":{"type":"Point","coordinates":[-99.74,32.45]},"properties":{"name":"Abilene TX","country.etc":"TX","pop":"113888","capital":"0"}},{"type":"Feature","geometry":{"type":"Point","coordinates":[-81.52,41.08]},"properties":{"name":"Akron OH","country.etc":"OH","pop":"206634","capital":"0"}}]}
```

as a __list__


```r
geojson_list(us_cities[1:2, ], lat = 'lat', lon = 'long')
#> $type
#> [1] "FeatureCollection"
#> 
#> $features
#> $features[[1]]
#> $features[[1]]$type
#> [1] "Feature"
#> 
#> $features[[1]]$geometry
#> $features[[1]]$geometry$type
...
```

From `SpatialPolygons` class


```r
library('sp')
poly1 <- Polygons(list(Polygon(cbind(c(-100, -90, -85, -100),
  c(40, 50, 45, 40)))), "1")
poly2 <- Polygons(list(Polygon(cbind(c(-90, -80, -75, -90),
  c(30, 40, 35, 30)))), "2")
(sp_poly <- SpatialPolygons(list(poly1, poly2), 1:2))
#> An object of class "SpatialPolygons"
#> Slot "polygons":
#> [[1]]
#> An object of class "Polygons"
#> Slot "Polygons":
#> [[1]]
#> An object of class "Polygon"
#> Slot "labpt":
#> [1] -91.66667  45.00000
#> 
...
```

to __json__


```r
geojson_json(sp_poly)
#> {"type":"FeatureCollection","features":[{"type":"Feature","id":1,"properties":{"dummy":0},"geometry":{"type":"Polygon","coordinates":[[[-100,40],[-90,50],[-85,45],[-100,40]]]}},{"type":"Feature","id":2,"properties":{"dummy":0},"geometry":{"type":"Polygon","coordinates":[[[-90,30],[-80,40],[-75,35],[-90,30]]]}}]}
```

to a __list__


```r
geojson_list(sp_poly)
#> $type
#> [1] "FeatureCollection"
#> 
#> $features
#> $features[[1]]
#> $features[[1]]$type
#> [1] "Feature"
#> 
#> $features[[1]]$id
#> [1] 1
...
```

From `SpatialPoints` class


```r
x <- c(1, 2, 3, 4, 5)
y <- c(3, 2, 5, 1, 4)
(s <- SpatialPoints(cbind(x, y)))
#> SpatialPoints:
#>      x y
#> [1,] 1 3
#> [2,] 2 2
#> [3,] 3 5
#> [4,] 4 1
#> [5,] 5 4
#> Coordinate Reference System (CRS) arguments: NA
```

to __json__


```r
geojson_json(s)
#> {"type":"FeatureCollection","features":[{"type":"Feature","id":1,"properties":{"dat":1},"geometry":{"type":"Point","coordinates":[1,3]}},{"type":"Feature","id":2,"properties":{"dat":2},"geometry":{"type":"Point","coordinates":[2,2]}},{"type":"Feature","id":3,"properties":{"dat":3},"geometry":{"type":"Point","coordinates":[3,5]}},{"type":"Feature","id":4,"properties":{"dat":4},"geometry":{"type":"Point","coordinates":[4,1]}},{"type":"Feature","id":5,"properties":{"dat":5},"geometry":{"type":"Point","coordinates":[5,4]}}]}
```

to a __list__


```r
geojson_list(s)
#> $type
#> [1] "FeatureCollection"
#> 
#> $features
#> $features[[1]]
#> $features[[1]]$type
#> [1] "Feature"
#> 
#> $features[[1]]$id
#> [1] 1
...
```

### Combine objects

`geo_list` + `geo_list`

> Note: `geo_list` is the output type from `geojson_list()`, it's just a list with a class attached so we know it's geojson :)


```r
vec <- c(-99.74, 32.45)
a <- geojson_list(vec)
vecs <- list(c(100.0, 0.0), c(101.0, 0.0), c(100.0, 0.0))
b <- geojson_list(vecs, geometry = "polygon")
a + b
#> $type
#> [1] "FeatureCollection"
#> 
#> $features
#> $features[[1]]
#> $features[[1]]$type
#> [1] "Feature"
#> 
#> $features[[1]]$geometry
#> $features[[1]]$geometry$type
...
```

`json` + `json`


```r
c <- geojson_json(c(-99.74, 32.45))
vecs <- list(c(100.0, 0.0), c(101.0, 0.0), c(101.0, 1.0), c(100.0, 1.0), c(100.0, 0.0))
d <- geojson_json(vecs, geometry = "polygon")
c + d
#> {"type":"FeatureCollection","features":[{"type":"Feature","geometry":{"type":"Point","coordinates":[-99.74,32.45]},"properties":{}},{"type":"Feature","geometry":{"type":"Polygon","coordinates":[[[100,0],[101,0],[101,1],[100,1],[100,0]]]},"properties":[]}]}
```

### Write geojson


```r
geojson_write(us_cities[1:2, ], lat = 'lat', lon = 'long')
#> <geojson>
#>   Path:       myfile.geojson
#>   From class: data.frame
```

## Topojson

In the current version of this package you can read topojson. Writing topojson was in this package, but is gone for now - will come back later as in interface to [topojson](https://github.com/mbostock/topojson) via [V8](https://github.com/jeroenooms/V8).

Read from a file


```r
file <- system.file("examples", "us_states.topojson", package = "geojsonio")
out <- topojson_read(file)
```

Read from a URL


```r
url <- "https://raw.githubusercontent.com/shawnbot/d3-cartogram/master/data/us-states.topojson"
out <- topojson_read(url)
```

## Lint geojson

There are two ways to do this in this package. 

### lint, locally

Uses the javascript library [geojsonhint](https://github.com/mapbox/geojsonhint) from Mapbox. We're running this locally via the [V8](http://cran.rstudio.com/web/packages/V8/) package.

Good


```r
lint('{"type": "Point", "coordinates": [-100, 80]}')
#> [1] "valid"
```

Bad


```r
lint('{"type": "Rhombus", "coordinates": [[1, 2], [3, 4], [5, 6]]}')
#> $message
#> [1] "The type Rhombus is unknown"
#> 
#> $line
#> [1] 1
```

### validate, with a web service

Uses the web service at [http://geojsonlint.com/](http://geojsonlint.com/)

Good


```r
validate('{"type": "Point", "coordinates": [-100, 80]}')
#> $status
#> [1] "ok"
```

Bad


```r
validate('{"type": "Rhombus", "coordinates": [[1, 2], [3, 4], [5, 6]]}')
#> $message
#> [1] "\"Rhombus\" is not a valid GeoJSON type."
#> 
#> $status
#> [1] "error"
```

## To do

* I'd like to replace `rgdal` with javascript libraries to read from various file types (kml, shp, etc.) and convert to geojson. This is [in development](https://github.com/ropensci/geojsonio/tree/js), and will come in the next version of this package most likely. This should make installation a bit easier as we won't have to depend on `rgdal` and `GDAL`
* Performance improvements. Some operations already use the gdal or geos C libraries, so are quite fast, though the round trip to disk and back does take significant time. I'd like to speed this up. 
* More input types. We already have operations (json, list, etc.) for lots of input types (data.frame, list, sp classes), but likely there will be more added. 
* Most likely add functions `topojson_list()`, `topojson_json()`

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/04/the-new-way/">
        the new way - httsnap
      </a>
    </h1>

    <span class="post-date">29 Apr 2015</span>

    Inspired by `httpie`, a Python command line client as a sort of drop in replacement for `curl`, I am playing around with something similar-ish in R, at least in spirit. I started a little R pkg called `httsnap` with the following ideas:

* The web is increasingly a JSON world, so set `content-type` and `accept` headers to `applications/json` by default 
* The workflow follows logically, or at least should, from, _hey, I got this url_, to _i need to add some options_, to _execute request_
* Whenever possible, transform output to data.frame's - facilitating downstream manipulation via `dplyr`, etc.
* Do `GET` requests by default. Specify a different type if you don't want `GET`. Some functionality does GET by default, though in some cases you need to specify GET
* You can use non-standard evaluation to easily pass in query parameters without worrying about `&`'s, URL escaping, etc. (see `Query()`)
* Same for body params (see `Body()`)

## Install

Install and load `httsnap`


```r
devtools::install_github("sckott/httsnap")
```


```r
library("httsnap")
library("dplyr")
```

## Functions so far

* `Get` - GET request
* `Query` - add query parameters
* `Authenticate` - add authentication details
* `Progress` - add progress bar
* `Timeout` - add a timeout
* `User_agent` - add a user agent
* `Verbose` - give verbose output
* `Body` - add a body
* `h` - add headers by key-value pair

These are named to avoid conflict with `httr`

## Intro

A simple `GET` request


```r
"http://httpbin.org/get" %>%
  Get()
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

You'll notice that `Get()` doesn't just get the response, but also checks for whether it was a good response (the HTTP status code), and extracts the data. 

Or you can just pass the URL into the function itself


```r
Get("http://httpbin.org/get")
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

You can buid up options by calling functions via pipes, and see what the options look like


```r
"http://httpbin.org/get" %>%
  Progress() %>%
  Verbose()
#> <http request> 
#>   url: http://httpbin.org/get
#>   config: 
#> Config: 
#> List of 4
#>  $ noprogress      :FALSE
#>  $ progressfunction:function (...)  
#>  $ debugfunction   :function (...)  
#>  $ verbose         :TRUE
```

Then execute the GET request when you're ready


```r
"http://httpbin.org/get" %>%
  Progress() %>%
  Verbose() %>%
  Get()
#> $args
#> named list()
#> 
#> $headers
#> $headers$Accept
#> [1] "application/json, text/xml, application/xml, */*"
#> 
#> $headers$`Accept-Encoding`
#> [1] "gzip"
#> 
#> $headers$Host
#> [1] "httpbin.org"
#> 
#> $headers$`User-Agent`
#> [1] "curl/7.37.1 Rcurl/1.95.4.1 httr/0.6.1 httsnap/0.0.2.99"
#> 
#> 
#> $origin
#> [1] "24.21.209.71"
#> 
#> $url
#> [1] "http://httpbin.org/get"
```

## Example 1

Get scholarly article metadata from the Crossref API


```r
"http://api.crossref.org/works" %>%
  Query(query = "ecology") %>% 
  .$message %>% 
  .$items %>% 
  select(DOI, title, publisher)
#>                            DOI                title
#> 1          10.4996/fireecology         Fire Ecology
#> 2              10.5402/ecology         ISRN Ecology
#> 3                 10.1155/8641         ISRN Ecology
#> 4      10.1111/(issn)1526-100x  Restoration Ecology
#> 5        10.1007/248.1432-184x    Microbial Ecology
#> 6      10.1007/10144.1438-390x   Population Ecology
#> 7      10.1007/10452.1573-5125      Aquatic Ecology
#> 8      10.1007/10682.1573-8477 Evolutionary Ecology
#> 9      10.1007/10745.1572-9915        Human Ecology
#> 10     10.1007/10980.1572-9761    Landscape Ecology
#> 11     10.1007/11258.1573-5052        Plant Ecology
#> 12     10.1007/12080.1874-1746  Theoretical Ecology
#> 13     10.1111/(issn)1442-9993      Austral Ecology
#> 14     10.1111/(issn)1439-0485       Marine Ecology
#> 15     10.1111/(issn)1365-2435   Functional Ecology
#> 16     10.1111/(issn)1365-294x    Molecular Ecology
#> 17     10.1111/(issn)1461-0248      Ecology Letters
#> 18   10.1002/9780470979365.ch7  Behavioural Ecology
#> 19 10.1111/fec.2007.21.issue-5                     
#> 20     10.1111/rec.0.0.issue-0                     
#>                            publisher
#> 1       Association for Fire Ecology
#> 2     Hindawi Publishing Corporation
#> 3     Hindawi Publishing Corporation
#> 4                    Wiley-Blackwell
#> 5  Springer Science + Business Media
#> 6  Springer Science + Business Media
#> 7  Springer Science + Business Media
#> 8  Springer Science + Business Media
#> 9  Springer Science + Business Media
#> 10 Springer Science + Business Media
#> 11 Springer Science + Business Media
#> 12 Springer Science + Business Media
#> 13                   Wiley-Blackwell
#> 14                   Wiley-Blackwell
#> 15                   Wiley-Blackwell
#> 16                   Wiley-Blackwell
#> 17                   Wiley-Blackwell
#> 18                   Wiley-Blackwell
#> 19                   Wiley-Blackwell
#> 20                   Wiley-Blackwell
```

## Example 2

Get Public Library of Science article metadata via their API, make a histogram of number of tweets for each article


```r
"http://api.plos.org/search" %>%
  Query(q = "*:*", wt = "json", rows = 100, 
        fl = "id,journal,alm_twitterCount",  
        fq = 'alm_twitterCount:[100 TO 10000]') %>% 
  .$response %>% 
  .$docs %>% 
  .$alm_twitterCount %>% 
  hist()
```
 
![image](/public/img/2015-04-29-the-new-way/unnamed-chunk-9-1.png)

## Notes

Okay, so this isn't drastically different from what `httr` already does, but its early days. 

  </div>
  
  <div class="post">
    <h1>
      <a href="/2015/03/faster-solr/">
        Faster solr with csv
      </a>
    </h1>

    <span class="post-date">20 Mar 2015</span>

    With the [help of user input](https://github.com/ropensci/solr/issues/47), I've tweaked `solr` just a bit to make things faster using default setings. I imagine the main interface for people using the `solr` R client is via `solr_search()`, which used to have `wt=json` by default. Changing this to `wt=csv` gives better performance. And it sorta makes sense to use csv, as the point of using an R client is probably do get data eventually into a data.frame, so it makes sense to go csv format (Already in tabular format) if it's faster too.

## Install

Install and load `solr`


```r
devtools::install_github("ropensci/solr")
```


```r
library("solr")
library("microbenchmark")
```

## Setup

Define base url and fields to return


```r
url <- 'http://api.plos.org/search'
fields <- c('id','cross_published_journal_name','cross_published_journal_key',
            'cross_published_journal_eissn','pmid','pmcid','publisher','journal',
            'publication_date','article_type','article_type_facet','author',
            'author_facet','volume','issue','elocation_id','author_display',
            'competing_interest','copyright')
```

## json

The previous default for `solr_search()` used `json`


```r
solr_search(q='*:*', rows=10, fl=fields, base=url, wt = "json")
#> Source: local data frame [10 x 19]
#> 
#>                                                                    id
#> 1             10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4
#> 2       10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/title
#> 3    10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/abstract
#> 4  10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/references
#> 5        10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/body
#> 6             10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525
#> 7       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/title
#> 8    10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/abstract
#> 9  10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/references
#> 10       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/body
#> Variables not shown: cross_published_journal_name (chr),
#>   cross_published_journal_key (chr), cross_published_journal_eissn (chr),
#>   pmid (chr), pmcid (chr), publisher (chr), journal (chr),
#>   publication_date (chr), article_type (chr), article_type_facet (chr),
#>   author (chr), author_facet (chr), volume (int), issue (int),
#>   elocation_id (chr), author_display (chr), competing_interest (chr),
#>   copyright (chr)
```

## csv

The default `wt` setting is now `csv`


```r
solr_search(q='*:*', rows=10, fl=fields, base=url, wt = "json")
#> Source: local data frame [10 x 19]
#> 
#>                                                                    id
#> 1             10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4
#> 2       10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/title
#> 3    10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/abstract
#> 4  10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/references
#> 5        10.1371/annotation/856f0890-9d85-4719-8e54-c27530ac94f4/body
#> 6             10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525
#> 7       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/title
#> 8    10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/abstract
#> 9  10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/references
#> 10       10.1371/annotation/8551e3d5-fdd5-413b-a253-170ba13b7525/body
#> Variables not shown: cross_published_journal_name (chr),
#>   cross_published_journal_key (chr), cross_published_journal_eissn (chr),
#>   pmid (chr), pmcid (chr), publisher (chr), journal (chr),
#>   publication_date (chr), article_type (chr), article_type_facet (chr),
#>   author (chr), author_facet (chr), volume (int), issue (int),
#>   elocation_id (chr), author_display (chr), competing_interest (chr),
#>   copyright (chr)
```

## Compare times

When parsing to a data.frame (which `solr_search()` does by default), csv is quite a bit faster.


```r
microbenchmark(
  json = solr_search(q='*:*', rows=500, fl=fields, base=url, wt = "json", verbose = FALSE),
  csv = solr_search(q='*:*', rows=500, fl=fields, base=url, wt = "csv", verbose = FALSE), 
  times = 20
)
#> Unit: milliseconds
#>  expr      min       lq      mean    median        uq       max neval cld
#>  json 965.7043 1013.014 1124.1229 1086.3225 1227.9054 1441.8332    20   b
#>   csv 509.6573  520.089  541.5784  532.4546  548.0303  723.7575    20  a
```

## json vs xml vs csv

When getting raw data, csv is best, json next, then xml pulling up the rear.


```r
microbenchmark(
  json = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "json", verbose = FALSE, raw = TRUE),
  csv = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "csv", verbose = FALSE, raw = TRUE),
  xml = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "xml", verbose = FALSE, raw = TRUE),
  times = 10
)
#> Unit: milliseconds
#>  expr       min       lq      mean    median        uq       max neval cld
#>  json 1110.9515 1142.478 1198.9981 1169.0808 1195.5709 1518.7412    10  b 
#>   csv  801.6871  802.516  826.0655  819.1532  835.0512  873.4266    10 a  
#>   xml 1507.1111 1554.002 1618.5963 1617.5208 1671.0026 1740.4448    10   c
```

## Notes

Note that `wt=csv` is only available in `solr_search()` and `solr_all()` because csv writer 
only returns the docs element in csv, dropping other elements, including facets, mlt, groups, 
stats, etc. 

Also, note the http client used in `solr` is `httr`, which passes in a gzip compression header by default, so as long as the server serving up the Solr data has compression turned on, that's all set.

Another way I've sped things up is if you use `wt=json` then parse to a data.frame, it uses `dplyr` which sped things up considerably.

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page10" class="older">Older</a>
  
  
    
      <a href="/page8" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
