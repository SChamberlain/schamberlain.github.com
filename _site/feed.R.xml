<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
	<channel>
		<title> - R</title>
		<description>Posts tagged as 'R'</description>
		<link>http://recology.info/</link>
		
			<item>
				<title>scrubr - clean species occurrence records</title>
				<description>&lt;p&gt;&lt;code&gt;scrubr&lt;/code&gt; is an R library for cleaning species occurrence records. It’s general purpose, and has the following approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We think using a piping workflow (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) makes code easier to build up, and easier to understand. However, you don’t have to use pipes in this package.&lt;/li&gt;
  &lt;li&gt;All inputs and outputs are data.frame’s - which makes the above point easier&lt;/li&gt;
  &lt;li&gt;Records trimmed off due to various filters are retained as attributes, so can still be accessed for later inspection, but don’t get in the way of the data.frame that gets modified for downstream use&lt;/li&gt;
  &lt;li&gt;User interface vs. speed: This is the kind of package that surely can get faster. However, we’re focusing on the UI first, then make speed improvements down the road.&lt;/li&gt;
  &lt;li&gt;Since occurrence record datasets should all have columns with lat/long information, we automatically look for those columns for you. If identified, we use them, but you can supply lat/long column names manually as well.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have many packages that fetch species occurrence records from GBIF, iNaturalist, VertNet, iDigBio, Ecoengine, and more. &lt;code&gt;scrubr&lt;/code&gt; fills a crucial missing niche as likely all uses of occurrence data requires cleaning of some kind. When using GBIF data via &lt;code&gt;rgbif&lt;/code&gt;, that package has some utilities for cleaning data based on the issues returned with GBIF data - &lt;code&gt;scrubr&lt;/code&gt; is a companion to do the rest of the cleaning.&lt;/p&gt;

&lt;h2 id=&quot;scrubr-use-cases&quot;&gt;scrubr use cases&lt;/h2&gt;

&lt;h3 id=&quot;those-covered&quot;&gt;Those covered&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Impossible lat/long values: e.g., latitude 75&lt;/li&gt;
  &lt;li&gt;Incomplete cases: one or the other of lat/long missing&lt;/li&gt;
  &lt;li&gt;Unlikely lat/long values: e.g., points at 0,0&lt;/li&gt;
  &lt;li&gt;Deduplication: try to identify duplicates, esp. when pulling data from multiple sources, e.g., can try to use occurrence IDs, if provided&lt;/li&gt;
  &lt;li&gt;Date based cleaning&lt;/li&gt;
  &lt;li&gt;Outside political boundary: User input to check for points in the wrong country, or points outside of a known country&lt;/li&gt;
  &lt;li&gt;Taxonomic name based cleaning: via &lt;code&gt;taxize&lt;/code&gt; (one method so far)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;to-be-covered&quot;&gt;To be covered&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Political centroids: unlikely that occurrences fall exactly on these points, more likely a
default position (Draft function started, but not exported, and commented out)&lt;/li&gt;
  &lt;li&gt;Herbaria/Museums: many specimens may have location of the collection they are housed in&lt;/li&gt;
  &lt;li&gt;Habitat type filtering: e.g., fish should not be on land; marine fish should not be in fresh water&lt;/li&gt;
  &lt;li&gt;Check for contextually wrong values: That is, if 99 out of 100 lat/long coordinates are within the continental US, but 1 is in China, then perhaps something is wrong with that one point&lt;/li&gt;
  &lt;li&gt;and many more…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What else do you want included? &lt;a href=&quot;https://github.com/ropenscilabs/scrubr/issues&quot;&gt;Open an issue in the repo&lt;/a&gt; to chat about use cases.&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;From CRAN (binaries may not be up yet, but source is)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;install.packages(&quot;scrubr&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or from GitHub&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;ropenscilabs/scrubr&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;scrubr&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;dframe&quot;&gt;dframe&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;dframe()&lt;/code&gt; is a tool to convert your data.frame to a compact &lt;code&gt;dplyr&lt;/code&gt; like data.frame so that you can get a quick peek at your data each time you call a function - BUT, you don’t have to use it.&lt;/p&gt;

&lt;p&gt;Compare &lt;code&gt;mtcars&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;mtcars
#&amp;gt;                      mpg cyl  disp  hp drat    wt  qsec vs am gear carb
#&amp;gt; Mazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
#&amp;gt; Mazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4
#&amp;gt; Datsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
#&amp;gt; Hornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1
#&amp;gt; Hornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2
#&amp;gt; Valiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1
#&amp;gt; Duster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4
#&amp;gt; Merc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2
#&amp;gt; Merc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2
#&amp;gt; Merc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4
#&amp;gt; Merc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4
#&amp;gt; Merc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3
#&amp;gt; Merc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3
#&amp;gt; Merc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3
#&amp;gt; Cadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4
#&amp;gt; Lincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4
#&amp;gt; Chrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4
#&amp;gt; Fiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1
#&amp;gt; Honda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2
#&amp;gt; Toyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1
#&amp;gt; Toyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1
#&amp;gt; Dodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2
#&amp;gt; AMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2
#&amp;gt; Camaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4
#&amp;gt; Pontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2
#&amp;gt; Fiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1
#&amp;gt; Porsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2
#&amp;gt; Lotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2
#&amp;gt; Ford Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4
#&amp;gt; Ferrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6
#&amp;gt; Maserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8
#&amp;gt; Volvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;dframe(mtcars)
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 32 X 11
#&amp;gt; 
#&amp;gt; 
#&amp;gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
#&amp;gt;    (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl) (dbl)
#&amp;gt; 1   21.0     6 160.0   110  3.90 2.620 16.46     0     1     4     4
#&amp;gt; 2   21.0     6 160.0   110  3.90 2.875 17.02     0     1     4     4
#&amp;gt; 3   22.8     4 108.0    93  3.85 2.320 18.61     1     1     4     1
#&amp;gt; 4   21.4     6 258.0   110  3.08 3.215 19.44     1     0     3     1
#&amp;gt; 5   18.7     8 360.0   175  3.15 3.440 17.02     0     0     3     2
#&amp;gt; 6   18.1     6 225.0   105  2.76 3.460 20.22     1     0     3     1
#&amp;gt; 7   14.3     8 360.0   245  3.21 3.570 15.84     0     0     3     4
#&amp;gt; 8   24.4     4 146.7    62  3.69 3.190 20.00     1     0     4     2
#&amp;gt; 9   22.8     4 140.8    95  3.92 3.150 22.90     1     0     4     2
#&amp;gt; 10  19.2     6 167.6   123  3.92 3.440 18.30     1     0     4     4
#&amp;gt; ..   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;coordinate-based-cleaning&quot;&gt;Coordinate based cleaning&lt;/h2&gt;

&lt;p&gt;Load some sample data that comes with the package&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;data(&quot;sampledata1&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove impossible coordinates (using sample data included in the pkg)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;dframe(sample_data_1) %&amp;gt;% coord_impossible()
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1500 X 5
#&amp;gt; Lat/Lon vars: latitude/longitude
#&amp;gt; 
#&amp;gt;                name  longitude latitude                date        key
#&amp;gt;               (chr)      (dbl)    (dbl)              (time)      (int)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 2015-01-14 16:36:45 1065590124
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 2015-01-13 00:25:39 1065588899
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 2015-02-20 23:00:00 1098894889
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 2015-02-13 16:16:41 1065611122
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 2015-03-01 20:20:45 1088908315
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 2015-03-20 21:11:24 1088923534
#&amp;gt; 10 Ursus americanus  -76.78671 35.53079 2015-04-05 23:00:00 1088954559
#&amp;gt; ..              ...        ...      ...                 ...        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove incomplete coordinates&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;dframe(sample_data_1) %&amp;gt;% coord_incomplete()
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1306 X 5
#&amp;gt; Lat/Lon vars: latitude/longitude
#&amp;gt; 
#&amp;gt;                name  longitude latitude                date        key
#&amp;gt;               (chr)      (dbl)    (dbl)              (time)      (int)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 2015-01-14 16:36:45 1065590124
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 2015-01-13 00:25:39 1065588899
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 2015-02-20 23:00:00 1098894889
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 2015-02-13 16:16:41 1065611122
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 2015-03-01 20:20:45 1088908315
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 2015-03-20 21:11:24 1088923534
#&amp;gt; 10 Ursus americanus  -76.78671 35.53079 2015-04-05 23:00:00 1088954559
#&amp;gt; ..              ...        ...      ...                 ...        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove unlikely coordinates (e.g., those at 0,0)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;dframe(sample_data_1) %&amp;gt;% coord_unlikely()
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1488 X 5
#&amp;gt; Lat/Lon vars: latitude/longitude
#&amp;gt; 
#&amp;gt;                name  longitude latitude                date        key
#&amp;gt;               (chr)      (dbl)    (dbl)              (time)      (int)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 2015-01-14 16:36:45 1065590124
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 2015-01-13 00:25:39 1065588899
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 2015-02-20 23:00:00 1098894889
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 2015-02-13 16:16:41 1065611122
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 2015-03-01 20:20:45 1088908315
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 2015-03-20 21:11:24 1088923534
#&amp;gt; 10 Ursus americanus  -76.78671 35.53079 2015-04-05 23:00:00 1088954559
#&amp;gt; ..              ...        ...      ...                 ...        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do all three&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;dframe(sample_data_1) %&amp;gt;%
  coord_impossible() %&amp;gt;%
  coord_incomplete() %&amp;gt;%
  coord_unlikely()
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1294 X 5
#&amp;gt; Lat/Lon vars: latitude/longitude
#&amp;gt; 
#&amp;gt;                name  longitude latitude                date        key
#&amp;gt;               (chr)      (dbl)    (dbl)              (time)      (int)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 2015-01-14 16:36:45 1065590124
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 2015-01-13 00:25:39 1065588899
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 2015-02-20 23:00:00 1098894889
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 2015-02-13 16:16:41 1065611122
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 2015-03-01 20:20:45 1088908315
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 2015-03-20 21:11:24 1088923534
#&amp;gt; 10 Ursus americanus  -76.78671 35.53079 2015-04-05 23:00:00 1088954559
#&amp;gt; ..              ...        ...      ...                 ...        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Do vs. don’t drop bad data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;# do
dframe(sample_data_1) %&amp;gt;% coord_incomplete(drop = TRUE) %&amp;gt;% NROW
#&amp;gt; [1] 1306
# don&#39;t
dframe(sample_data_1) %&amp;gt;% coord_incomplete(drop = FALSE) %&amp;gt;% NROW
#&amp;gt; [1] 1500
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;deduplicate&quot;&gt;Deduplicate&lt;/h2&gt;

&lt;p&gt;Get a smaller subset of a data.frame&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;smalldf &amp;lt;- sample_data_1[1:20, ]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;create a duplicate record&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;smalldf &amp;lt;- rbind(smalldf, smalldf[10,])
row.names(smalldf) &amp;lt;- NULL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;make it slightly different&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;smalldf[21, &quot;key&quot;] &amp;lt;- 1088954555
NROW(smalldf)
#&amp;gt; [1] 21
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It’s 21 rows, including 1 duplicate. Do the deduplication&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(dp &amp;lt;- dframe(smalldf) %&amp;gt;% dedup())
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 20 X 5
#&amp;gt; 
#&amp;gt; 
#&amp;gt;                name  longitude latitude                date        key
#&amp;gt;               (chr)      (dbl)    (dbl)              (time)      (dbl)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 2015-01-14 16:36:45 1065590124
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 2015-01-13 00:25:39 1065588899
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 2015-02-20 23:00:00 1098894889
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 2015-02-13 16:16:41 1065611122
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 2015-03-01 20:20:45 1088908315
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 2015-03-29 17:06:54 1088932238
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 2015-03-29 17:12:50 1088932273
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 2015-03-28 23:00:00 1132403409
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 2015-03-20 21:11:24 1088923534
#&amp;gt; 10 Ursus americanus -103.30058 29.27042 2015-04-29 22:00:00 1088964797
#&amp;gt; ..              ...        ...      ...                 ...        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now its 20 rows, duplicate removed&lt;/p&gt;

&lt;p&gt;Here’s the duplicates&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;attr(dp, &quot;dups&quot;)
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1 X 5
#&amp;gt; 
#&amp;gt; 
#&amp;gt;               name longitude latitude                date        key
#&amp;gt;              (chr)     (dbl)    (dbl)              (time)      (dbl)
#&amp;gt; 1 Ursus americanus -76.78671 35.53079 2015-04-05 23:00:00 1088954555
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;dates&quot;&gt;Dates&lt;/h2&gt;

&lt;p&gt;Standardize/convert dates&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;df &amp;lt;- sample_data_1
dframe(df) %&amp;gt;% 
  date_standardize(&quot;%d%b%Y&quot;)
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1500 X 5
#&amp;gt; 
#&amp;gt; 
#&amp;gt;                name  longitude latitude      date        key
#&amp;gt;               (chr)      (dbl)    (dbl)     (chr)      (int)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 14Jan2015 1065590124
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 13Jan2015 1065588899
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 20Feb2015 1098894889
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 13Feb2015 1065611122
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 01Mar2015 1088908315
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 29Mar2015 1088932238
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 29Mar2015 1088932273
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 28Mar2015 1132403409
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 20Mar2015 1088923534
#&amp;gt; 10 Ursus americanus  -76.78671 35.53079 05Apr2015 1088954559
#&amp;gt; ..              ...        ...      ...       ...        ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Drop records without dates&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;NROW(df)
#&amp;gt; [1] 1500
NROW(dframe(df) %&amp;gt;% date_missing())
#&amp;gt; [1] 1498
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Create date field from other fields&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;dframe(sample_data_2) %&amp;gt;% 
  date_create(year, month, day)
#&amp;gt; &amp;lt;scrubr dframe&amp;gt;
#&amp;gt; Size: 1500 X 8
#&amp;gt; 
#&amp;gt; 
#&amp;gt;                name  longitude latitude        key  year month   day
#&amp;gt;               (chr)      (dbl)    (dbl)      (int) (chr) (chr) (chr)
#&amp;gt; 1  Ursus americanus  -79.68283 38.36662 1065590124  2015    01    14
#&amp;gt; 2  Ursus americanus  -82.42028 35.73304 1065588899  2015    01    13
#&amp;gt; 3  Ursus americanus  -99.09625 23.66893 1098894889  2015    02    20
#&amp;gt; 4  Ursus americanus  -72.77432 43.94883 1065611122  2015    02    13
#&amp;gt; 5  Ursus americanus  -72.34617 43.86464 1088908315  2015    03    01
#&amp;gt; 6  Ursus americanus -108.53674 32.65219 1088932238  2015    03    29
#&amp;gt; 7  Ursus americanus -108.53691 32.65237 1088932273  2015    03    29
#&amp;gt; 8  Ursus americanus -123.82900 40.13240 1132403409  2015    03    28
#&amp;gt; 9  Ursus americanus  -78.25027 36.93018 1088923534  2015    03    20
#&amp;gt; 10 Ursus americanus  -76.78671 35.53079 1088954559  2015    04    05
#&amp;gt; ..              ...        ...      ...        ...   ...   ...   ...
#&amp;gt; Variables not shown: date (chr).
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;bugs-and-such&quot;&gt;bugs and such&lt;/h2&gt;

&lt;p&gt;Report them in the &lt;a href=&quot;https://github.com/ropenscilabs/scrubr/issues&quot;&gt;scrubr issue tracker&lt;/a&gt;&lt;/p&gt;
</description>
				<published>2016-03-04 00:00:00 -0800</published>
				<link>http://recology.info//2016/03/scrubr/</link>
			</item>
		
			<item>
				<title>request - a high level HTTP client for R</title>
				<description>&lt;p&gt;&lt;code&gt;request&lt;/code&gt; is DSL for http requests for R, and is inspired by the CLI tool &lt;a href=&quot;https://github.com/jakubroztocil/httpie&quot;&gt;httpie&lt;/a&gt;. It’s built on &lt;code&gt;httr&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The following were driving principles for this package:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The web is increasingly a JSON world, so we assume &lt;code&gt;applications/json&lt;/code&gt; by default, but give back other types if not&lt;/li&gt;
  &lt;li&gt;The workflow follows logically, or at least should, from, &lt;em&gt;hey, I got this url&lt;/em&gt;, to &lt;em&gt;i need to add some options&lt;/em&gt;, to &lt;em&gt;execute request&lt;/em&gt; - and functions support piping so that you can execute functions in this order&lt;/li&gt;
  &lt;li&gt;Whenever possible, we transform output to data.frame’s - facilitating downstream manipulation via &lt;code&gt;dplyr&lt;/code&gt;, etc.&lt;/li&gt;
  &lt;li&gt;We do &lt;code&gt;GET&lt;/code&gt; requests by default. Specify a different type if you don’t want &lt;code&gt;GET&lt;/code&gt;. Given &lt;code&gt;GET&lt;/code&gt; by default, this client is optimized for consumption of data, rather than creating new things on servers&lt;/li&gt;
  &lt;li&gt;You can use non-standard evaluation to easily pass in query parameters without worrying about &lt;code&gt;&amp;amp;&lt;/code&gt;’s, URL escaping, etc. (see &lt;code&gt;api_query()&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Same for body params (see &lt;code&gt;api_body()&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following is a brief demo of some of the package functionality:&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;From CRAN&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;install.packages(&quot;request&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or from GitHub&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;sckott/request&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;request&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;execute-on-last-pipe&quot;&gt;Execute on last pipe&lt;/h2&gt;

&lt;p&gt;When using pipes (&lt;code&gt;%&amp;gt;%&lt;/code&gt;) in &lt;code&gt;request&lt;/code&gt;, we autodetect last piped command, and execute &lt;code&gt;http()&lt;/code&gt; if it’s the last. If not the last, the output gets passed to the next command, and so on. This feature (and &lt;code&gt;magrittr&lt;/code&gt;) were done by Stefan Milton Bache.&lt;/p&gt;

&lt;p&gt;This feature is really nice because a) it’s one less thing you need to do, and b) you only need to care about the request itself.&lt;/p&gt;

&lt;p&gt;You can escape auto-execution if you use the function &lt;code&gt;peep()&lt;/code&gt;, which prints out a summary of the request you’ve created, but does not execute an HTTP request.&lt;/p&gt;

&lt;h2 id=&quot;http-requests&quot;&gt;HTTP Requests&lt;/h2&gt;

&lt;p&gt;A high level function &lt;code&gt;http()&lt;/code&gt; wraps a lower level &lt;code&gt;R6&lt;/code&gt; object &lt;code&gt;RequestIterator&lt;/code&gt;, which holds a series of variables and functions to execute &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;POST&lt;/code&gt; requests, and will hold other HTTP verbs as well. In addition, it can hold state, which will allow us to do paging internally for you (see below). You have direct access to the &lt;code&gt;R6&lt;/code&gt; object if you call &lt;code&gt;http_client()&lt;/code&gt; instead of &lt;code&gt;http()&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;nse-and-se&quot;&gt;NSE and SE&lt;/h2&gt;

&lt;p&gt;Most if not all functions in &lt;code&gt;request&lt;/code&gt; support non-standard evaluation (NSE) as well as standard evaluation (SE). If a function supports both, there’s a version without an underscore for NSE, while a version with an underscore is for SE. For example, here, we make a HTTP request by passing a base URL, then a series of paths that get combined together. First the NSE version&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;https://api.github.com/&#39;) %&amp;gt;%
  api_path(repos, ropensci, rgbif, issues)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then the SE version&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;https://api.github.com/&#39;) %&amp;gt;%
  api_path_(&#39;repos&#39;, &#39;ropensci&#39;, &#39;rgbif&#39;, &#39;issues&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;building-api-routes&quot;&gt;Building API routes&lt;/h2&gt;

&lt;p&gt;The first thing you’ll want to do is lay out the base URL for your request. The function &lt;code&gt;api()&lt;/code&gt; is your friend.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;api()&lt;/code&gt; works with full or partial URLs:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;https://api.github.com/&#39;)
#&amp;gt; URL: https://api.github.com/
api(&#39;http://api.gbif.org/v1&#39;)
#&amp;gt; URL: http://api.gbif.org/v1
api(&#39;api.gbif.org/v1&#39;)
#&amp;gt; URL: api.gbif.org/v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And works with ports, full or partial&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;http://localhost:9200&#39;)
#&amp;gt; URL: http://localhost:9200
api(&#39;localhost:9200&#39;)
#&amp;gt; URL: http://localhost:9200
api(&#39;:9200&#39;)
#&amp;gt; URL: http://localhost:9200
api(&#39;9200&#39;)
#&amp;gt; URL: http://localhost:9200
api(&#39;9200/stuff&#39;)
#&amp;gt; URL: http://localhost:9200/stuff
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;make-http-requests&quot;&gt;Make HTTP requests&lt;/h2&gt;

&lt;p&gt;The above examples with &lt;code&gt;api()&lt;/code&gt; are not passed through a pipe, so only define a URL, but don’t do an HTTP request. To make an HTTP request, you can either pipe a url or partial url to e.g., &lt;code&gt;api()&lt;/code&gt;, or call &lt;code&gt;http()&lt;/code&gt; at the end of a string of function calls:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;&#39;https://api.github.com/&#39; %&amp;gt;% api()
#&amp;gt; $current_user_url
#&amp;gt; [1] &quot;https://api.github.com/user&quot;
#&amp;gt;
#&amp;gt; $current_user_authorizations_html_url
#&amp;gt; [1] &quot;https://github.com/settings/connections/applications{/client_id}&quot;
#&amp;gt;
#&amp;gt; $authorizations_url
#&amp;gt; [1] &quot;https://api.github.com/authorizations&quot;
#&amp;gt;
#&amp;gt; $code_search_url
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;https://api.github.com/&#39;) %&amp;gt;% http()
#&amp;gt; $current_user_url
#&amp;gt; [1] &quot;https://api.github.com/user&quot;
#&amp;gt;
#&amp;gt; $current_user_authorizations_html_url
#&amp;gt; [1] &quot;https://github.com/settings/connections/applications{/client_id}&quot;
#&amp;gt;
#&amp;gt; $authorizations_url
#&amp;gt; [1] &quot;https://api.github.com/authorizations&quot;
#&amp;gt;
#&amp;gt; $code_search_url
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;http()&lt;/code&gt; is called at the end of a chain of piped commands, so no need to invoke it. However, you can if you like.&lt;/p&gt;

&lt;h2 id=&quot;templating&quot;&gt;Templating&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;repo_info &amp;lt;- list(username = &#39;craigcitro&#39;, repo = &#39;r-travis&#39;)
api(&#39;https://api.github.com/&#39;) %&amp;gt;%
  api_template(template = &#39;repos///issues&#39;, data = repo_info)
#&amp;gt; [[1]]
#&amp;gt; [[1]]$url
#&amp;gt; [1] &quot;https://api.github.com/repos/craigcitro/r-travis/issues/164&quot;
#&amp;gt;
#&amp;gt; [[1]]$labels_url
#&amp;gt; [1] &quot;https://api.github.com/repos/craigcitro/r-travis/issues/164/labels{/name}&quot;
#&amp;gt;
#&amp;gt; [[1]]$comments_url
#&amp;gt; [1] &quot;https://api.github.com/repos/craigcitro/r-travis/issues/164/comments&quot;
#&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;set-paths&quot;&gt;Set paths&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;api_path()&lt;/code&gt; adds paths to the base URL&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;https://api.github.com/&#39;) %&amp;gt;%
  api_path(repos, ropensci, rgbif, issues) %&amp;gt;%
  peep
#&amp;gt; &amp;lt;http request&amp;gt;
#&amp;gt;   url: https://api.github.com/
#&amp;gt;   paths: repos/ropensci/rgbif/issues
#&amp;gt;   query:
#&amp;gt;   body:
#&amp;gt;   paging:
#&amp;gt;   headers:
#&amp;gt;   rate limit:
#&amp;gt;   retry (n/delay (s)): /
#&amp;gt;   error handler:
#&amp;gt;   config:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;query&quot;&gt;Query&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&quot;http://api.plos.org/search&quot;) %&amp;gt;%
  api_query(q = ecology, wt = json, fl = journal) %&amp;gt;%
  peep
#&amp;gt; &amp;lt;http request&amp;gt;
#&amp;gt;   url: http://api.plos.org/search
#&amp;gt;   paths:
#&amp;gt;   query: q=ecology, wt=json, fl=journal
#&amp;gt;   body:
#&amp;gt;   paging:
#&amp;gt;   headers:
#&amp;gt;   rate limit:
#&amp;gt;   retry (n/delay (s)): /
#&amp;gt;   error handler:
#&amp;gt;   config:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;headers&quot;&gt;Headers&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;http://httpbin.org/headers&#39;) %&amp;gt;%
  api_headers(`X-FARGO-SEASON` = 3, `X-NARCOS-SEASON` = 5) %&amp;gt;%
  peep
#&amp;gt; &amp;lt;http request&amp;gt;
#&amp;gt;   url: http://httpbin.org/headers
#&amp;gt;   paths:
#&amp;gt;   query:
#&amp;gt;   body:
#&amp;gt;   paging:
#&amp;gt;   headers:
#&amp;gt;     X-FARGO-SEASON: 3
#&amp;gt;     X-NARCOS-SEASON: 5
#&amp;gt;   rate limit:
#&amp;gt;   retry (n/delay (s)): /
#&amp;gt;   error handler:
#&amp;gt;   config:
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;curl-configuration&quot;&gt;curl configuration&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;httr&lt;/code&gt; is exported in &lt;code&gt;request&lt;/code&gt;, so you can use &lt;code&gt;httr&lt;/code&gt; functions like &lt;code&gt;verbose()&lt;/code&gt; to get verbose curl output&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;api(&#39;http://httpbin.org/headers&#39;) %&amp;gt;%
  api_config(verbose())
#&amp;gt; -&amp;gt; GET /headers HTTP/1.1
#&amp;gt; -&amp;gt; Host: httpbin.org
#&amp;gt; -&amp;gt; User-Agent: curl/7.43.0 curl/0.9.4 httr/1.0.0 request/0.1.0
#&amp;gt; -&amp;gt; Accept-Encoding: gzip, deflate
#&amp;gt; -&amp;gt; Accept: application/json, text/xml, application/xml, */*
#&amp;gt; -&amp;gt;
#&amp;gt; &amp;lt;- HTTP/1.1 200 OK
#&amp;gt; &amp;lt;- Server: nginx
#&amp;gt; &amp;lt;- Date: Sun, 03 Jan 2016 16:56:29 GMT
#&amp;gt; &amp;lt;- Content-Type: application/json
#&amp;gt; &amp;lt;- Content-Length: 227
#&amp;gt; &amp;lt;- Connection: keep-alive
#&amp;gt; &amp;lt;- Access-Control-Allow-Origin: *
#&amp;gt; &amp;lt;- Access-Control-Allow-Credentials: true
#&amp;gt; &amp;lt;-
#&amp;gt; $headers
#&amp;gt; $headers$Accept
#&amp;gt; [1] &quot;application/json, text/xml, application/xml, */*&quot;
#&amp;gt; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;coming-soon&quot;&gt;Coming soon&lt;/h2&gt;

&lt;p&gt;There’s a number of interesting features that should be coming soon to &lt;code&gt;request&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Paging - a paging helper will make it easy to do paing, and will attempt to handle any parameters used for paging. Some user input will be required, like what parameter names are, and how many records you want returned  &lt;a href=&quot;https://github.com/sckott/request/issues/2&quot;&gt;sckott/request#2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Retry - a retry helper will make it easy to retry http requests on any failure, and execute a user defined function on failure &lt;a href=&quot;https://github.com/sckott/request/issues/6&quot;&gt;sckott/request#6&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Rate limit - a rate limit helper will add info to a set of many requests - still in early design stages &lt;a href=&quot;https://github.com/sckott/request/issues/5&quot;&gt;sckott/request#5&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Caching - a caching helper - may use in the background the in development &lt;a href=&quot;https://github.com/ropensci/vcr&quot;&gt;vcr R client&lt;/a&gt; when on CRAN or perhaps &lt;a href=&quot;https://github.com/richfitz/storr&quot;&gt;storr&lt;/a&gt;  &lt;a href=&quot;https://github.com/sckott/request/issues/4&quot;&gt;sckott/request#4&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<published>2016-01-05 00:00:00 -0800</published>
				<link>http://recology.info//2016/01/request-hello-world/</link>
			</item>
		
			<item>
				<title>binomen - Tools for slicing and dicing taxonomic names</title>
				<description>&lt;p&gt;The first version of &lt;code&gt;binomen&lt;/code&gt; is now up on &lt;a href=&quot;https://cran.rstudio.com/web/packages/binomen&quot;&gt;CRAN&lt;/a&gt;. It provides various taxonomic classes for defining a single taxon, multiple taxa, and a taxonomic data.frame. It is designed as a companion to &lt;a href=&quot;https://github.com/ropensci/taxize&quot;&gt;taxize&lt;/a&gt;, where you can get taxonomic data on taxonomic names from the web.&lt;/p&gt;

&lt;p&gt;The classes (S3):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;taxon&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;taxonref&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;taxonrefs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;binomial&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;grouping&lt;/code&gt; (i.e., classification - used different term to avoid conflict with classification in &lt;code&gt;taxize&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For example, the &lt;code&gt;binomial&lt;/code&gt; class is defined by a genus, epithet, authority, and optional full species name and canonical version.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;binomial(&quot;Poa&quot;, &quot;annua&quot;, authority=&quot;L.&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;&amp;lt;binomial&amp;gt;
  genus: Poa
  epithet: annua
  canonical:
  species:
  authority: L.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The package has a suite of functions to work on these taxonomic classes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;gethier()&lt;/code&gt; - get hierarchy from a &lt;code&gt;taxon&lt;/code&gt; class&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;scatter()&lt;/code&gt; - make each row in taxonomic data.frame (&lt;code&gt;taxondf&lt;/code&gt;) a separate &lt;code&gt;taxon&lt;/code&gt; object within a single &lt;code&gt;taxa&lt;/code&gt; object&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;assemble()&lt;/code&gt; - make a &lt;code&gt;taxa&lt;/code&gt; object into a &lt;code&gt;taxondf&lt;/code&gt; data.frame&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pick()&lt;/code&gt; - pick out one or more taxonomic groups&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;pop()&lt;/code&gt; - pop out (drop) one or more taxonomic groups&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;span()&lt;/code&gt; - pick a range between two taxonomic groups (inclusive)&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;strain()&lt;/code&gt; - filter by taxonomic groups, like dplyr’s filter&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;name()&lt;/code&gt; - get the taxon name for each &lt;code&gt;taxonref&lt;/code&gt; object&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;uri()&lt;/code&gt; - get the reference uri for each &lt;code&gt;taxonref&lt;/code&gt; object&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;rank()&lt;/code&gt; - get the taxonomic rank for each &lt;code&gt;taxonref&lt;/code&gt; object&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;id()&lt;/code&gt; - get the reference uri for each &lt;code&gt;taxonref&lt;/code&gt; object&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The approach in this package I suppose is sort of like &lt;code&gt;split-apply-combine&lt;/code&gt; from &lt;code&gt;plyr&lt;/code&gt;/&lt;code&gt;dplyr&lt;/code&gt;, whereas this is aims to make it easy to do with taxonomic names.&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;For examples below, you’ll need the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;install.packages(&quot;binomen&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;binomen&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;make-a-taxon&quot;&gt;Make a taxon&lt;/h2&gt;

&lt;p&gt;Make a taxon object&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(obj &amp;lt;- make_taxon(genus=&quot;Poa&quot;, epithet=&quot;annua&quot;, authority=&quot;L.&quot;,
  family=&#39;Poaceae&#39;, clazz=&#39;Poales&#39;, kingdom=&#39;Plantae&#39;, variety=&#39;annua&#39;))
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa annua
#&amp;gt;   grouping: 
#&amp;gt;     kingdom: Plantae
#&amp;gt;     clazz: Poales
#&amp;gt;     family: Poaceae
#&amp;gt;     genus: Poa
#&amp;gt;     species: Poa annua
#&amp;gt;     variety: annua
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Index to various parts of the object&lt;/p&gt;

&lt;p&gt;The binomial&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj$binomial
#&amp;gt; &amp;lt;binomial&amp;gt;
#&amp;gt;   genus: Poa
#&amp;gt;   epithet: annua
#&amp;gt;   canonical: Poa annua
#&amp;gt;   species: Poa annua L.
#&amp;gt;   authority: L.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The authority&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj$binomial$authority
#&amp;gt; [1] &quot;L.&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The classification&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj$grouping
#&amp;gt; &amp;lt;grouping&amp;gt;
#&amp;gt;   kingdom: Plantae
#&amp;gt;   clazz: Poales
#&amp;gt;   family: Poaceae
#&amp;gt;   genus: Poa
#&amp;gt;   species: Poa annua
#&amp;gt;   variety: annua
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The family&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj$grouping$family
#&amp;gt; &amp;lt;taxonref&amp;gt;
#&amp;gt;   rank: family
#&amp;gt;   name: Poaceae
#&amp;gt;   id: none
#&amp;gt;   uri: none
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;subset-taxon-objects&quot;&gt;Subset taxon objects&lt;/h2&gt;

&lt;p&gt;Get one or more ranks via &lt;code&gt;pick()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj %&amp;gt;% pick(family)
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa annua
#&amp;gt;   grouping: 
#&amp;gt;     family: Poaceae
obj %&amp;gt;% pick(family, genus)
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa annua
#&amp;gt;   grouping: 
#&amp;gt;     family: Poaceae
#&amp;gt;     genus: Poa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Drop one or more ranks via &lt;code&gt;pop()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj %&amp;gt;% pop(family)
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa annua
#&amp;gt;   grouping: 
#&amp;gt;     kingdom: Plantae
#&amp;gt;     clazz: Poales
#&amp;gt;     genus: Poa
#&amp;gt;     species: Poa annua
#&amp;gt;     variety: annua
obj %&amp;gt;% pop(family, genus)
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa annua
#&amp;gt;   grouping: 
#&amp;gt;     kingdom: Plantae
#&amp;gt;     clazz: Poales
#&amp;gt;     species: Poa annua
#&amp;gt;     variety: annua
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get a range of ranks via &lt;code&gt;span()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;obj %&amp;gt;% span(kingdom, family)
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa annua
#&amp;gt;   grouping: 
#&amp;gt;     kingdom: Plantae
#&amp;gt;     clazz: Poales
#&amp;gt;     family: Poaceae
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extract classification as a &lt;code&gt;data.frame&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;gethier(obj)
#&amp;gt;      rank      name
#&amp;gt; 1 kingdom   Plantae
#&amp;gt; 2   clazz    Poales
#&amp;gt; 3  family   Poaceae
#&amp;gt; 4   genus       Poa
#&amp;gt; 5 species Poa annua
#&amp;gt; 6 variety     annua
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;taxonomic-dataframes&quot;&gt;Taxonomic data.frame’s&lt;/h2&gt;

&lt;p&gt;Make one&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;df &amp;lt;- data.frame(order = c(&#39;Asterales&#39;,&#39;Asterales&#39;,&#39;Fagales&#39;,&#39;Poales&#39;,&#39;Poales&#39;,&#39;Poales&#39;),
  family = c(&#39;Asteraceae&#39;,&#39;Asteraceae&#39;,&#39;Fagaceae&#39;,&#39;Poaceae&#39;,&#39;Poaceae&#39;,&#39;Poaceae&#39;),
  genus = c(&#39;Helianthus&#39;,&#39;Helianthus&#39;,&#39;Quercus&#39;,&#39;Poa&#39;,&#39;Festuca&#39;,&#39;Holodiscus&#39;),
  stringsAsFactors = FALSE)
(df2 &amp;lt;- taxon_df(df))
#&amp;gt;       order     family      genus
#&amp;gt; 1 Asterales Asteraceae Helianthus
#&amp;gt; 2 Asterales Asteraceae Helianthus
#&amp;gt; 3   Fagales   Fagaceae    Quercus
#&amp;gt; 4    Poales    Poaceae        Poa
#&amp;gt; 5    Poales    Poaceae    Festuca
#&amp;gt; 6    Poales    Poaceae Holodiscus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Parse - get rank order via &lt;code&gt;pick()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;df2 %&amp;gt;% pick(order)
#&amp;gt;       order
#&amp;gt; 1 Asterales
#&amp;gt; 2 Asterales
#&amp;gt; 3   Fagales
#&amp;gt; 4    Poales
#&amp;gt; 5    Poales
#&amp;gt; 6    Poales
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;get ranks order, family, and genus via &lt;code&gt;pick()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;df2 %&amp;gt;% pick(order, family, genus)
#&amp;gt;       order     family      genus
#&amp;gt; 1 Asterales Asteraceae Helianthus
#&amp;gt; 2 Asterales Asteraceae Helianthus
#&amp;gt; 3   Fagales   Fagaceae    Quercus
#&amp;gt; 4    Poales    Poaceae        Poa
#&amp;gt; 5    Poales    Poaceae    Festuca
#&amp;gt; 6    Poales    Poaceae Holodiscus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;get range of names via &lt;code&gt;span()&lt;/code&gt;, from rank &lt;code&gt;X&lt;/code&gt; to rank &lt;code&gt;Y&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;df2 %&amp;gt;% span(family, genus)
#&amp;gt;       family      genus
#&amp;gt; 1 Asteraceae Helianthus
#&amp;gt; 2 Asteraceae Helianthus
#&amp;gt; 3   Fagaceae    Quercus
#&amp;gt; 4    Poaceae        Poa
#&amp;gt; 5    Poaceae    Festuca
#&amp;gt; 6    Poaceae Holodiscus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Separate each row into a &lt;code&gt;taxon&lt;/code&gt; class (many &lt;code&gt;taxon&lt;/code&gt; objects are a &lt;code&gt;taxa&lt;/code&gt; class)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;scatter(df2)
#&amp;gt; [[1]]
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Helianthus none
#&amp;gt;   grouping: 
#&amp;gt;     order: Asterales
#&amp;gt;     family: Asteraceae
#&amp;gt;     genus: Helianthus
#&amp;gt;     species: Helianthus none
#&amp;gt; 
#&amp;gt; [[2]]
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Helianthus none
#&amp;gt;   grouping: 
#&amp;gt;     order: Asterales
#&amp;gt;     family: Asteraceae
#&amp;gt;     genus: Helianthus
#&amp;gt;     species: Helianthus none
#&amp;gt; 
#&amp;gt; [[3]]
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Quercus none
#&amp;gt;   grouping: 
#&amp;gt;     order: Fagales
#&amp;gt;     family: Fagaceae
#&amp;gt;     genus: Quercus
#&amp;gt;     species: Quercus none
#&amp;gt; 
#&amp;gt; [[4]]
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Poa none
#&amp;gt;   grouping: 
#&amp;gt;     order: Poales
#&amp;gt;     family: Poaceae
#&amp;gt;     genus: Poa
#&amp;gt;     species: Poa none
#&amp;gt; 
#&amp;gt; [[5]]
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Festuca none
#&amp;gt;   grouping: 
#&amp;gt;     order: Poales
#&amp;gt;     family: Poaceae
#&amp;gt;     genus: Festuca
#&amp;gt;     species: Festuca none
#&amp;gt; 
#&amp;gt; [[6]]
#&amp;gt; &amp;lt;taxon&amp;gt;
#&amp;gt;   binomial: Holodiscus none
#&amp;gt;   grouping: 
#&amp;gt;     order: Poales
#&amp;gt;     family: Poaceae
#&amp;gt;     genus: Holodiscus
#&amp;gt;     species: Holodiscus none
#&amp;gt; 
#&amp;gt; attr(,&quot;class&quot;)
#&amp;gt; [1] &quot;taxa&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you can re-assemble a data.frame from the output of &lt;code&gt;scatter()&lt;/code&gt; with &lt;code&gt;assemble()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;out &amp;lt;- scatter(df2)
assemble(out)
#&amp;gt;       order     family      genus         species
#&amp;gt; 1 Asterales Asteraceae Helianthus Helianthus none
#&amp;gt; 2 Asterales Asteraceae Helianthus Helianthus none
#&amp;gt; 3   Fagales   Fagaceae    Quercus    Quercus none
#&amp;gt; 4    Poales    Poaceae        Poa        Poa none
#&amp;gt; 5    Poales    Poaceae    Festuca    Festuca none
#&amp;gt; 6    Poales    Poaceae Holodiscus Holodiscus none
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;thoughts&quot;&gt;Thoughts?&lt;/h2&gt;

&lt;p&gt;I’m really curious what people think of &lt;code&gt;binomen&lt;/code&gt;. I’m not sure how useful this will be in the wild. Try it. Let me know. Thanks much :)&lt;/p&gt;

</description>
				<published>2015-12-08 00:00:00 -0800</published>
				<link>http://recology.info//2015/12/binomen-taxonomy-tools/</link>
			</item>
		
			<item>
				<title>Crossref programmatic clients</title>
				<description>&lt;p&gt;I gave two talks recently at the annual &lt;a href=&quot;http://www.crossref.org/annualmeeting/agenda.html&quot;&gt;Crossref meeting&lt;/a&gt;, one of which was a somewhat technical overview of programmatic clients for Crossref APIs. Check out the talk &lt;a href=&quot;https://crossref.wistia.com/medias/8rh0jm5eda&quot;&gt;here&lt;/a&gt;. I talked about the motivation for working with Crossref data by writing code/etc. rather than going the GUI route, then went over the various clients, with brief examples.&lt;/p&gt;

&lt;p&gt;We (rOpenSci) have been working on the R client &lt;a href=&quot;https://github.com/ropensci/rcrossref&quot;&gt;rcrossref&lt;/a&gt; for a while now, but I’m also working on the Python and Ruby clients for Crossref. In addition, the Ruby client has a CLI client inside. The Javascript client is worked on independently by &lt;a href=&quot;https://science.ai/&quot;&gt;ScienceAI&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The R, Ruby, and Python clients are useable but not feature complete yet, and would benefit from lots of users surfacing bugs and highlighting nice to have features.&lt;/p&gt;

&lt;p&gt;The main Crossref API used in all the clients is documented at &lt;a href=&quot;https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md&quot;&gt;api.crossref.org&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I’ve tried to make the APIs similar-ish across clients. Functions in each client match the main Crossref search API (api.crossref.org) routes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;/works&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/members&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/funders&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/journals&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/types&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;/licenses&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other methods in all three clients:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get DOI minting agency
    &lt;ul&gt;
      &lt;li&gt;Uses api.crossref.org API&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Get random DOIs
    &lt;ul&gt;
      &lt;li&gt;Uses api.crossref.org API&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Content negotiation
    &lt;ul&gt;
      &lt;li&gt;Documented at &lt;a href=&quot;http://www.crosscite.org/cn&quot;&gt;http://www.crosscite.org/cn&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Get full text
    &lt;ul&gt;
      &lt;li&gt;other clients in each language will focus on this use case&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Get citation count
    &lt;ul&gt;
      &lt;li&gt;Uses service at &lt;a href=&quot;http://www.crossref.org/openurl&quot;&gt;http://www.crossref.org/openurl&lt;/a&gt; - though this functionality may be in the api.crossref.org API at some point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following shows how to install, and then examples from each client for a few use cases.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;h3 id=&quot;python&quot;&gt;Python&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;pip install habanero
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;ruby&quot;&gt;Ruby&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;gem install serrano
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;r&quot;&gt;R&lt;/h3&gt;

&lt;p&gt;Inside R:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;install.packages(&quot;rcrossref&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;javascript&quot;&gt;Javascript&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;npm install crossref
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I won’t do any examples with the js library, as I don’t maintain it.&lt;/p&gt;

&lt;h2 id=&quot;use-case-get-orcid-ids-for-authors&quot;&gt;Use case: get ORCID IDs for authors&lt;/h2&gt;

&lt;p&gt;Python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from habanero import Crossref
cr = Crossref()
res = cr.works(filter = {&#39;has_orcid&#39;: True}, limit = 10)
res2 = [ [ z.get(&#39;ORCID&#39;) for z in x[&#39;author&#39;] ] for x in res.result[&#39;message&#39;][&#39;items&#39;] ]
filter(None, reduce(lambda x, y: x+y, res2))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;[u&#39;http://orcid.org/0000-0003-4087-8021&#39;,
 u&#39;http://orcid.org/0000-0002-2076-5452&#39;,
 u&#39;http://orcid.org/0000-0003-4087-8021&#39;,
 u&#39;http://orcid.org/0000-0002-2076-5452&#39;,
 u&#39;http://orcid.org/0000-0003-1710-1580&#39;,
 u&#39;http://orcid.org/0000-0003-1710-1580&#39;,
 u&#39;http://orcid.org/0000-0003-4637-238X&#39;,
 u&#39;http://orcid.org/0000-0003-4637-238X&#39;,
 u&#39;http://orcid.org/0000-0003-4637-238X&#39;,
 u&#39;http://orcid.org/0000-0003-4637-238X&#39;,
 u&#39;http://orcid.org/0000-0003-4637-238X&#39;,
 u&#39;http://orcid.org/0000-0003-2510-4271&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ruby&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;require &#39;serrano&#39;
res = Serrano.works(filter: {&#39;has_orcid&#39;: true}, limit: 10)
res2 = res[&#39;message&#39;][&#39;items&#39;].collect { |x| x[&#39;author&#39;].collect { |z| z[&#39;ORCID&#39;] } }
res2.flatten.compact
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;=&amp;gt; [&quot;http://orcid.org/0000-0003-4087-8021&quot;,
 &quot;http://orcid.org/0000-0002-2076-5452&quot;,
 &quot;http://orcid.org/0000-0003-4087-8021&quot;,
 &quot;http://orcid.org/0000-0002-2076-5452&quot;,
 &quot;http://orcid.org/0000-0003-1710-1580&quot;,
 &quot;http://orcid.org/0000-0003-1710-1580&quot;,
 &quot;http://orcid.org/0000-0003-4637-238X&quot;,
 &quot;http://orcid.org/0000-0003-4637-238X&quot;,
 &quot;http://orcid.org/0000-0003-4637-238X&quot;,
 &quot;http://orcid.org/0000-0003-4637-238X&quot;,
 &quot;http://orcid.org/0000-0003-4637-238X&quot;,
 &quot;http://orcid.org/0000-0003-2510-4271&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt;library(&quot;rcrossref&quot;)
res &amp;lt;- cr_works(filter=c(has_orcid=TRUE), limit = 10)
orcids &amp;lt;- unlist(lapply(res$data$author, function(z) z$ORCID))
Filter(function(x) !is.na(x), orcids)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-R&quot;&gt; [1] &quot;http://orcid.org/0000-0003-4087-8021&quot;
 [2] &quot;http://orcid.org/0000-0002-2076-5452&quot;
 [3] &quot;http://orcid.org/0000-0003-4087-8021&quot;
 [4] &quot;http://orcid.org/0000-0002-2076-5452&quot;
 [5] &quot;http://orcid.org/0000-0003-1710-1580&quot;
 [6] &quot;http://orcid.org/0000-0003-1710-1580&quot;
 [7] &quot;http://orcid.org/0000-0003-4637-238X&quot;
 [8] &quot;http://orcid.org/0000-0003-4637-238X&quot;
 [9] &quot;http://orcid.org/0000-0003-4637-238X&quot;
[10] &quot;http://orcid.org/0000-0003-4637-238X&quot;
[11] &quot;http://orcid.org/0000-0003-4637-238X&quot;
[12] &quot;http://orcid.org/0000-0003-2510-4271&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CLI&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;serrano works --filter=has_orcid:true --json --limit=12 | jq &#39;.message.items[].author[].ORCID | select(. != null)&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;&quot;http://orcid.org/0000-0003-4087-8021&quot;
&quot;http://orcid.org/0000-0002-2076-5452&quot;
&quot;http://orcid.org/0000-0003-4087-8021&quot;
&quot;http://orcid.org/0000-0002-2076-5452&quot;
&quot;http://orcid.org/0000-0003-1710-1580&quot;
&quot;http://orcid.org/0000-0003-1710-1580&quot;
&quot;http://orcid.org/0000-0003-4637-238X&quot;
&quot;http://orcid.org/0000-0003-4637-238X&quot;
&quot;http://orcid.org/0000-0003-4637-238X&quot;
&quot;http://orcid.org/0000-0003-4637-238X&quot;
&quot;http://orcid.org/0000-0003-4637-238X&quot;
&quot;http://orcid.org/0000-0003-2510-4271&quot;
&quot;http://orcid.org/0000-0001-9408-8207&quot;
&quot;http://orcid.org/0000-0002-2076-5452&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;use-case-content-negotation&quot;&gt;Use case: content negotation&lt;/h2&gt;

&lt;p&gt;Python&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from habanero import cn
cn.content_negotiation(ids = &#39;10.1126/science.169.3946.635&#39;, format = &quot;text&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;u&#39;Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635\xe2\x80\x93641. doi:10.1126/science.169.3946.635\n&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ruby&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;require &#39;serrano&#39;
Serrano.content_negotiation(ids: &#39;10.1126/science.169.3946.635&#39;, format: &quot;text&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-ruby&quot;&gt;=&amp;gt; [&quot;Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635\xE2\x80\x93641. doi:10.1126/science.169.3946.635\n&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;rcrossref&quot;)
cr_cn(dois=&quot;10.1126/science.169.3946.635&quot;, &quot;text&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;[1] &quot;Frank, H. S. (1970). The Structure of Ordinary Water: New data and interpretations are yielding new insights into this fascinating substance. Science, 169(3946), 635–641. doi:10.1126/science.169.3946.635&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CLI&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;serrano contneg 10.1890/13-0590.1 --format=text
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;Murtaugh, P. A. (2014).  In defense of P values . Ecology, 95(3), 611–617. doi:10.1890/13-0590.1
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;

&lt;p&gt;There are definitely issues with data in the Crossref search API, some of which I cover in my talks. However, it is still the best place to go for scholarly metadata.&lt;/p&gt;

&lt;p&gt;Let us know of other use cases - there are others not covered here for brevity sake.&lt;/p&gt;

&lt;p&gt;There are lots of examples in the docs for each client. If you can think of any doc improvements file an issue.&lt;/p&gt;

&lt;p&gt;If you find any bugs, please do file an issue.&lt;/p&gt;

</description>
				<published>2015-11-30 00:00:00 -0800</published>
				<link>http://recology.info//2015/11/crossref-clients/</link>
			</item>
		
			<item>
				<title>noaa - Integrated Surface Database data</title>
				<description>&lt;p&gt;I’ve recently made some improvements to the functions that work with ISD 
(Integrated Surface Database) data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;isd data&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;code&gt;isd()&lt;/code&gt; function now caches more intelligently. We now cache using 
&lt;code&gt;.rds&lt;/code&gt; files via &lt;code&gt;saveRDS&lt;/code&gt;/&lt;code&gt;readRDS&lt;/code&gt;, whereas we used to use &lt;code&gt;.csv&lt;/code&gt; files, 
which take up much more disk space, and we have to worry about not changing 
data formats on reading data back into an R session. This has the downside
that you can’t just go directly to open up a cached file in your favorite 
spreadsheet viewer, but you can do that manually after reading in to R.&lt;/li&gt;
  &lt;li&gt;In addition, &lt;code&gt;isd()&lt;/code&gt; now has a function &lt;code&gt;cleanup&lt;/code&gt;, if &lt;code&gt;TRUE&lt;/code&gt; after 
downloading the data file from NOAA’s ftp server and processing, we delete 
the file. That’s fine since we have the cached processed file. But you 
can choose not to cleanup the original data files.&lt;/li&gt;
  &lt;li&gt;Data processing in &lt;code&gt;isd()&lt;/code&gt; is improved as well. We convert key variables
to appropriate classes to be more useful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;isd stations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In &lt;code&gt;isd_stations()&lt;/code&gt;, there’s now a cached version of the station data in 
the package, or you can get optionally get fresh station data from NOAA’s 
FTP server.&lt;/li&gt;
  &lt;li&gt;There’s a new function &lt;code&gt;isd_stations_search()&lt;/code&gt; that uses the station data
to allow you to search for stations via either:
    &lt;ul&gt;
      &lt;li&gt;A bounding box&lt;/li&gt;
      &lt;li&gt;Radius froma point&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;For examples below, you’ll need the development version:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;ropensci/rnoaa&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load &lt;code&gt;rnoaa&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;rnoaa&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;isd-stations&quot;&gt;ISD stations&lt;/h2&gt;

&lt;h3 id=&quot;get-stations&quot;&gt;Get stations&lt;/h3&gt;

&lt;p&gt;There’s a cached version of the station data in the package, or you can get fresh
station data from NOAA’s FTP server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;stations &amp;lt;- isd_stations()
head(stations)
#&amp;gt;   usaf  wban station_name ctry state icao lat lon elev_m    begin      end
#&amp;gt; 1 7005 99999   CWOS 07005                  NA  NA     NA 20120127 20120127
#&amp;gt; 2 7011 99999   CWOS 07011                  NA  NA     NA 20111025 20121129
#&amp;gt; 3 7018 99999   WXPOD 7018                   0   0   7018 20110309 20130730
#&amp;gt; 4 7025 99999   CWOS 07025                  NA  NA     NA 20120127 20120127
#&amp;gt; 5 7026 99999   WXPOD 7026   AF              0   0   7026 20120713 20141120
#&amp;gt; 6 7034 99999   CWOS 07034                  NA  NA     NA 20121024 20121106
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;filter-and-visualize-stations&quot;&gt;Filter and visualize stations&lt;/h3&gt;

&lt;p&gt;In addition to getting the entire station data.frame, you can also search for stations,
either with a bounding box or within a radius from a point. First, the bounding box&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;bbox &amp;lt;- c(-125.0, 38.4, -121.8, 40.9)
out &amp;lt;- isd_stations_search(bbox = bbox)
head(out)
#&amp;gt;     usaf  wban                          station_name ctry state icao
#&amp;gt; 1 720193 99999 LONNIE POOL FLD / WEAVERVILLE AIRPORT   US    CA KO54
#&amp;gt; 2 724834 99999                        POINT CABRILLO   US    CA     
#&amp;gt; 3 724953 99999                              RIO NIDO   US    CA     
#&amp;gt; 4 724957 23213                 SONOMA COUNTY AIRPORT   US    CA KSTS
#&amp;gt; 5 724957 99999                  C M SCHULZ SONOMA CO   US    CA KSTS
#&amp;gt; 6 724970 99999                  CHICO CALIFORNIA MAP   US    CA  CIC
#&amp;gt;   elev_m    begin      end      lon    lat
#&amp;gt; 1  716.0 20101030 20150831 -122.922 40.747
#&amp;gt; 2   20.0 19810906 19871007 -123.820 39.350
#&amp;gt; 3 -999.0 19891111 19900303 -122.917 38.517
#&amp;gt; 4   34.8 20000101 20150831 -122.810 38.504
#&amp;gt; 5   38.0 19430404 19991231 -122.817 38.517
#&amp;gt; 6   69.0 19420506 19760305 -121.850 39.783
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Where is the bounding box? (you’ll need &lt;a href=&quot;https://cran.rstudio.com/web/packages/lawn/&quot;&gt;lawn&lt;/a&gt;, or you can vizualize some other way)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;lawn&quot;)
lawn::lawn_bbox_polygon(bbox) %&amp;gt;% view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-10-21-noaa-isd/bbox_area.png&quot; alt=&quot;plot1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Vizualize station subset - yep, looks right&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;leaflet&quot;)
leaflet(data = out) %&amp;gt;%
  addTiles() %&amp;gt;%
  addCircles()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-10-21-noaa-isd/bbox_result.png&quot; alt=&quot;plot1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, search with a lat/lon coordinate, with a radius. That is, we search for stations
within X km from the coordinate.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;out &amp;lt;- isd_stations_search(lat = 38.4, lon = -123, radius = 250)
head(out)
#&amp;gt;     usaf  wban             station_name ctry state icao elev_m    begin
#&amp;gt; 1 690070 93217            FRITZSCHE AAF   US    CA KOAR   43.0 19600404
#&amp;gt; 2 720267 23224 AUBURN MUNICIPAL AIRPORT   US    CA KAUN  466.7 20060101
#&amp;gt; 3 720267 99999         AUBURN MUNICIPAL   US    CA KAUN  468.0 20040525
#&amp;gt; 4 720406 99999      GNOSS FIELD AIRPORT   US    CA KDVO    0.6 20071114
#&amp;gt; 5 720576   174       UNIVERSITY AIRPORT   US    CA KEDU   21.0 20130101
#&amp;gt; 6 720576 99999                    DAVIS   US    CA KEDU   21.0 20080721
#&amp;gt;        end      lon    lat
#&amp;gt; 1 19930831 -121.767 36.683
#&amp;gt; 2 20150831 -121.082 38.955
#&amp;gt; 3 20051231 -121.082 38.955
#&amp;gt; 4 20150831 -122.550 38.150
#&amp;gt; 5 20150831 -121.783 38.533
#&amp;gt; 6 20121231 -121.783 38.533
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Again, compare search area to stations found&lt;/p&gt;

&lt;p&gt;&lt;em&gt;search area&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;pt &amp;lt;- lawn::lawn_point(c(-123, 38.4))
lawn::lawn_buffer(pt, dist = 250) %&amp;gt;% view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-10-21-noaa-isd/circle_radius.png&quot; alt=&quot;plot1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;stations found&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;leaflet(data = out) %&amp;gt;%
  addTiles() %&amp;gt;%
  addCircles()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-10-21-noaa-isd/lastplot.png&quot; alt=&quot;plot1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;isd-data&quot;&gt;ISD data&lt;/h2&gt;

&lt;h3 id=&quot;get-isd-data&quot;&gt;Get ISD data&lt;/h3&gt;

&lt;p&gt;Here, I get data for four stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res1 &amp;lt;- isd(usaf=&quot;011690&quot;, wban=&quot;99999&quot;, year=1993)
res2 &amp;lt;- isd(usaf=&quot;172007&quot;, wban=&quot;99999&quot;, year=2015)
res3 &amp;lt;- isd(usaf=&quot;702700&quot;, wban=&quot;00489&quot;, year=2015)
res4 &amp;lt;- isd(usaf=&quot;109711&quot;, wban=99999, year=1970)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, combine data, with &lt;code&gt;rnoaa:::rbind.isd()&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res_all &amp;lt;- rbind(res1, res2, res3, res4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add date time&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;lubridate&quot;)
res_all$date_time &amp;lt;- ymd_hm(
  sprintf(&quot;%s %s&quot;, as.character(res_all$date), res_all$time)
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Remove 999’s (NOAA’s way to indicate missing/no data)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;dplyr&quot;)
res_all &amp;lt;- res_all %&amp;gt;% filter(temperature &amp;lt; 900)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;visualize-isd-data&quot;&gt;Visualize ISD data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;ggplot2&quot;)
ggplot(res_all, aes(date_time, temperature)) +
  geom_line() + 
  facet_wrap(~usaf_station, scales = &quot;free_x&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-10-21-noaa-isd/unnamed-chunk-12-1.png&quot; alt=&quot;img&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-10-21 00:00:00 -0700</published>
				<link>http://recology.info//2015/10/noaa-isd/</link>
			</item>
		
			<item>
				<title>Metrics for open source projects</title>
				<description>&lt;p&gt;Measuring use of open source software isn’t always straightforward. The problem is especially acute for software targeted largely at academia, where usage is not measured just by software downloads, but also by citations.&lt;/p&gt;

&lt;p&gt;Citations are a well-known pain point because the citation graph is privately held by iron doors (e.g., &lt;a href=&quot;http://www.scopus.com/&quot;&gt;Scopus&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/&quot;&gt;Google Scholar&lt;/a&gt;). New ventures aim to open up citation data, but of course it’s an immense amount of work, and so does not come quickly.&lt;/p&gt;

&lt;p&gt;The following is a laundry list of metrics on software of which I am aware, and some of which I use in our &lt;a href=&quot;http://ropensci.github.io/biweekly/&quot;&gt;rOpenSci twice monthly updates&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I primarily develop software for the R language, so some of the metrics are specific to R, but many are not. In addition, we (rOpenSci) don’t develop web apps, which may bring in an additional set of metrics not covered below.&lt;/p&gt;

&lt;p&gt;I organize by source instead of type of data because some sources give multiple kinds of data - I note what kinds of data they give with &lt;span class=&quot;label label-default&quot;&gt;labels&lt;/span&gt;.&lt;/p&gt;

&lt;h2 id=&quot;cran-downloads&quot;&gt;CRAN downloads&lt;/h2&gt;

&lt;p&gt;&lt;span class=&quot;label label-warning&quot;&gt;downloads&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Link: &lt;a href=&quot;https://github.com/metacran/cranlogs.app&quot;&gt;https://github.com/metacran/cranlogs.app&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;This is a REST API for CRAN downloads from the RStudio CRAN CDN. Note however, that the RStudio CDN is only one of many - there are other mirrors users can insall packages from, and are not included in this count. However, a significant portion of downloads probably come from the RStudio CDN.&lt;/li&gt;
  &lt;li&gt;Other programming languages have similar support, e.g., &lt;a href=&quot;http://guides.rubygems.org/rubygems-org-api/&quot;&gt;Ruby&lt;/a&gt; and &lt;a href=&quot;https://github.com/npm/download-counts&quot;&gt;Node&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lagotto&quot;&gt;Lagotto&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;span class=&quot;label label-success&quot;&gt;citations&lt;/span&gt; &lt;span class=&quot;label label-info&quot;&gt;github&lt;/span&gt; &lt;span class=&quot;label label-primary&quot;&gt;social-media&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Link: &lt;a href=&quot;http://software.lagotto.io/works&quot;&gt;http://software.lagotto.io/works&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lagotto is a Rails application, developed by &lt;a href=&quot;https://github.com/mfenner&quot;&gt;Martin Fenner&lt;/a&gt;, originally designed to collect and provide article level metrics for scientific publications at Public Library of Science. It is now used by many publishers, and there are installations of Lagotto targeting &lt;a href=&quot;http://mdc.lagotto.io/&quot;&gt;datasets&lt;/a&gt; and &lt;a href=&quot;http://software.lagotto.io/works&quot;&gt;software&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Discussion forum: &lt;a href=&quot;http://discuss.lagotto.io/&quot;&gt;http://discuss.lagotto.io/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;depsy&quot;&gt;Depsy&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;span class=&quot;label label-success&quot;&gt;citations&lt;/span&gt; &lt;span class=&quot;label label-info&quot;&gt;github&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Link: &lt;a href=&quot;http://depsy.org&quot;&gt;http://depsy.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;This is a nascent venture by the &lt;a href=&quot;https://impactstory.org/about&quot;&gt;ImpactStory team&lt;/a&gt; that seeks to uncover the impact of research software. As far as I can tell, they’ll collect usage via software downloads and citations in the literature.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;web-site-analytics&quot;&gt;Web Site Analytics&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;span class=&quot;label label-danger&quot;&gt;page-views&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If you happen to have a website for your project, collecting analytics is a way to gauge views of the landing page, and any help/tutorial pages you may have. A good easy way to do this is a deploy a basic site on your &lt;code&gt;gh-pages&lt;/code&gt; branch of your GitHub repo, and use the easily integrated Google Analytics.&lt;/li&gt;
  &lt;li&gt;Whatever analytics you use, in my experience this mostly brings up links from google searches and blog posts that may mention your project&lt;/li&gt;
  &lt;li&gt;Google Analytics beacon (for README views): &lt;a href=&quot;https://github.com/igrigorik/ga-beacon&quot;&gt;https://github.com/igrigorik/ga-beacon&lt;/a&gt;. I haven’t tried this yet, but seems promising.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;auomated-tracking-ssnmp&quot;&gt;Auomated tracking: SSNMP&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;span class=&quot;label label-success&quot;&gt;citations&lt;/span&gt; &lt;span class=&quot;label label-info&quot;&gt;github&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Link: &lt;a href=&quot;http://scisoft-net-map.isri.cmu.edu&quot;&gt;http://scisoft-net-map.isri.cmu.edu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Scientific Software Network Map Project&lt;/li&gt;
  &lt;li&gt;This is a cool NSF funded project by Chris Bogart that tracks software usage via GitHub and citations in literature.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;google-scholar&quot;&gt;Google Scholar&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;span class=&quot;label label-success&quot;&gt;citations&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Link: &lt;a href=&quot;https://scholar.google.com/&quot;&gt;https://scholar.google.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Searching Google Scholar for software citations manually is fine at a small scale, but at a larger scale scraping is best. However, you’re not legally supposed to do this, and Google will shut you down.&lt;/li&gt;
  &lt;li&gt;Could try using g-scholar alerts as well, especially if new citations of your work are infrequent.&lt;/li&gt;
  &lt;li&gt;If you have institutional access to Scopus/Web of Science, you could search those, but I don’t push this as an option since it’s available to so few.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;github&quot;&gt;GitHub&lt;/h2&gt;

&lt;p&gt;&lt;small&gt;&lt;span class=&quot;label label-info&quot;&gt;github&lt;/span&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Links: &lt;a href=&quot;https://developer.github.com/v3/&quot;&gt;https://developer.github.com/v3/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;I keep a list of rOpenSci uses found in GitHub repos at &lt;a href=&quot;https://discuss.ropensci.org/t/use-of-some-ropensci-packages-on-github/137&quot;&gt;https://discuss.ropensci.org/t/use-of-some-ropensci-packages-on-github/137&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GitHub does collect traffic data on each repo (clones, downloads, page views), but they are not exposed in the API. I’ve bugged them a bit about this - hopefully we’ll be able to get that dat in their API soon.&lt;/li&gt;
  &lt;li&gt;Bitbucket/Gitlab - don’t use them, but I assume they also provide some metrics via their APIs&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other&quot;&gt;Other&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Support forums: Whether you use UserVoice, Discourse, Google Groups, Gitter, etc., depending on your viewpoint, these interactions could be counted as metrics of software usage.&lt;/li&gt;
  &lt;li&gt;Emails: I personally get a lot of emails asking for help with software I maintain. I imagine this is true for most software developers. Counting these could be another metric of software usage, although I never have counted mine.&lt;/li&gt;
  &lt;li&gt;Social media: See Lagotto above, which tracks some social media outlets.&lt;/li&gt;
  &lt;li&gt;Code coverage: There are many options now for code coverage, integrated with each Travis-CI build. A good option is &lt;a href=&quot;https://codecov.io&quot;&gt;CodeCov&lt;/a&gt;. CodeCov gives percentage test coverage, which one could use as one measure of code quality.&lt;/li&gt;
  &lt;li&gt;Reviews: There isn’t a lot of code review going on that I’m aware of. Even if there was, I suppose this would just be a logical TRUE/FALSE.&lt;/li&gt;
  &lt;li&gt;Cash money y’all: Grants/consulting income/etc. could be counted as a metric.&lt;/li&gt;
  &lt;li&gt;Users: If you require users to create an account or similar before getting your software, you have a sense of number of users and perhaps their demographics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;promising&quot;&gt;Promising&lt;/h2&gt;

&lt;p&gt;Some software metrics things on the horizon that look interesting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://geodynamics.org/cig/projects/saga/&quot;&gt;Software Attribution for Geoscience Applications&lt;/a&gt; (SAGA)&lt;/li&gt;
  &lt;li&gt;Crossref: They have &lt;a href=&quot;https://github.com/CrossRef/rest-api-doc/blob/master/rest_api.md&quot;&gt;a very nice API&lt;/a&gt;, but they don’t yet provide citation counts - but &lt;a href=&quot;https://github.com/CrossRef/rest-api-doc/issues/46&quot;&gt;they may soon&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/njsmith/sempervirens&quot;&gt;njsmith/sempervirens&lt;/a&gt; - a prototype for &lt;em&gt;gathering anonymous, opt-in usage data for open scientific software&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/force11/force11-scwg&quot;&gt;Force11 Software Citation Working Group&lt;/a&gt; - &lt;em&gt;…produce a consolidated set of citation principles in order to encourage broad adoption of a consistent policy for software citation across disciplines and venues&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;missed&quot;&gt;Missed?&lt;/h2&gt;

&lt;p&gt;I’m sure I missed things. Let me know.&lt;/p&gt;

</description>
				<published>2015-10-19 00:00:00 -0700</published>
				<link>http://recology.info//2015/10/open-source-metrics/</link>
			</item>
		
			<item>
				<title>analogsea - an R client for the Digital Ocean API</title>
				<description>&lt;p&gt;&lt;code&gt;analogsea&lt;/code&gt; is now on CRAN. We started developing the pkg back in &lt;a href=&quot;https://github.com/sckott/analogsea/commit/b129164dd87969d2fc6bcf3b51576fe1da932fdb&quot;&gt;May 2014&lt;/a&gt;, but just 
now getting the first version on CRAN. It’s a collaboration with &lt;a href=&quot;http://had.co.nz/&quot;&gt;Hadley&lt;/a&gt; and &lt;a href=&quot;https://github.com/wch/&quot;&gt;Winston Chang&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Most of &lt;code&gt;analogsea&lt;/code&gt; package is for interacting with the &lt;a href=&quot;https://developers.digitalocean.com/documentation/v2/&quot;&gt;Digital Ocean API&lt;/a&gt;, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Manage domains&lt;/li&gt;
  &lt;li&gt;Manage ssh keys&lt;/li&gt;
  &lt;li&gt;Get actions&lt;/li&gt;
  &lt;li&gt;Manage images&lt;/li&gt;
  &lt;li&gt;Manage droplets (servers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A number of convenience functions are included for doing tasks (e.g., resizing 
a droplet) that aren’t supported by Digital Ocean’s API out of the box (i.e., 
there’s no API route for it).&lt;/p&gt;

&lt;p&gt;In addition to wrapping their API routes, we provide other functionality, e.g.:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;execute shell commands on a droplet (server)&lt;/li&gt;
  &lt;li&gt;execute R commands on a droplet&lt;/li&gt;
  &lt;li&gt;install R&lt;/li&gt;
  &lt;li&gt;install RStudio server&lt;/li&gt;
  &lt;li&gt;install Shiny server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other functionality we’re working on, not yet available:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;install OpenCPU&lt;/li&gt;
  &lt;li&gt;use &lt;code&gt;packrat&lt;/code&gt; to move projects from local to server, and vice versa&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See also: two previous blog posts on this package &lt;a href=&quot;http://recology.info/2014/05/analogsea/&quot;&gt;http://recology.info/2014/05/analogsea/&lt;/a&gt; and &lt;a href=&quot;http://recology.info/2014/06/analogsea-v01/&quot;&gt;http://recology.info/2014/06/analogsea-v01/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;Binaries are not yet on CRAN, but you can install from source.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;# install.packages(&quot;analogsea&quot;) # when binaries available
install.packages(&quot;analogsea&quot;, repos = &quot;https://cran.r-project.org&quot;, type = &quot;source&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or install development version from GitHub&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;sckott/analogsea&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load &lt;code&gt;analogsea&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;analogsea&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;etc&quot;&gt;Etc.&lt;/h2&gt;

&lt;p&gt;As this post is mostly to announce that this pkg is on CRAN now, I won’t go through examples, but instead point you to the package &lt;a href=&quot;https://github.com/sckott/analogsea/blob/master/README.md&quot;&gt;README&lt;/a&gt; and &lt;a href=&quot;https://github.com/sckott/analogsea/blob/master/vignettes/doapi.Rmd&quot;&gt;vignette&lt;/a&gt; in which we cover 
creating a Digital Ocean account, authenticating, and have many examples.&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;Let us know what you think. We’d love to hear about any problems, use cases, feature requests.&lt;/p&gt;

</description>
				<published>2015-10-02 00:00:00 -0700</published>
				<link>http://recology.info//2015/10/analogsea-cran/</link>
			</item>
		
			<item>
				<title>oai - an OAI-PMH client</title>
				<description>&lt;p&gt;&lt;code&gt;oai&lt;/code&gt; is a general purpose client to work with any ‘OAI-PMH’ service. The ‘OAI-PMH’ protocol is described at &lt;a href=&quot;http://www.openarchives.org/OAI/openarchivesprotocol.html&quot;&gt;http://www.openarchives.org/OAI/openarchivesprotocol.html&lt;/a&gt;. The main functions follow the OAI-PMH verbs:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;GetRecord&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Identify&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ListIdentifiers&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ListMetadataFormats&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ListRecords&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;ListSets&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The repo is at &lt;a href=&quot;https://github.com/sckott/oai&quot;&gt;https://github.com/sckott/oai&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will be using this in a number of packages I maintain that use OAI-PMH data services. If you try it, let me know what you think.&lt;/p&gt;

&lt;p&gt;This package is heading to rOpenSci soon: &lt;a href=&quot;https://github.com/ropensci/onboarding/issues/19&quot;&gt;https://github.com/ropensci/onboarding/issues/19&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here’s a few usage examples:&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;Is on CRAN now, but binaries may not be available yet.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;install.packages(&quot;oai&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or install development version from GitHub&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;sckott/oai&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load &lt;code&gt;oai&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;oai&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;identify&quot;&gt;Identify&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;id(&quot;http://oai.datacite.org/oai&quot;)
#&amp;gt;   repositoryName                     baseURL protocolVersion
#&amp;gt; 1   DataCite MDS http://oai.datacite.org/oai             2.0
#&amp;gt;           adminEmail    earliestDatestamp deletedRecord
#&amp;gt; 1 admin@datacite.org 2011-01-01T00:00:00Z    persistent
#&amp;gt;            granularity compression compression.1
#&amp;gt; 1 YYYY-MM-DDThh:mm:ssZ        gzip       deflate
#&amp;gt;                                      description
#&amp;gt; 1 oaioai.datacite.org:oai:oai.datacite.org:12425
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;listidentifiers&quot;&gt;ListIdentifiers&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;list_identifiers(from = &#39;2011-05-01T&#39;, until = &#39;2011-09-01T&#39;)
#&amp;gt; &amp;lt;ListRecords&amp;gt; 925 X 6 
#&amp;gt; 
#&amp;gt;                    identifier            datestamp setSpec setSpec.1
#&amp;gt; 1  oai:oai.datacite.org:32153 2011-06-08T08:57:11Z     TIB  TIB.WDCC
#&amp;gt; 2  oai:oai.datacite.org:32200 2011-06-20T08:12:41Z     TIB TIB.DAGST
#&amp;gt; 3  oai:oai.datacite.org:32220 2011-06-28T14:11:08Z     TIB TIB.DAGST
#&amp;gt; 4  oai:oai.datacite.org:32241 2011-06-30T13:24:45Z     TIB TIB.DAGST
#&amp;gt; 5  oai:oai.datacite.org:32255 2011-07-01T12:09:24Z     TIB TIB.DAGST
#&amp;gt; 6  oai:oai.datacite.org:32282 2011-07-05T09:08:10Z     TIB TIB.DAGST
#&amp;gt; 7  oai:oai.datacite.org:32309 2011-07-06T12:30:54Z     TIB TIB.DAGST
#&amp;gt; 8  oai:oai.datacite.org:32310 2011-07-06T12:42:32Z     TIB TIB.DAGST
#&amp;gt; 9  oai:oai.datacite.org:32325 2011-07-07T11:17:46Z     TIB TIB.DAGST
#&amp;gt; 10 oai:oai.datacite.org:32326 2011-07-07T11:18:47Z     TIB TIB.DAGST
#&amp;gt; ..                        ...                  ...     ...       ...
#&amp;gt; Variables not shown: setSpec.2 (chr), setSpec.3 (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;count-identifiers&quot;&gt;Count Identifiers&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;count_identifiers()
#&amp;gt;                           url   count
#&amp;gt; 1 http://oai.datacite.org/oai 6350706
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;listrecords&quot;&gt;ListRecords&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;list_records(from = &#39;2011-05-01T&#39;, until = &#39;2011-08-15T&#39;)
#&amp;gt; &amp;lt;ListRecords&amp;gt; 126 X 46 
#&amp;gt; 
#&amp;gt;                    identifier            datestamp setSpec setSpec.1
#&amp;gt; 1  oai:oai.datacite.org:32153 2011-06-08T08:57:11Z     TIB  TIB.WDCC
#&amp;gt; 2  oai:oai.datacite.org:32200 2011-06-20T08:12:41Z     TIB TIB.DAGST
#&amp;gt; 3  oai:oai.datacite.org:32220 2011-06-28T14:11:08Z     TIB TIB.DAGST
#&amp;gt; 4  oai:oai.datacite.org:32241 2011-06-30T13:24:45Z     TIB TIB.DAGST
#&amp;gt; 5  oai:oai.datacite.org:32255 2011-07-01T12:09:24Z     TIB TIB.DAGST
#&amp;gt; 6  oai:oai.datacite.org:32282 2011-07-05T09:08:10Z     TIB TIB.DAGST
#&amp;gt; 7  oai:oai.datacite.org:32309 2011-07-06T12:30:54Z     TIB TIB.DAGST
#&amp;gt; 8  oai:oai.datacite.org:32310 2011-07-06T12:42:32Z     TIB TIB.DAGST
#&amp;gt; 9  oai:oai.datacite.org:32325 2011-07-07T11:17:46Z     TIB TIB.DAGST
#&amp;gt; 10 oai:oai.datacite.org:32326 2011-07-07T11:18:47Z     TIB TIB.DAGST
#&amp;gt; ..                        ...                  ...     ...       ...
#&amp;gt; Variables not shown: title (chr), creator (chr), creator.1 (chr),
#&amp;gt;      creator.2 (chr), creator.3 (chr), creator.4 (chr), creator.5 (chr),
#&amp;gt;      creator.6 (chr), creator.7 (chr), publisher (chr), date (chr),
#&amp;gt;      identifier.2 (chr), identifier.1 (chr), subject (chr), description
#&amp;gt;      (chr), description.1 (chr), contributor (chr), language (chr), type
#&amp;gt;      (chr), type.1 (chr), format (chr), format.1 (chr), rights (chr),
#&amp;gt;      subject.1 (chr), relation (chr), subject.2 (chr), subject.3 (chr),
#&amp;gt;      subject.4 (chr), setSpec.2 (chr), setSpec.3 (chr), format.2 (chr),
#&amp;gt;      subject.5 (chr), subject.6 (chr), subject.7 (chr), description.2
#&amp;gt;      (chr), description.3 (chr), description.4 (chr), description.5 (chr),
#&amp;gt;      title.1 (chr), relation.1 (chr), relation.2 (chr), contributor.1
#&amp;gt;      (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;getrecords&quot;&gt;GetRecords&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;get_records(c(&quot;oai:oai.datacite.org:32255&quot;, &quot;oai:oai.datacite.org:32325&quot;))
#&amp;gt; &amp;lt;GetRecord&amp;gt; 2 X 23 
#&amp;gt; 
#&amp;gt;                   identifier            datestamp setSpec setSpec.1
#&amp;gt; 1 oai:oai.datacite.org:32255 2011-07-01T12:09:24Z     TIB TIB.DAGST
#&amp;gt; 2 oai:oai.datacite.org:32325 2011-07-07T11:17:46Z     TIB TIB.DAGST
#&amp;gt; Variables not shown: title (chr), creator (chr), creator.1 (chr),
#&amp;gt;      creator.2 (chr), creator.3 (chr), publisher (chr), date (chr),
#&amp;gt;      identifier.1 (chr), subject (chr), subject.1 (chr), description
#&amp;gt;      (chr), description.1 (chr), contributor (chr), language (chr), type
#&amp;gt;      (chr), type.1 (chr), format (chr), format.1 (chr), rights (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;list-metadataformats&quot;&gt;List MetadataFormats&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;list_metadataformats(id = &quot;oai:oai.datacite.org:32348&quot;)
#&amp;gt; $`oai:oai.datacite.org:32348`
#&amp;gt;   metadataPrefix
#&amp;gt; 1         oai_dc
#&amp;gt; 2       datacite
#&amp;gt; 3   oai_datacite
#&amp;gt;                                                        schema
#&amp;gt; 1              http://www.openarchives.org/OAI/2.0/oai_dc.xsd
#&amp;gt; 2 http://schema.datacite.org/meta/nonexistant/nonexistant.xsd
#&amp;gt; 3              http://schema.datacite.org/oai/oai-1.0/oai.xsd
#&amp;gt;                             metadataNamespace
#&amp;gt; 1 http://www.openarchives.org/OAI/2.0/oai_dc/
#&amp;gt; 2      http://datacite.org/schema/nonexistant
#&amp;gt; 3     http://schema.datacite.org/oai/oai-1.0/
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;list-sets&quot;&gt;List Sets&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;list_sets(&quot;http://oai.datacite.org/oai&quot;)
#&amp;gt; &amp;lt;ListSets&amp;gt; 1227 X 2 
#&amp;gt; 
#&amp;gt;                     setSpec
#&amp;gt; 1                REFQUALITY
#&amp;gt; 2                      ANDS
#&amp;gt; 3           ANDS.REFQUALITY
#&amp;gt; 4             ANDS.CENTRE-1
#&amp;gt; 5  ANDS.CENTRE-1.REFQUALITY
#&amp;gt; 6             ANDS.CENTRE-2
#&amp;gt; 7  ANDS.CENTRE-2.REFQUALITY
#&amp;gt; 8             ANDS.CENTRE-3
#&amp;gt; 9  ANDS.CENTRE-3.REFQUALITY
#&amp;gt; 10            ANDS.CENTRE-5
#&amp;gt; ..                      ...
#&amp;gt; Variables not shown: setName (chr)
&lt;/code&gt;&lt;/pre&gt;
</description>
				<published>2015-09-11 00:00:00 -0700</published>
				<link>http://recology.info//2015/09/oai-client/</link>
			</item>
		
			<item>
				<title>fulltext - a package to help you mine text</title>
				<description>&lt;p&gt;Finally, we got &lt;code&gt;fulltext&lt;/code&gt; up on CRAN - our first commit was &lt;a href=&quot;https://github.com/ropensci/fulltext/commit/2d4f7e270040b2c8914853113073fc4d3134445e&quot;&gt;May last year&lt;/a&gt;. &lt;code&gt;fulltext&lt;/code&gt; is a package to facilitate text mining. It focuses on open access journals. This package makes it easier to search for articles, download those articles in full text if available, convert pdf format to plain text, and extract text chunks for vizualization/analysis. We are planning to add bits for analysis in future versions. We’ve been working on this package for a while now. It has a lot of moving parts and package dependencies, so it took a while to get a first useable version.&lt;/p&gt;

&lt;p&gt;The tasks facilitated by &lt;code&gt;fulltext&lt;/code&gt; in bullet form:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Search - search for articles&lt;/li&gt;
  &lt;li&gt;Retrieve - get full text&lt;/li&gt;
  &lt;li&gt;Convert - convert from format X to Y&lt;/li&gt;
  &lt;li&gt;Text - if needed, get text from pdfs/etc.&lt;/li&gt;
  &lt;li&gt;Extract - pull out the bits of articles that you want&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I won’t be surprised if users uncover a lot of bugs in this package given the huge number of publishers/journals users want to get literature data from, and the surely wide diversity of use cases. But I thought it was important to get out a first version to get feedback on the user interface, and gather use cases.&lt;/p&gt;

&lt;p&gt;We hope that this package can help bring text-mining to the masses - making it easy for anyone to do do, not just text-mining experts.&lt;/p&gt;

&lt;p&gt;If you have any feedback, please do get in touch in the issue tracker for &lt;code&gt;fulltext&lt;/code&gt; at https://github.com/ropensci/fulltext/issues - If you have use case thoughts, the &lt;a href=&quot;https://discuss.ropensci.org/&quot;&gt;rOpenSci discussion forum&lt;/a&gt; might be a good place to go.&lt;/p&gt;

&lt;p&gt;Let’s kick the tires, shall we?&lt;/p&gt;

&lt;h2 id=&quot;install&quot;&gt;Install&lt;/h2&gt;

&lt;p&gt;Will be on CRAN soon, not as of AM PDT on 2015-08-07.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;install.packages(&quot;fulltext&quot;)
# if binaries not avail. yet on your favorite CRAN mirror
install.packages(&quot;https://cran.rstudio.com/src/contrib/fulltext_0.1.0.tar.gz&quot;, repos = NULL, type = &quot;source&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or install development version from GitHub&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;ropensci/fulltext&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Load &lt;code&gt;fulltext&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;fulltext&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;search-for-articles&quot;&gt;Search for articles&lt;/h2&gt;

&lt;p&gt;Currently, there are hooks for searching for articles from PLOS, BMC, Crossref, Entrez, arXiv, and BioRxiv. We’ll add more in the future, but that does cover a lot of articles, especially given inclusion of Crossref (which mints most DOIs) and Entrez (which houses PMC and Pubmed).&lt;/p&gt;

&lt;p&gt;An example: Search for the term &lt;em&gt;ecology&lt;/em&gt; in PLOS journals.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(res1 &amp;lt;- ft_search(query = &#39;ecology&#39;, from = &#39;plos&#39;))
#&amp;gt; Query:
#&amp;gt;   [ecology] 
#&amp;gt; Found:
#&amp;gt;   [PLoS: 28589; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] 
#&amp;gt; Returned:
#&amp;gt;   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each publisher/search-engine has a slot with metadata and data, saying how many articles were found and how many were returned. We can dig into what PLOS gave us:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res1$plos
#&amp;gt; Query: [ecology] 
#&amp;gt; Records found, returned: [28589, 10] 
#&amp;gt; License: [CC-BY] 
#&amp;gt;                                                         id
#&amp;gt; 1                             10.1371/journal.pone.0059813
#&amp;gt; 2                             10.1371/journal.pone.0001248
#&amp;gt; 3  10.1371/annotation/69333ae7-757a-4651-831c-f28c5eb02120
#&amp;gt; 4                             10.1371/journal.pone.0080763
#&amp;gt; 5                             10.1371/journal.pone.0102437
#&amp;gt; 6                             10.1371/journal.pone.0017342
#&amp;gt; 7                             10.1371/journal.pone.0091497
#&amp;gt; 8                             10.1371/journal.pone.0092931
#&amp;gt; 9  10.1371/annotation/28ac6052-4f87-4b88-a817-0cd5743e83d6
#&amp;gt; 10                            10.1371/journal.pcbi.1003594
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For each of the data sources to search on you can pass in additional options (basically, you can use the query parameters in the functions that hit each service). Here, we can modify our search to PLOS by requesting a particular set of fields with the &lt;code&gt;fl&lt;/code&gt; parameter (PLOS uses a Solr backed search engine, and &lt;code&gt;fl&lt;/code&gt; is short for &lt;code&gt;fields&lt;/code&gt; in Solr land):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;ft_search(query = &#39;ecology&#39;, from = &#39;plos&#39;, plosopts = list(
   fl = c(&#39;id&#39;,&#39;author&#39;,&#39;eissn&#39;,&#39;journal&#39;,&#39;counter_total_all&#39;,&#39;alm_twitterCount&#39;)))
#&amp;gt; Query:
#&amp;gt;   [ecology] 
#&amp;gt; Found:
#&amp;gt;   [PLoS: 28589; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] 
#&amp;gt; Returned:
#&amp;gt;   [PLoS: 10; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that PLOS is a bit unique in allowing you to request specific parts of articles. Other sources in ft_search() don’t let you do that.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;get-full-text&quot;&gt;Get full text&lt;/h2&gt;

&lt;p&gt;After you’ve found the set of articles you want to get full text for, we can use the results from &lt;code&gt;ft_search()&lt;/code&gt; to grab full text. &lt;code&gt;ft_get()&lt;/code&gt; accepts a character vector of list of DOIs (or PMC IDs if fetching from Entrez), or the output of &lt;code&gt;ft_search()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(out &amp;lt;- ft_get(res1))
#&amp;gt; [Docs] 8 
#&amp;gt; [Source] R session  
#&amp;gt; [IDs] 10.1371/journal.pone.0059813 10.1371/journal.pone.0001248
#&amp;gt;      10.1371/journal.pone.0080763 10.1371/journal.pone.0102437
#&amp;gt;      10.1371/journal.pone.0017342 10.1371/journal.pone.0091497
#&amp;gt;      10.1371/journal.pone.0092931 10.1371/journal.pcbi.1003594 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We got eight articles in full text in the result. We didn’t get 10, even though 10 were returned from &lt;code&gt;ft_search()&lt;/code&gt; because PLOS often returns records for annotations, that is, comments on articles, which we auto-seive out within &lt;code&gt;ft_get()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Dig in to the PLOS data&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;out$plos
#&amp;gt; $found
#&amp;gt; [1] 8
#&amp;gt; 
#&amp;gt; $dois
#&amp;gt; [1] &quot;10.1371/journal.pone.0059813&quot; &quot;10.1371/journal.pone.0001248&quot;
#&amp;gt; [3] &quot;10.1371/journal.pone.0080763&quot; &quot;10.1371/journal.pone.0102437&quot;
#&amp;gt; [5] &quot;10.1371/journal.pone.0017342&quot; &quot;10.1371/journal.pone.0091497&quot;
#&amp;gt; [7] &quot;10.1371/journal.pone.0092931&quot; &quot;10.1371/journal.pcbi.1003594&quot;
#&amp;gt; 
#&amp;gt; $data
#&amp;gt; $data$backend
#&amp;gt; NULL
#&amp;gt; 
#&amp;gt; $data$path
#&amp;gt; [1] &quot;session&quot;
#&amp;gt; 
#&amp;gt; $data$data
#&amp;gt; 8 full-text articles retrieved 
#&amp;gt; Min. Length: 3828 - Max. Length: 104702 
#&amp;gt; DOIs: 10.1371/journal.pone.0059813 10.1371/journal.pone.0001248
#&amp;gt;   10.1371/journal.pone.0080763 10.1371/journal.pone.0102437
#&amp;gt;   10.1371/journal.pone.0017342 10.1371/journal.pone.0091497
#&amp;gt;   10.1371/journal.pone.0092931 10.1371/journal.pcbi.1003594 ... 
#&amp;gt; 
#&amp;gt; NOTE: extract xml strings like output[&#39;&amp;lt;doi&amp;gt;&#39;]
#&amp;gt; 
#&amp;gt; $opts
#&amp;gt; $opts$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0059813&quot; &quot;10.1371/journal.pone.0001248&quot;
#&amp;gt; [3] &quot;10.1371/journal.pone.0080763&quot; &quot;10.1371/journal.pone.0102437&quot;
#&amp;gt; [5] &quot;10.1371/journal.pone.0017342&quot; &quot;10.1371/journal.pone.0091497&quot;
#&amp;gt; [7] &quot;10.1371/journal.pone.0092931&quot; &quot;10.1371/journal.pcbi.1003594&quot;
#&amp;gt; 
#&amp;gt; $opts$callopts
#&amp;gt; list()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Dig in further to get to one of the articles in XML format&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;xml2&quot;)
xml2::read_xml(out$plos$data$data$`10.1371/journal.pone.0059813`)
#&amp;gt; {xml_document}
#&amp;gt; &amp;lt;article&amp;gt;
#&amp;gt; [1] &amp;lt;front&amp;gt;\n&amp;lt;journal-meta&amp;gt;\n&amp;lt;journal-id journal-id-type=&quot;nlm-ta&quot;&amp;gt;PLoS O ...
#&amp;gt; [2] &amp;lt;body&amp;gt;\n  &amp;lt;sec id=&quot;s1&quot;&amp;gt;\n&amp;lt;title&amp;gt;Introduction&amp;lt;/title&amp;gt;\n&amp;lt;p&amp;gt;Ecologists  ...
#&amp;gt; [3] &amp;lt;back&amp;gt;\n&amp;lt;ack&amp;gt;\n&amp;lt;p&amp;gt;Curtis Flather, Mark Burgman, Leon Blaustein, Yaac ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now with the xml, you can dig into whatever you like, e.g., using &lt;code&gt;xml2&lt;/code&gt; or &lt;code&gt;rvest&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;extract-text-from-pdfs&quot;&gt;Extract text from pdfs&lt;/h2&gt;

&lt;p&gt;Ideally for text mining you have access to XML or other text based formats. However, sometimes you only have access to PDFs. In this case you want to extract text from PDFs. &lt;code&gt;fulltext&lt;/code&gt; can help with that.&lt;/p&gt;

&lt;p&gt;You can extract from any pdf from a file path, like:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;path &amp;lt;- system.file(&quot;examples&quot;, &quot;example1.pdf&quot;, package = &quot;fulltext&quot;)
ft_extract(path)
#&amp;gt; &amp;lt;document&amp;gt;/Library/Frameworks/R.framework/Versions/3.2/Resources/library/fulltext/examples/example1.pdf
#&amp;gt;   Pages: 18
#&amp;gt;   Title: Suffering and mental health among older people living in nursing homes---a mixed-methods study
#&amp;gt;   Producer: pdfTeX-1.40.10
#&amp;gt;   Creation date: 2015-07-17
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s search for articles from arXiv, a preprint service. Here, get pdf from an article with ID &lt;code&gt;cond-mat/9309029&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res &amp;lt;- ft_get(&#39;cond-mat/9309029&#39;, from = &quot;arxiv&quot;)
res2 &amp;lt;- ft_extract(res)
res2$arxiv$data
#&amp;gt; $backend
#&amp;gt; NULL
#&amp;gt; 
#&amp;gt; $path
#&amp;gt; $path$`cond-mat/9309029`
#&amp;gt; [1] &quot;~/.fulltext/cond-mat_9309029.pdf&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $data
#&amp;gt; $data[[1]]
#&amp;gt; &amp;lt;document&amp;gt;/Users/sacmac/.fulltext/cond-mat_9309029.pdf
#&amp;gt;   Pages: 14
#&amp;gt;   Title: arXiv:cond-mat/9309029v8  26 Jan 1994
#&amp;gt;   Producer: GPL Ghostscript SVN PRE-RELEASE 8.62
#&amp;gt;   Creation date: 2008-02-06
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And a short snippet of the full text&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res2$arxiv$data$data[[1]]$data
#&amp;gt; &quot;arXiv:cond-mat/9309029v8 26 Jan 1994, , FERMILAB-PUB-93/15-T March 1993, Revised:
#&amp;gt; January 1994, The Thermodynamics and Economics of Waste, Dallas C. Kennedy, Research
#&amp;gt; Associate, Fermi National Accelerator Laboratory, P.O. Box 500 MS106, Batavia, Illinois
#&amp;gt; 60510 USA, Abstract, The increasingly relevant problem of natural resource use and
#&amp;gt; waste production, disposal, and reuse is examined from several viewpoints: economic,
#&amp;gt; technical, and thermodynamic. Alternative economies are studied, with emphasis on
#&amp;gt; recycling of waste to close the natural resource cycle. The physical nature of human
#&amp;gt; economies and constraints on recycling and energy efficiency are stated in terms
#&amp;gt; ...&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;extract-text-chunks&quot;&gt;Extract text chunks&lt;/h2&gt;

&lt;p&gt;We have a few functions to help you pull out certain parts of an article. For example, perhaps you want to get just the authors from your articles, or just the abstracts.&lt;/p&gt;

&lt;p&gt;Here, we’ll search for some PLOS articles, then get their full text, then extract various parts of each article with &lt;code&gt;chunks()&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res &amp;lt;- ft_search(query = &quot;ecology&quot;, from = &quot;plos&quot;)
(x &amp;lt;- ft_get(res))
#&amp;gt; [Docs] 8 
#&amp;gt; [Source] R session  
#&amp;gt; [IDs] 10.1371/journal.pone.0059813 10.1371/journal.pone.0001248
#&amp;gt;      10.1371/journal.pone.0080763 10.1371/journal.pone.0102437
#&amp;gt;      10.1371/journal.pone.0017342 10.1371/journal.pone.0091497
#&amp;gt;      10.1371/journal.pone.0092931 10.1371/journal.pcbi.1003594 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extract DOIs&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;x %&amp;gt;% chunks(&quot;doi&quot;)
#&amp;gt; $plos
#&amp;gt; $plos$`10.1371/journal.pone.0059813`
#&amp;gt; $plos$`10.1371/journal.pone.0059813`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0059813&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0001248`
#&amp;gt; $plos$`10.1371/journal.pone.0001248`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0001248&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0080763`
#&amp;gt; $plos$`10.1371/journal.pone.0080763`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0080763&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0102437`
#&amp;gt; $plos$`10.1371/journal.pone.0102437`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0102437&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0017342`
#&amp;gt; $plos$`10.1371/journal.pone.0017342`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0017342&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0091497`
#&amp;gt; $plos$`10.1371/journal.pone.0091497`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0091497&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0092931`
#&amp;gt; $plos$`10.1371/journal.pone.0092931`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0092931&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pcbi.1003594`
#&amp;gt; $plos$`10.1371/journal.pcbi.1003594`$doi
#&amp;gt; [1] &quot;10.1371/journal.pcbi.1003594&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extract DOIs and categories&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;x %&amp;gt;% chunks(c(&quot;doi&quot;,&quot;categories&quot;))
#&amp;gt; $plos
#&amp;gt; $plos$`10.1371/journal.pone.0059813`
#&amp;gt; $plos$`10.1371/journal.pone.0059813`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0059813&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0059813`$categories
#&amp;gt;  [1] &quot;Research Article&quot;                 &quot;Biology&quot;                         
#&amp;gt;  [3] &quot;Ecology&quot;                          &quot;Community ecology&quot;               
#&amp;gt;  [5] &quot;Species interactions&quot;             &quot;Science policy&quot;                  
#&amp;gt;  [7] &quot;Research assessment&quot;              &quot;Research monitoring&quot;             
#&amp;gt;  [9] &quot;Research funding&quot;                 &quot;Government funding of science&quot;   
#&amp;gt; [11] &quot;Research laboratories&quot;            &quot;Science policy and economics&quot;    
#&amp;gt; [13] &quot;Science and technology workforce&quot; &quot;Careers in research&quot;             
#&amp;gt; [15] &quot;Social and behavioral sciences&quot;   &quot;Sociology&quot;                       
#&amp;gt; [17] &quot;Sociology of knowledge&quot;          
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0001248`
#&amp;gt; $plos$`10.1371/journal.pone.0001248`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0001248&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0001248`$categories
#&amp;gt; [1] &quot;Research Article&quot;             &quot;Ecology&quot;                     
#&amp;gt; [3] &quot;Ecology/Ecosystem Ecology&quot;    &quot;Ecology/Evolutionary Ecology&quot;
#&amp;gt; [5] &quot;Ecology/Theoretical Ecology&quot; 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0080763`
#&amp;gt; $plos$`10.1371/journal.pone.0080763`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0080763&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0080763`$categories
#&amp;gt;  [1] &quot;Research Article&quot;     &quot;Biology&quot;              &quot;Ecology&quot;             
#&amp;gt;  [4] &quot;Autecology&quot;           &quot;Behavioral ecology&quot;   &quot;Community ecology&quot;   
#&amp;gt;  [7] &quot;Evolutionary ecology&quot; &quot;Population ecology&quot;   &quot;Evolutionary biology&quot;
#&amp;gt; [10] &quot;Behavioral ecology&quot;   &quot;Evolutionary ecology&quot; &quot;Population biology&quot;  
#&amp;gt; [13] &quot;Population ecology&quot;  
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0102437`
#&amp;gt; $plos$`10.1371/journal.pone.0102437`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0102437&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0102437`$categories
#&amp;gt;  [1] &quot;Research Article&quot;                  
#&amp;gt;  [2] &quot;Biology and life sciences&quot;         
#&amp;gt;  [3] &quot;Biogeography&quot;                      
#&amp;gt;  [4] &quot;Ecology&quot;                           
#&amp;gt;  [5] &quot;Ecosystems&quot;                        
#&amp;gt;  [6] &quot;Ecosystem engineering&quot;             
#&amp;gt;  [7] &quot;Ecosystem functioning&quot;             
#&amp;gt;  [8] &quot;Industrial ecology&quot;                
#&amp;gt;  [9] &quot;Spatial and landscape ecology&quot;     
#&amp;gt; [10] &quot;Urban ecology&quot;                     
#&amp;gt; [11] &quot;Computer and information sciences&quot; 
#&amp;gt; [12] &quot;Geoinformatics&quot;                    
#&amp;gt; [13] &quot;Spatial analysis&quot;                  
#&amp;gt; [14] &quot;Earth sciences&quot;                    
#&amp;gt; [15] &quot;Geography&quot;                         
#&amp;gt; [16] &quot;Human geography&quot;                   
#&amp;gt; [17] &quot;Cultural geography&quot;                
#&amp;gt; [18] &quot;Social geography&quot;                  
#&amp;gt; [19] &quot;Ecology and environmental sciences&quot;
#&amp;gt; [20] &quot;Conservation science&quot;              
#&amp;gt; [21] &quot;Environmental protection&quot;          
#&amp;gt; [22] &quot;Nature-society interactions&quot;       
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0017342`
#&amp;gt; $plos$`10.1371/journal.pone.0017342`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0017342&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0017342`$categories
#&amp;gt;  [1] &quot;Research Article&quot;     &quot;Biology&quot;              &quot;Ecology&quot;             
#&amp;gt;  [4] &quot;Community ecology&quot;    &quot;Community assembly&quot;   &quot;Community structure&quot; 
#&amp;gt;  [7] &quot;Niche construction&quot;   &quot;Ecological metrics&quot;   &quot;Species diversity&quot;   
#&amp;gt; [10] &quot;Species richness&quot;     &quot;Biodiversity&quot;         &quot;Biogeography&quot;        
#&amp;gt; [13] &quot;Population ecology&quot;   &quot;Mathematics&quot;          &quot;Statistics&quot;          
#&amp;gt; [16] &quot;Biostatistics&quot;        &quot;Statistical theories&quot; &quot;Ecology&quot;             
#&amp;gt; [19] &quot;Mathematics&quot;         
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0091497`
#&amp;gt; $plos$`10.1371/journal.pone.0091497`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0091497&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0091497`$categories
#&amp;gt; [1] &quot;Correction&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0092931`
#&amp;gt; $plos$`10.1371/journal.pone.0092931`$doi
#&amp;gt; [1] &quot;10.1371/journal.pone.0092931&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pone.0092931`$categories
#&amp;gt; [1] &quot;Correction&quot;
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pcbi.1003594`
#&amp;gt; $plos$`10.1371/journal.pcbi.1003594`$doi
#&amp;gt; [1] &quot;10.1371/journal.pcbi.1003594&quot;
#&amp;gt; 
#&amp;gt; $plos$`10.1371/journal.pcbi.1003594`$categories
#&amp;gt; [1] &quot;Research Article&quot;          &quot;Biology and life sciences&quot;
#&amp;gt; [3] &quot;Computational biology&quot;     &quot;Microbiology&quot;             
#&amp;gt; [5] &quot;Theoretical biology&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tabularize&lt;/code&gt; attempts to help you put the data that comes out of &lt;code&gt;chunks()&lt;/code&gt; in to a &lt;code&gt;data.frame&lt;/code&gt;, that we all know and love.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;x %&amp;gt;% chunks(c(&quot;doi&quot;, &quot;history&quot;)) %&amp;gt;% tabularize()
#&amp;gt; $plos
#&amp;gt;                            doi history.received history.accepted
#&amp;gt; 1 10.1371/journal.pone.0059813       2012-09-16       2013-02-19
#&amp;gt; 2 10.1371/journal.pone.0001248       2007-07-02       2007-11-06
#&amp;gt; 3 10.1371/journal.pone.0080763       2013-08-15       2013-10-16
#&amp;gt; 4 10.1371/journal.pone.0102437       2013-11-27       2014-06-19
#&amp;gt; 5 10.1371/journal.pone.0017342       2010-08-24       2011-01-31
#&amp;gt; 6 10.1371/journal.pone.0091497             &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;
#&amp;gt; 7 10.1371/journal.pone.0092931             &amp;lt;NA&amp;gt;             &amp;lt;NA&amp;gt;
#&amp;gt; 8 10.1371/journal.pcbi.1003594       2014-01-09       2014-03-14
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;bring-it-all-together&quot;&gt;Bring it all together&lt;/h2&gt;

&lt;p&gt;With the pieces above, let’s see what it looks like all in one go. Here, we’ll search for articles on &lt;em&gt;climate change&lt;/em&gt;, then visualize word usage in those articles.&lt;/p&gt;

&lt;h3 id=&quot;search&quot;&gt;Search&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(out &amp;lt;- ft_search(query = &#39;climate change&#39;, from = &#39;plos&#39;, limit = 100))
#&amp;gt; Query:
#&amp;gt;   [climate change] 
#&amp;gt; Found:
#&amp;gt;   [PLoS: 11737; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0] 
#&amp;gt; Returned:
#&amp;gt;   [PLoS: 100; BMC: 0; Crossref: 0; Entrez: 0; arxiv: 0; biorxiv: 0]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;get-full-text-1&quot;&gt;Get full text&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(texts &amp;lt;- ft_get(out))
#&amp;gt; [Docs] 99 
#&amp;gt; [Source] R session  
#&amp;gt; [IDs] 10.1371/journal.pone.0054839 10.1371/journal.pone.0045683
#&amp;gt;      10.1371/journal.pone.0050182 10.1371/journal.pone.0118489
#&amp;gt;      10.1371/journal.pone.0053646 10.1371/journal.pone.0015103
#&amp;gt;      10.1371/journal.pone.0008320 10.1371/journal.pmed.1001227
#&amp;gt;      10.1371/journal.pmed.1001374 10.1371/journal.pone.0097480 ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Because PLOS returns XML, we don’t need to do a PDF extraction step. However, if we got full text from arXiv or bioRxiv, we’d need to extract from PDFs first.&lt;/p&gt;

&lt;h3 id=&quot;pull-out-chunks&quot;&gt;Pull out chunks&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;abs &amp;lt;- texts %&amp;gt;% chunks(&quot;abstract&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let’s pull out just the text&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;abs &amp;lt;- lapply(abs$plos, function(z) {
  paste0(z$abstract, collapse = &quot; &quot;)
})
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;analyze&quot;&gt;Analyze&lt;/h3&gt;

&lt;p&gt;Using the &lt;code&gt;tm&lt;/code&gt; package, we can analyze our articles&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;tm&quot;)
corp &amp;lt;- VCorpus(VectorSource(abs))
# remove stop words, strip whitespace, remove punctuation
corp &amp;lt;- tm_map(corp, removeWords, stopwords(&quot;english&quot;))
corp &amp;lt;- tm_map(corp, stripWhitespace)
corp &amp;lt;- tm_map(corp, removePunctuation)
# Make a term document matrix
tdm &amp;lt;- TermDocumentMatrix(corp)
# remove sparse terms
tdm &amp;lt;- removeSparseTerms(tdm, sparse = 0.8)
# get data
rs &amp;lt;- rowSums(as.matrix(tdm))
df &amp;lt;- data.frame(word = names(rs), n = unname(rs), stringsAsFactors = FALSE)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;visualize&quot;&gt;Visualize&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;ggplot2&quot;)
ggplot(df, aes(reorder(word, n), n)) +
  geom_point() +
  coord_flip() +
  labs(y = &quot;Count&quot;, x = &quot;Word&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-08-07-full-text/unnamed-chunk-23-1.png&quot; alt=&quot;plot of chunk unnamed-chunk-23&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-08-07 00:00:00 -0700</published>
				<link>http://recology.info//2015/08/full-text/</link>
			</item>
		
			<item>
				<title>rnoaa - Weather data in R</title>
				<description>&lt;p&gt;NOAA provides a lot of weather data, across many different websites under different project names. The R package &lt;code&gt;rnoaa&lt;/code&gt; accesses many of these, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;NOAA NCDC climate data, using the &lt;a href=&quot;http://www.ncdc.noaa.gov/cdo-web/webservices/v2&quot;&gt;NCDC API version 2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/&quot;&gt;GHCND FTP data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;ftp://ftp.ncdc.noaa.gov/pub/data/noaa/&quot;&gt;ISD FTP data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Severe weather data docs are at &lt;a href=&quot;http://www.ncdc.noaa.gov/swdiws/&quot;&gt;http://www.ncdc.noaa.gov/swdiws/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;ftp://sidads.colorado.edu/DATASETS/NOAA/G02135/shapefiles&quot;&gt;Sea ice data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ndbc.noaa.gov/&quot;&gt;NOAA buoy data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tornadoes! Data from the &lt;a href=&quot;http://www.spc.noaa.gov/gis/svrgis/&quot;&gt;NOAA Storm Prediction Center&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;HOMR - Historical Observing Metadata Repository - from &lt;a href=&quot;http://www.ncdc.noaa.gov/homr/api&quot;&gt;NOAA NCDC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Storm data - from the &lt;a href=&quot;http://www.ncdc.noaa.gov/ibtracs/index.php?name=wmo-data&quot;&gt;International Best Track Archive for Climate Stewardship (IBTrACS)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;rnoaa&lt;/code&gt; used to provide access to &lt;a href=&quot;http://upwell.pfeg.noaa.gov/erddap/index.html&quot;&gt;ERDDAP servers&lt;/a&gt;, but a separate package &lt;a href=&quot;https://github.com/ropensci/rerddap&quot;&gt;rerddap&lt;/a&gt; focuses on just those data sources.&lt;/p&gt;

&lt;p&gt;We focus on getting you the data, so there’s very little in &lt;code&gt;rnoaa&lt;/code&gt; for visualizing, statistics, etc.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;p&gt;The newest version should be on CRAN in the next few days. In the meantime, let’s install from GitHub&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;devtools::install_github(&quot;ropensci/rnoaa&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;rnoaa&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There’s an example using the &lt;code&gt;lawn&lt;/code&gt;, &lt;code&gt;sp&lt;/code&gt;, and &lt;code&gt;dplyr&lt;/code&gt; packages. If you want to try those, install like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;install.packages(c(&quot;lawn&quot;, &quot;dplyr&quot;, &quot;sp&quot;))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;ncdc&quot;&gt;NCDC&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;NCDC = National Climatic Data Center&lt;/li&gt;
  &lt;li&gt;Data comes from a RESTful API described at &lt;a href=&quot;http://www.ncdc.noaa.gov/cdo-web/webservices/v2&quot;&gt;http://www.ncdc.noaa.gov/cdo-web/webservices/v2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This web service requires an API key - get one at &lt;a href=&quot;http://www.ncdc.noaa.gov/cdo-web/token&quot;&gt;http://www.ncdc.noaa.gov/cdo-web/token&lt;/a&gt; if you don’t already have one. NCDC provides access to many different datasets:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th align=&quot;left&quot;&gt;Dataset&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Description&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;Start date&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;End date&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td align=&quot;left&quot;&gt;ANNUAL&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Annual Summaries&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1831-02-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2013-11-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td align=&quot;left&quot;&gt;GHCND&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Daily Summaries&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1763-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2014-03-15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td align=&quot;left&quot;&gt;GHCNDMS&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Monthly Summaries&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1763-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2014-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td align=&quot;left&quot;&gt;NORMAL_ANN&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Normals Annual/Seasonal&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td align=&quot;left&quot;&gt;NORMAL_DLY&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Normals Daily&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-12-31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td align=&quot;left&quot;&gt;NORMAL_HLY&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Normals Hourly&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-12-31&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td align=&quot;left&quot;&gt;NORMAL_MLY&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Normals Monthly&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2010-12-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td align=&quot;left&quot;&gt;PRECIP_15&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Precipitation 15 Minute&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1970-05-12&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2013-03-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td align=&quot;left&quot;&gt;PRECIP_HLY&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Precipitation Hourly&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1900-01-01&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2013-03-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td align=&quot;left&quot;&gt;NEXRAD2&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Nexrad Level II&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1991-06-05&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2014-03-14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td align=&quot;left&quot;&gt;NEXRAD3&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;Nexrad Level III&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;1994-05-20&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;2014-03-11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The main function to get data from NCDC is &lt;code&gt;ncdc()&lt;/code&gt;. &lt;code&gt;datasetid&lt;/code&gt;, &lt;code&gt;startdate&lt;/code&gt;, and &lt;code&gt;enddate&lt;/code&gt; are required parameters. A quick example, here getting data from the GHCND dataset, from a particular station, and from Oct 1st 2013 to Dec 12th 2013:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;ncdc(datasetid = &#39;GHCND&#39;, stationid = &#39;GHCND:USW00014895&#39;, startdate = &#39;2013-10-01&#39;,
   enddate = &#39;2013-12-01&#39;)
#&amp;gt; $meta
#&amp;gt; $meta$totalCount
#&amp;gt; [1] 697
#&amp;gt; 
#&amp;gt; $meta$pageCount
#&amp;gt; [1] 25
#&amp;gt; 
#&amp;gt; $meta$offset
#&amp;gt; [1] 1
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $data
#&amp;gt; Source: local data frame [25 x 8]
#&amp;gt; 
#&amp;gt;                   date datatype           station value fl_m fl_q fl_so
#&amp;gt; 1  2013-10-01T00:00:00     AWND GHCND:USW00014895    29               W
#&amp;gt; 2  2013-10-01T00:00:00     PRCP GHCND:USW00014895     0               W
#&amp;gt; 3  2013-10-01T00:00:00     SNOW GHCND:USW00014895     0               W
#&amp;gt; 4  2013-10-01T00:00:00     SNWD GHCND:USW00014895     0               W
#&amp;gt; 5  2013-10-01T00:00:00     TAVG GHCND:USW00014895   179    H          S
#&amp;gt; 6  2013-10-01T00:00:00     TMAX GHCND:USW00014895   250               W
#&amp;gt; 7  2013-10-01T00:00:00     TMIN GHCND:USW00014895   133               W
#&amp;gt; 8  2013-10-01T00:00:00     WDF2 GHCND:USW00014895   210               W
#&amp;gt; 9  2013-10-01T00:00:00     WDF5 GHCND:USW00014895   230               W
#&amp;gt; 10 2013-10-01T00:00:00     WSF2 GHCND:USW00014895    76               W
#&amp;gt; ..                 ...      ...               ...   ...  ...  ...   ...
#&amp;gt; Variables not shown: fl_t (chr)
#&amp;gt; 
#&amp;gt; attr(,&quot;class&quot;)
#&amp;gt; [1] &quot;ncdc_data&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You probably won’t know what station you want data from off hand though, so you can first search for  stations, in this example using a bounding box that defines a rectangular area near Seattle&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;lawn&quot;)
lawn_bbox_polygon(c(-122.2047, 47.5204, -122.1065, 47.6139)) %&amp;gt;% view
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-07-07-weather-data-with-rnoaa/lawnplot.png&quot; alt=&quot;lawnplot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ll search within that bounding box for weather stations.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;ncdc_stations(extent = c(47.5204, -122.2047, 47.6139, -122.1065))
#&amp;gt; $meta
#&amp;gt; $meta$totalCount
#&amp;gt; [1] 9
#&amp;gt; 
#&amp;gt; $meta$pageCount
#&amp;gt; [1] 25
#&amp;gt; 
#&amp;gt; $meta$offset
#&amp;gt; [1] 1
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $data
#&amp;gt; Source: local data frame [9 x 9]
#&amp;gt; 
#&amp;gt;   elevation    mindate    maxdate latitude                         name
#&amp;gt; 1     199.6 2008-06-01 2015-06-29  47.5503      EASTGATE 1.7 SSW, WA US
#&amp;gt; 2     240.8 2010-05-01 2015-07-05  47.5604       EASTGATE 1.1 SW, WA US
#&amp;gt; 3      85.6 2008-07-01 2015-07-05  47.5916        BELLEVUE 0.8 S, WA US
#&amp;gt; 4     104.2 2008-06-01 2015-07-05  47.5211 NEWPORT HILLS 1.9 SSE, WA US
#&amp;gt; 5      58.5 2008-08-01 2009-04-12  47.6138      BELLEVUE 2.3 ENE, WA US
#&amp;gt; 6     199.9 2008-06-01 2009-11-22  47.5465   NEWPORT HILLS 1.4 E, WA US
#&amp;gt; 7      27.1 2008-07-01 2015-07-05  47.6046        BELLEVUE 1.8 W, WA US
#&amp;gt; 8     159.4 2008-11-01 2015-07-05  47.5694      BELLEVUE 2.3 SSE, WA US
#&amp;gt; 9      82.3 2008-12-01 2010-09-17  47.6095       BELLEVUE 0.6 NE, WA US
#&amp;gt; Variables not shown: datacoverage (dbl), id (chr), elevationUnit (chr),
#&amp;gt;   longitude (dbl)
#&amp;gt; 
#&amp;gt; attr(,&quot;class&quot;)
#&amp;gt; [1] &quot;ncdc_stations&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And there are 9 found. We could then use their station ids (e.g., &lt;code&gt;GHCND:US1WAKG0024&lt;/code&gt;) to search for data using &lt;code&gt;ncdc()&lt;/code&gt;, or search for what kind of data that station has with &lt;code&gt;ncdc_datasets()&lt;/code&gt;, or other functions.&lt;/p&gt;

&lt;h2 id=&quot;ghcnd&quot;&gt;GHCND&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GHCND = Global Historical Climatology Network Daily (Data)&lt;/li&gt;
  &lt;li&gt;Data comes from an FTP server&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;library(&quot;dplyr&quot;)
dat &amp;lt;- ghcnd(stationid = &quot;AGE00147704&quot;)
dat$data %&amp;gt;%
  filter(element == &quot;PRCP&quot;, year == 1909)
#&amp;gt;            id year month element VALUE1 MFLAG1 QFLAG1 SFLAG1 VALUE2 MFLAG2
#&amp;gt; 1 AGE00147704 1909    11    PRCP  -9999     NA                -9999     NA
#&amp;gt; 2 AGE00147704 1909    12    PRCP     23     NA             E      0     NA
#&amp;gt;   QFLAG2 SFLAG2 VALUE3 MFLAG3 QFLAG3 SFLAG3 VALUE4 MFLAG4 QFLAG4 SFLAG4
#&amp;gt; 1                -9999     NA                -9999     NA              
#&amp;gt; 2             E      0     NA             E      0     NA             E
#&amp;gt;   VALUE5 MFLAG5 QFLAG5 SFLAG5 VALUE6 MFLAG6 QFLAG6 SFLAG6 VALUE7 MFLAG7
#&amp;gt; 1  -9999     NA                -9999     NA                -9999     NA
#&amp;gt; 2      0     NA             E      0     NA             E      0     NA
#&amp;gt;   QFLAG7 SFLAG7 VALUE8 MFLAG8 QFLAG8 SFLAG8 VALUE9 MFLAG9 QFLAG9 SFLAG9
#&amp;gt; 1     NA         -9999     NA                -9999     NA              
#&amp;gt; 2     NA      E    250     NA             E     75     NA             E
#&amp;gt;   VALUE10 MFLAG10 QFLAG10 SFLAG10 VALUE11 MFLAG11 QFLAG11 SFLAG11 VALUE12
#&amp;gt; 1   -9999      NA                   -9999      NA                   -9999
#&amp;gt; 2     131      NA               E       0      NA               E       0
#&amp;gt;   MFLAG12 QFLAG12 SFLAG12 VALUE13 MFLAG13 QFLAG13 SFLAG13 VALUE14 MFLAG14
#&amp;gt; 1      NA                   -9999      NA                   -9999      NA
#&amp;gt; 2      NA               E       0      NA               E       0      NA
#&amp;gt;   QFLAG14 SFLAG14 VALUE15 MFLAG15 QFLAG15 SFLAG15 VALUE16 MFLAG16 QFLAG16
#&amp;gt; 1                   -9999      NA                   -9999      NA        
#&amp;gt; 2               E       0      NA               E       0      NA        
#&amp;gt;   SFLAG16 VALUE17 MFLAG17 QFLAG17 SFLAG17 VALUE18 MFLAG18 QFLAG18 SFLAG18
#&amp;gt; 1           -9999      NA                   -9999      NA                
#&amp;gt; 2       E       0      NA               E       0      NA               E
#&amp;gt;   VALUE19 MFLAG19 QFLAG19 SFLAG19 VALUE20 MFLAG20 QFLAG20 SFLAG20 VALUE21
#&amp;gt; 1   -9999      NA      NA           -9999      NA      NA           -9999
#&amp;gt; 2       0      NA      NA       E       0      NA      NA       E       0
#&amp;gt;   MFLAG21 QFLAG21 SFLAG21 VALUE22 MFLAG22 QFLAG22 SFLAG22 VALUE23 MFLAG23
#&amp;gt; 1      NA                   -9999      NA                      22      NA
#&amp;gt; 2      NA               E       0      NA               E       0      NA
#&amp;gt;   QFLAG23 SFLAG23 VALUE24 MFLAG24 QFLAG24 SFLAG24 VALUE25 MFLAG25 QFLAG25
#&amp;gt; 1      NA       E       9      NA      NA       E       5      NA      NA
#&amp;gt; 2      NA       E       0      NA      NA       E       0      NA      NA
#&amp;gt;   SFLAG25 VALUE26 MFLAG26 QFLAG26 SFLAG26 VALUE27 MFLAG27 QFLAG27 SFLAG27
#&amp;gt; 1       E       0      NA               E      86      NA      NA       E
#&amp;gt; 2       E       0      NA               E       0      NA      NA       E
#&amp;gt;   VALUE28 MFLAG28 QFLAG28 SFLAG28 VALUE29 MFLAG29 QFLAG29 SFLAG29 VALUE30
#&amp;gt; 1       0      NA      NA       E      28      NA      NA       E       0
#&amp;gt; 2       0      NA      NA       E       0      NA      NA       E       0
#&amp;gt;   MFLAG30 QFLAG30 SFLAG30 VALUE31 MFLAG31 QFLAG31 SFLAG31
#&amp;gt; 1      NA               E   -9999      NA      NA        
#&amp;gt; 2      NA               E      57      NA      NA       E
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also get to datasets by searching by station id, date min, date max, and variable. E.g.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;ghcnd_search(&quot;AGE00147704&quot;, var = &quot;PRCP&quot;)
#&amp;gt; $prcp
#&amp;gt; Source: local data frame [9,803 x 6]
#&amp;gt; 
#&amp;gt;             id  prcp       date mflag qflag sflag
#&amp;gt; 1  AGE00147704 -9999 1909-11-01    NA            
#&amp;gt; 2  AGE00147704    23 1909-12-01    NA           E
#&amp;gt; 3  AGE00147704    81 1910-01-01    NA           E
#&amp;gt; 4  AGE00147704     0 1910-02-01    NA           E
#&amp;gt; 5  AGE00147704    18 1910-03-01    NA           E
#&amp;gt; 6  AGE00147704     0 1910-04-01    NA           E
#&amp;gt; 7  AGE00147704   223 1910-05-01    NA           E
#&amp;gt; 8  AGE00147704     0 1910-06-01    NA           E
#&amp;gt; 9  AGE00147704     0 1910-07-01    NA           E
#&amp;gt; 10 AGE00147704     0 1910-08-01    NA           E
#&amp;gt; ..         ...   ...        ...   ...   ...   ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;isd&quot;&gt;ISD&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ISD = Integrated Surface Database&lt;/li&gt;
  &lt;li&gt;Data comes from an FTP server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You’ll likely first want to run &lt;code&gt;isd_stations()&lt;/code&gt; to get list of stations&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;stations &amp;lt;- isd_stations()
head(stations)
#&amp;gt;   usaf  wban station_name ctry state icao lat lon elev_m    begin      end
#&amp;gt; 1 7005 99999   CWOS 07005                  NA  NA     NA 20120127 20120127
#&amp;gt; 2 7011 99999   CWOS 07011                  NA  NA     NA 20111025 20121129
#&amp;gt; 3 7018 99999   WXPOD 7018                   0   0   7018 20110309 20130730
#&amp;gt; 4 7025 99999   CWOS 07025                  NA  NA     NA 20120127 20120127
#&amp;gt; 5 7026 99999   WXPOD 7026   AF              0   0   7026 20120713 20141120
#&amp;gt; 6 7034 99999   CWOS 07034                  NA  NA     NA 20121024 20121106
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then get data from particular stations, like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(res &amp;lt;- isd(usaf = &quot;011490&quot;, wban = &quot;99999&quot;, year = 1986))
#&amp;gt; &amp;lt;ISD Data&amp;gt;
#&amp;gt; Size: 1328 X 85
#&amp;gt; 
#&amp;gt;    total_chars usaf_station wban_station     date time date_flag latitude
#&amp;gt; 1           50        11490        99999 19860101    0         4    66267
#&amp;gt; 2          123        11490        99999 19860101  600         4    66267
#&amp;gt; 3           50        11490        99999 19860101 1200         4    66267
#&amp;gt; 4           94        11490        99999 19860101 1800         4    66267
#&amp;gt; 5           50        11490        99999 19860102    0         4    66267
#&amp;gt; 6          123        11490        99999 19860102  600         4    66267
#&amp;gt; 7           50        11490        99999 19860102 1200         4    66267
#&amp;gt; 8           94        11490        99999 19860102 1800         4    66267
#&amp;gt; 9           50        11490        99999 19860103    0         4    66267
#&amp;gt; 10         123        11490        99999 19860103  600         4    66267
#&amp;gt; ..         ...          ...          ...      ...  ...       ...      ...
#&amp;gt; Variables not shown: longitude (int), type_code (chr), elevation (int),
#&amp;gt;      call_letter (int), quality (chr), wind_direction (int),
#&amp;gt;      wind_direction_quality (int), wind_code (chr), wind_speed (int),
#&amp;gt;      wind_speed_quality (int), ceiling_height (int),
#&amp;gt;      ceiling_height_quality (int), ceiling_height_determination (chr),
#&amp;gt;      ceiling_height_cavok (chr), visibility_distance (int),
#&amp;gt;      visibility_distance_quality (int), visibility_code (chr),
#&amp;gt;      visibility_code_quality (int), temperature (int), temperature_quality
#&amp;gt;      (int), temperature_dewpoint (int), temperature_dewpoint_quality
#&amp;gt;      (int), air_pressure (int), air_pressure_quality (int),
#&amp;gt;      AG1.precipitation (chr), AG1.discrepancy (int), AG1.est_water_depth
#&amp;gt;      (int), GF1.sky_condition (chr), GF1.coverage (int),
#&amp;gt;      GF1.opaque_coverage (int), GF1.coverage_quality (int),
#&amp;gt;      GF1.lowest_cover (int), GF1.lowest_cover_quality (int),
#&amp;gt;      GF1.low_cloud_genus (int), GF1.low_cloud_genus_quality (int),
#&amp;gt;      GF1.lowest_cloud_base_height (int),
#&amp;gt;      GF1.lowest_cloud_base_height_quality (int), GF1.mid_cloud_genus
#&amp;gt;      (int), GF1.mid_cloud_genus_quality (int), GF1.high_cloud_genus (int),
#&amp;gt;      GF1.high_cloud_genus_quality (int), MD1.atmospheric_change (chr),
#&amp;gt;      MD1.tendency (int), MD1.tendency_quality (int), MD1.three_hr (int),
#&amp;gt;      MD1.three_hr_quality (int), MD1.twentyfour_hr (int),
#&amp;gt;      MD1.twentyfour_hr_quality (int), REM.remarks (chr), REM.identifier
#&amp;gt;      (chr), REM.length_quantity (int), REM.comment (chr), KA1.extreme_temp
#&amp;gt;      (chr), KA1.period_quantity (int), KA1.max_min (chr), KA1.temp (int),
#&amp;gt;      KA1.temp_quality (int), AY1.manual_occurrence (chr),
#&amp;gt;      AY1.condition_code (int), AY1.condition_quality (int), AY1.period
#&amp;gt;      (int), AY1.period_quality (int), AY2.manual_occurrence (chr),
#&amp;gt;      AY2.condition_code (int), AY2.condition_quality (int), AY2.period
#&amp;gt;      (int), AY2.period_quality (int), MW1.first_weather_reported (chr),
#&amp;gt;      MW1.condition (int), MW1.condition_quality (int),
#&amp;gt;      EQD.observation_identifier (chr), EQD.observation_text (int),
#&amp;gt;      EQD.reason_code (int), EQD.parameter (chr),
#&amp;gt;      EQD.observation_identifier.1 (chr), EQD.observation_text.1 (int),
#&amp;gt;      EQD.reason_code.1 (int), EQD.parameter.1 (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;severe-weather&quot;&gt;Severe weather&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;SWDI = Severe Weather Data Inventory&lt;/li&gt;
  &lt;li&gt;From the SWDI site&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;The Severe Weather Data Inventory (SWDI) is an integrated database of severe weather records for the United States. The records in SWDI come from a variety of sources in the NCDC archive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;code&gt;swdi()&lt;/code&gt; function allows you to get data in xml, csv, shp, or kmz format. You can get data from many different datasets:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;nx3tvs NEXRAD Level-3 Tornado Vortex Signatures (point)&lt;/li&gt;
  &lt;li&gt;nx3meso NEXRAD Level-3 Mesocyclone Signatures (point)&lt;/li&gt;
  &lt;li&gt;nx3hail NEXRAD Level-3 Hail Signatures (point)&lt;/li&gt;
  &lt;li&gt;nx3structure NEXRAD Level-3 Storm Cell Structure Information (point)&lt;/li&gt;
  &lt;li&gt;plsr Preliminary Local Storm Reports (point)&lt;/li&gt;
  &lt;li&gt;warn Severe Thunderstorm, Tornado, Flash Flood and Special Marine warnings (polygon)&lt;/li&gt;
  &lt;li&gt;nldn Lightning strikes from Vaisala (.gov and .mil ONLY) (point)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An example: Get all &lt;code&gt;plsr&lt;/code&gt; within the bounding box (-91,30,-90,31)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;swdi(dataset = &#39;plsr&#39;, startdate = &#39;20060505&#39;, enddate = &#39;20060510&#39;,
bbox = c(-91, 30, -90, 31))
#&amp;gt; $meta
#&amp;gt; $meta$totalCount
#&amp;gt; numeric(0)
#&amp;gt; 
#&amp;gt; $meta$totalTimeInSeconds
#&amp;gt; [1] 0
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $data
#&amp;gt; Source: local data frame [5 x 8]
#&amp;gt; 
#&amp;gt;                  ztime     id        event magnitude            city
#&amp;gt; 1 2006-05-09T02:20:00Z 427540         HAIL         1    5 E KENTWOOD
#&amp;gt; 2 2006-05-09T02:40:00Z 427536         HAIL         1    MOUNT HERMAN
#&amp;gt; 3 2006-05-09T02:40:00Z 427537 TSTM WND DMG     -9999    MOUNT HERMAN
#&amp;gt; 4 2006-05-09T03:00:00Z 427199         HAIL         0     FRANKLINTON
#&amp;gt; 5 2006-05-09T03:17:00Z 427200      TORNADO     -9999 5 S FRANKLINTON
#&amp;gt; Variables not shown: county (chr), state (chr), source (chr)
#&amp;gt; 
#&amp;gt; $shape
#&amp;gt;                  shape
#&amp;gt; 1 POINT (-90.43 30.93)
#&amp;gt; 2  POINT (-90.3 30.96)
#&amp;gt; 3  POINT (-90.3 30.96)
#&amp;gt; 4 POINT (-90.14 30.85)
#&amp;gt; 5 POINT (-90.14 30.78)
#&amp;gt; 
#&amp;gt; attr(,&quot;class&quot;)
#&amp;gt; [1] &quot;swdi&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;sea-ice&quot;&gt;Sea ice&lt;/h2&gt;

&lt;p&gt;The &lt;code&gt;seaice()&lt;/code&gt; function simply grabs shape files that describe sea ice cover at the Northa and South poles, and can be useful for examining change through time in sea ice cover, among other things.&lt;/p&gt;

&lt;p&gt;An example: Plot sea ice cover for April 1990 for the North pole.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;urls &amp;lt;- seaiceeurls(mo = &#39;Apr&#39;, pole = &#39;N&#39;, yr = 1990)
out &amp;lt;- seaice(urls)

library(&#39;ggplot2&#39;)
ggplot(out, aes(long, lat, group = group)) +
   geom_polygon(fill = &quot;steelblue&quot;) +
   theme_ice()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-07-07-weather-data-with-rnoaa/unnamed-chunk-13-1.png&quot; alt=&quot;plot of chunk unnamed-chunk-13&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;buoys&quot;&gt;Buoys&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Get NOAA buoy data from the National Buoy Data Center&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using buoy data requires the &lt;code&gt;ncdf&lt;/code&gt; package. Make sure you have that installed, like &lt;code&gt;install.packages(&quot;ncdf&quot;)&lt;/code&gt;. &lt;code&gt;buoy()&lt;/code&gt; and &lt;code&gt;buoys()&lt;/code&gt; will fail if you don’t have &lt;code&gt;ncdf&lt;/code&gt; installed.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;buoys()&lt;/code&gt; - Get available buoys given a dataset name&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;head(buoys(dataset = &#39;cwind&#39;))
#&amp;gt;      id
#&amp;gt; 1 41001
#&amp;gt; 2 41002
#&amp;gt; 3 41004
#&amp;gt; 4 41006
#&amp;gt; 5 41008
#&amp;gt; 6 41009
#&amp;gt;                                                                       url
#&amp;gt; 1 http://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41001/catalog.html
#&amp;gt; 2 http://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41002/catalog.html
#&amp;gt; 3 http://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41004/catalog.html
#&amp;gt; 4 http://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41006/catalog.html
#&amp;gt; 5 http://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41008/catalog.html
#&amp;gt; 6 http://dods.ndbc.noaa.gov/thredds/catalog/data/cwind/41009/catalog.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;buoy()&lt;/code&gt; - Get data for a buoy - if no year or datatype specified, we get the first file&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;buoy(dataset = &#39;cwind&#39;, buoyid = 46085)
#&amp;gt; Dimensions (rows/cols): [33486 X 5] 
#&amp;gt; 2 variables: [wind_dir, wind_spd] 
#&amp;gt; 
#&amp;gt;                    time latitude longitude wind_dir wind_spd
#&amp;gt; 1  2007-05-05T02:00:00Z   55.855  -142.559      331      2.8
#&amp;gt; 2  2007-05-05T02:10:00Z   55.855  -142.559      328      2.6
#&amp;gt; 3  2007-05-05T02:20:00Z   55.855  -142.559      329      2.2
#&amp;gt; 4  2007-05-05T02:30:00Z   55.855  -142.559      356      2.1
#&amp;gt; 5  2007-05-05T02:40:00Z   55.855  -142.559      360      1.5
#&amp;gt; 6  2007-05-05T02:50:00Z   55.855  -142.559       10      1.9
#&amp;gt; 7  2007-05-05T03:00:00Z   55.855  -142.559       10      2.2
#&amp;gt; 8  2007-05-05T03:10:00Z   55.855  -142.559       14      2.2
#&amp;gt; 9  2007-05-05T03:20:00Z   55.855  -142.559       16      2.1
#&amp;gt; 10 2007-05-05T03:30:00Z   55.855  -142.559       22      1.6
#&amp;gt; ..                  ...      ...       ...      ...      ...
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;tornadoes&quot;&gt;Tornadoes&lt;/h2&gt;

&lt;p&gt;The function &lt;code&gt;tornadoes()&lt;/code&gt; gets tornado data from &lt;a href=&quot;http://www.spc.noaa.gov/gis/svrgis/&quot;&gt;http://www.spc.noaa.gov/gis/svrgis/&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;shp &amp;lt;- tornadoes()
library(&#39;sp&#39;)
plot(shp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-07-07-weather-data-with-rnoaa/tornadoes.png&quot; alt=&quot;tornadoes&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;historical-observing-metadata-repository&quot;&gt;Historical Observing Metadata Repository&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;HOMR = Historical Observing Metadata Repository&lt;/li&gt;
  &lt;li&gt;Data from RESTful API at &lt;a href=&quot;http://www.ncdc.noaa.gov/homr/api&quot;&gt;http://www.ncdc.noaa.gov/homr/api&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;homr_definitions()&lt;/code&gt; gets you definitions and metadata for datasets&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;head(homr_definitions())
#&amp;gt; Source: local data frame [6 x 7]
#&amp;gt; 
#&amp;gt;   defType  abbr                fullName    displayName
#&amp;gt; 1     ids GHCND        GHCND IDENTIFIER       GHCND ID
#&amp;gt; 2     ids  COOP             COOP NUMBER        COOP ID
#&amp;gt; 3     ids  WBAN             WBAN NUMBER        WBAN ID
#&amp;gt; 4     ids   FAA FAA LOCATION IDENTIFIER         FAA ID
#&amp;gt; 5     ids  ICAO                 ICAO ID        ICAO ID
#&amp;gt; 6     ids TRANS          TRANSMITTAL ID Transmittal ID
#&amp;gt; Variables not shown: description (chr), cssaName (chr), ghcndName (chr)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;homr()&lt;/code&gt; gets you metadata for stations given query parameters. In this example, search for data for the state of Delaware&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;res &amp;lt;- homr(state = &#39;DE&#39;)
names(res) # the stations
#&amp;gt;  [1] &quot;10001871&quot; &quot;10100162&quot; &quot;10100164&quot; &quot;10100166&quot; &quot;20004155&quot; &quot;20004158&quot;
#&amp;gt;  [7] &quot;20004160&quot; &quot;20004162&quot; &quot;20004163&quot; &quot;20004168&quot; &quot;20004171&quot; &quot;20004176&quot;
#&amp;gt; [13] &quot;20004178&quot; &quot;20004179&quot; &quot;20004180&quot; &quot;20004182&quot; &quot;20004184&quot; &quot;20004185&quot;
#&amp;gt; [19] &quot;30001831&quot; &quot;30017384&quot; &quot;30020917&quot; &quot;30021161&quot; &quot;30021998&quot; &quot;30022674&quot;
#&amp;gt; [25] &quot;30026770&quot; &quot;30027455&quot; &quot;30032423&quot; &quot;30032685&quot; &quot;30034222&quot; &quot;30039554&quot;
#&amp;gt; [31] &quot;30043742&quot; &quot;30046662&quot; &quot;30046814&quot; &quot;30051475&quot; &quot;30057217&quot; &quot;30063570&quot;
#&amp;gt; [37] &quot;30064900&quot; &quot;30065901&quot; &quot;30067636&quot; &quot;30069663&quot; &quot;30075067&quot; &quot;30077378&quot;
#&amp;gt; [43] &quot;30077857&quot; &quot;30077923&quot; &quot;30077988&quot; &quot;30079088&quot; &quot;30079240&quot; &quot;30082430&quot;
#&amp;gt; [49] &quot;30084216&quot; &quot;30084262&quot; &quot;30084537&quot; &quot;30084796&quot; &quot;30094582&quot; &quot;30094639&quot;
#&amp;gt; [55] &quot;30094664&quot; &quot;30094670&quot; &quot;30094683&quot; &quot;30094730&quot; &quot;30094806&quot; &quot;30094830&quot;
#&amp;gt; [61] &quot;30094917&quot; &quot;30094931&quot; &quot;30094936&quot; &quot;30094991&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can index to each one to get more data&lt;/p&gt;

&lt;h2 id=&quot;storms&quot;&gt;Storms&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Data from: International Best Track Archive for Climate Stewardship (IBTrACS)&lt;/li&gt;
  &lt;li&gt;Data comes from an FTP server&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Flat files (csv’s) are served up as well as shp files. In this example, plot storm data for the year 1940&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-r&quot;&gt;(res3 &amp;lt;- storm_shp(year = 1940))
#&amp;gt; &amp;lt;NOAA Storm Shp Files&amp;gt;
#&amp;gt; Path: ~/.rnoaa/storms/year/Year.1940.ibtracs_all_points.v03r06.shp
#&amp;gt; Basin: &amp;lt;NA&amp;gt;
#&amp;gt; Storm: &amp;lt;NA&amp;gt;
#&amp;gt; Year: 1940
#&amp;gt; Type: points
res3shp &amp;lt;- storm_shp_read(res3)
sp::plot(res3shp)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;/public/img/2015-07-07-weather-data-with-rnoaa/unnamed-chunk-19-1.png&quot; alt=&quot;plot of chunk unnamed-chunk-19&quot; /&gt;&lt;/p&gt;
</description>
				<published>2015-07-07 00:00:00 -0700</published>
				<link>http://recology.info//2015/07/weather-data-with-rnoaa/</link>
			</item>
		
	</channel>
</rss>
