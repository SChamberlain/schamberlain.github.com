<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

  <head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <title>
    
    Recology, R/etc.
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Open+Sans:300,400italic,400,600,700|Abril+Fatface">

  <link rel="stylesheet" href="/public/css/bootstrap/css/bootstrap.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/favicon.ico">
  <link rel="shortcut icon" href="/public/favicon.ico">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.css" rel="stylesheet">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0f layout-reverse">

    <header class="masthead">
      <div class="masthead-inner">
        <h1>Recology</h1>
        <!-- <h1> <a href="http://recology.info/">Recology</a></h1> -->
        <p class="lead">R/etc.</p>

        <div class="colophon">
          <ul class="colophon-links">
            <li>
              <a href="/"><i class="fa fa-home fa-lg"></i></a>&nbsp;
              <a href="/about"><i class="fa fa-info-circle fa-lg"></i></a>&nbsp;
              <a href="/archives"><i class="fa fa-archive fa-lg"></i></a>&nbsp;
              <a href="/rresources"><i class="fa fa-book fa-lg"></i></a>&nbsp;
              <a href="http://rforcats.net/" rel><i class="fa fa-graduation-cap fa-lg"></i></a>&nbsp;
              <a href="/feed.xml"><i class="fa fa-rss fa-lg"></i></a>&nbsp;
              <a href="https://twitter.com/sckottie"><i class="fa fa-twitter fa-lg"></i></a>&nbsp;
              <a href="/fork"><i class="fa fa-spinner fa-lg"></i></a>
            </li>
          </ul>
          <!-- <small><a href="https://github.com/mdo/hyde">Hyde</a> from <a href="https://twitter.com/mdo" target="_blank">@mdo</a>.</small> -->
        </div>
      </div>
    </header>

    <div class="content container">
      <div class="posts">
  <a style="float:right;" href="/archives" data-toggle="tooltip" data-placement="bottom" title="Archives"><i class="fa fa-archive fa-lg"></i></a>
  <a style="float:right;" href="/tags"><i class="fa fa-tags fa-lg"></i></a>&nbsp;
  
  <div class="post">
    <h1>
      <a href="/2012/01/rnetlogo/">
        RNetLogo - A package for running NetLogo from R
      </a>
    </h1>

    <span class="post-date">23 Jan 2012</span>

    Described in a new Methods in Ecology and Evolution paper [here][], a new [R][] package [RNetLogo][] allows you to use [NetLogo][] from R. 

NetLogo is software is a "multi-agent programmable modeling environment". NetLogo can be used in individual- and agent-based modeling, and is used in the book [_Agent-based and Individual-based Modeling: A Practical Introduction_][book] by Railsback & Grimm. 

I have not tried the package yet, but looks interesting. I am always a fan of running stand-alone programs from R if possible. 

[RNetLogo]: http://cran.r-project.org/web/packages/RNetLogo/index.html
[NetLogo]: http://ccl.northwestern.edu/netlogo/
[here]: http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2011.00180.x/abstract
[R]: http://cran.r-project.org/ 
[book]: http://www.railsback-grimm-abm-book.com/ 

  </div>
  
  <div class="post">
    <h1>
      <a href="/2012/01/reviewing-peer-review-process/">
        Taking a Closer Look at Peer Review
      </a>
    </h1>

    <span class="post-date">16 Jan 2012</span>

    This post is only tangentially about open science.  It is more directly about the process of peer review and how it might be improved.  I am working on a follow-up post about how these points can be addressed in an open publishing environment.

A [recent paper on the arXiv][arXivpaper] got me thinking about the sticking points in the publishing pipeline.  As it stands, most scientists have a pretty good understanding of how peer reviewed publishing is supposed to work.  Once an author—or more likely, a group of authors—decides that a manuscript is ready for action, the following series of events will occur:

1. the authors submit the manuscript to the journal of choice;
2. the journal's editor makes a general decision about whether the article is appropriate for the journal;
3. in the affirmative case, the editor selects referees for the manuscript and sends them the text for review;
4. the referees return reviews of the manuscript (the referees are not typically identified to the authors);
5. the editor makes the decision to reject the manuscript, accept it with minor revisions, or accept it with major revisions.  Rejected manuscripts usually start over the process in another journal.  Minor revisions to accepted manuscripts are usually made quickly and publication proceeds.  In the case of major revisions, the suggested changes are made, if possible, and the manuscript is returned to the editor.  At this point, the referees may get a second crack at the material (but not necessarily), before the editor makes a final accept/reject decision based on the feedback from the referees.

Peer review of manuscripts exists for several reasons.  For one, self-regulation determines the suitability of the material for publication if it was not already obvious to the editor of the journal.  Having peer reviewers also improves the material and its presentation.  Furthermore, having expert reviewers lends credibility to the work and insures that misleading, wrong, or crackpot material does not receive the stamp of credibility.  Finally, finding appropriately skilled referees spreads the workload beyond the editors, who may not have the resources to evaluate every paper arriving at their desk.

Though peer review has a storied history, it also has its drawbacks.  First, and perhaps foremost, the process is often a slow one, with many months elapsing during even one round of communications between the authors, the editor, and the referees.  Peer review is not always an objective process either: referees have the power to delay, or outright reject, work that their competitors have completed, and thus they may lose their impartiality in the process.  Additionally, the publishing process does not reveal the feedback process that occurs between authors and referees, which can be a scientifically and pedagogically valuable exchange.

[One proposal][arXivpaper] to address the shortcomings of the peer review process (alluded to in the first paragraph) was posted by Sergey Bozhevolnyi on the [arXiv][arXiv], a pre-publication website for many physics-related manuscripts.  Bozhevolnyi calls his model of publishing Rapid, Impartial, and Comprehensive (RIC) publishing.  To him, "rapid" means that editors should approve or reject manuscripts before the manuscripts are sent to the referees for review.  Then, "impartial" means that referees, who might otherwise have an interest in rejecting a perfectly fine paper, lose the power to dictate whether or not a manuscript is published.  Instead, the referees critique the paper without assessing whether it is publication-worthy.  Lastly, "comprehensive" involves publishing everything having to do with the manuscript.  That is, all positive and negative reviews are published in conjunction with the all versions of a manuscript.

The primary benefit of RIC, according to Bozhevolnyi, is that it saves the energies of authors, editors, and referees, thus allowing them all to do more research and less wrangling.  Since most papers are ultimately accepted somewhere, then we should not cause additional delays in publishing by first rejecting them in multiple places.  Instead, collate the manuscript and the reviews and publish them all together, along with any revisions to the manuscript.  Having the reviews be publicly viewable will encourage referees to be more careful about writing their critiques and supporting their assertions, and the process as a whole will be more transparent than it currently is.

Before I critique the RIC publishing proposal, I should point out that some aspects of the proposal are very appealing.  I particularly like the idea of publishing all reviews in addition to the manuscript.  That said, I find it difficult to believe that the incentives for authors and referees change for the better under this proposal.  For example, what happens if authors receive feedback, do not wish to invest the time to address the critique, and subsequently allow the original manuscript and the reviews to stand as they are?  This situation seems like a moral hazard for authors that does a disservice to the quality of scientific literature.  On the part of the referees, does removing decision-making authority make reviewing less appealing?  Disempowering the referees by potentially ignoring their critique and only counting it as a minor part of the publishing process will not motivate them to write better reviews.  In the case of editors, what makes us believe that an editor, or an editorial board, has the background to properly evaluate the suitability of work for acceptance into a journal?  The reason we have referees in the current peer review system is because they have the very expertise and familiarity needed for this task.

Does the fact that Bozhevolnyi's RIC proposal does not make sense mean that peer review is fine as it is?  I do not think so.  Instead, it is worth asking what parts of peer review we like and what parts we would like to improve.  I posit that rejection, or the threat of rejection, is the greatest motivator for authors to make necessary changes to their manuscript.  As such, rejection by peers is still the best way to require and receive revisions.  Though I think that referees should retain their rejecting power (and their anonymity!), I feel strongly that the entire peer review process would benefit from the increased transparency and accountability that publishing unsigned reviews would add.  As far as editors, they play a role in shaping the kind of journal they run by selecting appropriate material on a general level, but they should not play too large a role in determining the "important" research in any field.  The model used by the journal [Public Library of Science One][] is a promising one in this regard, with the only acceptance criterion being whether the science is sound.

The amount of time that it takes to publish is one of the most frustrating aspects of peer review, however.  Journals could voluntarily publish time-to-publication figures, a number which could then be used by authors—along with impact factors and acceptance rates—to decide which journals to submit to.  For instance, an editor of the Journal of Orthodontics writes about just this fact in [an editorial][orthodonticseditorial].  A Google search for "journal time to publication" reveals that people have been thinking about this problem for a while (e.g. [computer science comparisons][compscicomps]), but no general standard exists across journals.  In fact, I suspect these are numbers most journals are afraid will hurt them more than help them.  Nevertheless, journals acknowledge the demand for rapid publication when they offer services like [Springer's Fast Track publishing[fasttrackpub] or [Physical Review's Rapid Communications][rapidcomm].

Ultimately, it may not matter what journals do because authors are routing around this problem via pre-publication archives such as the [arXiv][arXiv] for physics-related subject matter.  Though not without complications, especially in the health sciences (see, for example, ["The Promise and Perils of Pre-Publication Review"][prepubperils]), pre-publication allows authors to communicate results and establish priority without stressing about getting through the peer review process as fast as possible.  Instead, the process takes its normal, slower course while authors move along their on-going research.

I will conclude by leaving an open question that I may address in a future post:  how do you encourage peer reviewers to do the best possible job, in a timely manner, without only relying on their altruism to doing good science and being good members of the community?  It is this question about peer review, I feel, that is the most fraught with complication and subject to the law of unintended consequences if the incentives are changed.

[arXivpaper]: http://arxiv.org/abs/1110.0791
[arXiv]: http://arxiv.org/
[PLOSone]: http://www.plosone.org/static/reviewerGuidelines.action#about
[orthodonticseditorial]: http://jorthod.maneyjournals.org/content/29/3/171.full
[compscicomps]: http://www.hutter1.net/journals.htm
[fasttrackpub]: http://www.springer.com/societies+%26+publishing+partners/society+%26+partner+zone?SGWID=0-173202-12-772912-0
[rapidcomm]: http://pra.aps.org/highlighting-rapids
[prepubperils]: http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0010782

  </div>
  
  <div class="post">
    <h1>
      <a href="/2012/01/phylogeny-resolution/">
        Function for phylogeny resolution
      </a>
    </h1>

    <span class="post-date">13 Jan 2012</span>

    UPDATE:  Yeah, so the treeresstats function had a problem in one of the calculations.  I fixed that and added some more calulcations to the function. 

I couldn't find any functions to calculate number of polytomies, and related metrics. 

Here's a simple function that gives four metrics on a phylo tree object:

<script src="https://gist.github.com/1607531.js?file=treeresstats.R"></script>

Here's output from the gist above:

{% highlight r %}
$trsize_tips
[1] 15

$trsize_nodes
[1] 13

$numpolys
[1] 1

$numpolysbytrsize_tips
[1] 0.06666667

$numpolysbytrsize_nodes
[1] 0.07692308

$proptipsdescpoly
[1] 0.2

$propnodesdich
[1] 0.9230769
{% endhighlight %}

And an example with many trees:

<table border="1">
	<tr>
		<th>trsize_tips</th>
		<th>trsize_nodes</th>
		<th>numpolys</th>
		<th>numpolysbytrsize_tips</th>
		<th>numpolysbytrsize_nodes</th>
		<th>proptipsdescpoly</th>
		<th>propnodesdich</th>
	</tr>
	<tr>
		<td>20</td> <td>13</td> <td>4</td> <td>0.20</td> <td>0.31</td> <td>0.7</td> <td>0.69</td>
	</tr>
	<tr>
		<td>20</td> <td>7</td> <td>3</td> <td>0.15</td> <td>0.43</td> <td>0.9</td> <td>0.57</td>
	</tr>
	<tr>
		<td>20</td> <td>11</td> <td>6</td> <td>0.30</td> <td>0.55</td> <td>1.0</td> <td>0.45</td>
	</tr>
	<tr>
		<td>20</td> <td>13</td> <td>4</td> <td>0.20</td> <td>0.31</td> <td>0.7</td> <td>0.69</td>
	</tr>
	<tr>
		<td>20</td> <td>9</td> <td>5</td> <td>0.25</td> <td>0.56</td> <td>1.0</td> <td>0.44</td>
	</tr>
</table>

  </div>
  
</div>

<!-- Pagination links -->
<div class="pagination">
  
    <a href="/page34" class="older">Older</a>
  
  
    
      <a href="/page32" class="newer">Newer</a>
    
  
</div>

    </div>

    <!-- for bootstrap tooltips -->
    <script type="text/javascript">
      $("[data-toggle=\"tooltip\"]").tooltip();
    </script>

  </body>

  <footer>
  <!-- Disqus code -->
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'recology'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function () {
          var s = document.createElement('script'); s.async = true;
          s.type = 'text/javascript';
          s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
          (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
  </script>

  <!-- google analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-63197374-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

</html>
