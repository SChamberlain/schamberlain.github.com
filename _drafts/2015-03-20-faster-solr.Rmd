---
name: faster-solr
layout: post
title: Faster solr with csv
date: 2015-03-20
author: Scott Chamberlain
sourceslug: _drafts/2015-03-20-faster-solr.Rmd
tags:
- R
- solr
- database
---

```{r echo=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE, 
  message = FALSE
)
```

With the [help of user input](https://github.com/ropensci/solr/issues/47), I've tweaked `solr` just a bit to make things faster using default setings. I imagine the main interface for people using the `solr` R client is via `solr_search()`, which used to have `wt=json` by default. Changing this to `wt=csv` gives better performance. And it sorta makes sense to use csv, as the point of using an R client is probably do get data eventually into a data.frame, so it makes sense to go csv format (Already in tabular format) if it's faster too.

## Install

Install and load `solr`

```{r eval=FALSE}
devtools::install_github("ropensci/solr")
```

```{r}
library("solr")
library("microbenchmark")
```

## Setup

Define base url and fields to return

```{r}
url <- 'http://api.plos.org/search'
fields <- c('id','cross_published_journal_name','cross_published_journal_key',
            'cross_published_journal_eissn','pmid','pmcid','publisher','journal',
            'publication_date','article_type','article_type_facet','author',
            'author_facet','volume','issue','elocation_id','author_display',
            'competing_interest','copyright')
```

## json

The previous default for `solr_search()` used `json`

```{r}
solr_search(q='*:*', rows=10, fl=fields, base=url, wt = "json")
```

## csv

The default `wt` setting is now `csv`

```{r}
solr_search(q='*:*', rows=10, fl=fields, base=url, wt = "json")
```

## Compare times

When parsing to a data.frame (which `solr_search()` does by default), csv is quite a bit faster.

```{r cache=TRUE}
microbenchmark(
  json = solr_search(q='*:*', rows=500, fl=fields, base=url, wt = "json", verbose = FALSE),
  csv = solr_search(q='*:*', rows=500, fl=fields, base=url, wt = "csv", verbose = FALSE), 
  times = 20
)
```

## json vs xml vs csv

When getting raw data, csv is best, json next, then xml pulling up the rear.

```{r cache=TRUE}
microbenchmark(
  json = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "json", verbose = FALSE, raw = TRUE),
  csv = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "csv", verbose = FALSE, raw = TRUE),
  xml = solr_search(q='*:*', rows=1000, fl=fields, base=url, wt = "xml", verbose = FALSE, raw = TRUE),
  times = 10
)
```

## Notes

Note that `wt=csv` is only available in `solr_search()` and `solr_all()` because csv writer 
only returns the docs element in csv, dropping other elements, including facets, mlt, groups, 
stats, etc. 

Also, note the http client used in `solr` is `httr`, which passes in a gzip compression header by default, so as long as the server serving up the Solr data has compression turned on, that's all set.

Another way I've sped things up is if you use `wt=json` then parse to a data.frame, it uses `dplyr` which sped things up considerably.
