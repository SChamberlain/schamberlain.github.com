---
name: apis-text-mining-logs
layout: post
title: "text mining, apis, and parsing api logs"
date: 2019-03-21
author: Scott Chamberlain
sourceslug: _drafts/2019-03-21-apis-text-mining-logs.Rmd
tags:
- R
- API
- text-mining
- DOI
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Acquiring full text articles

[fulltext][] is an R package I maintain to obtain full text versions of research articles
for text mining.

It's a hard problem, with a sphagetti web of code. One of the hard problems is 
figuring out what the URL is for the full text version of an article. Publishers
do not have consistent URL patterns through time, and so you can not set rules once 
and never revisit them. 

The [Crossref API](https://github.com/CrossRef/rest-api-doc) has links available to 
full text versions for publishers that choose to share them. However, even if 
publishers choose to share their full text links, they may be out of date or completely
wrong (not actually lead to the full text). 

There's a variety of other APIs out there for getting links to articles, but none 
really hit the spot, which lead to the creation of the [ftdoi API][ftdoi]. 

## the ftdoi API

The [ftdoi API][ftdoi] is a web API, with it's main goal for getting a best guess at
the URL for the full text version of an article from it's DOI (this is done via the 
`/api/doi/{doi}/` route). The API gives back URLs for all those possible, including
pdf, xml, and html. Most publishers only give full text as PDF, but when XML is
available we give those URLs as well.

The API uses the rules maintained in the [pubpatterns](https://github.com/ropenscilabs/pubpatterns/tree/master/src)
repo. The rules are only rough guidelines though and often require at least one 
step of making a web request to the publishers site or another site, that's NOT 
specified in the pubpatterns rules. For example, the [Biorxiv file](https://github.com/ropenscilabs/pubpatterns/blob/master/src/biorxiv.json)
has notes about how to get the parts necessary for the full URL, but the actual logic 
to do so in in the API code base [here](https://github.com/ropenscilabs/pubpatternsapi/blob/master/utils.rb#L590-L601). 

The ftdoi API caches responses for requests for 24 hrs, so even if a request takes 5 seconds
or so to process, at least for the next 24 hrs it will be nearly instantaneous. We don't 
want to cache indefinitely because URLs may be changed at any time by the publishers.

The `fulltext` package uses the ftdoi API internally, mostly hidden from users, to 
get a full text URL.

## But why an API?

Why not just have a set of rules in the `fulltext` R package, and go from there?
An API was relatively easy for me to stand up, and i think it has many benefits:
can be used by anything/anyone, not just R users; updates to publisher specific
rules for generating URLs can evolve independently of `fulltext`; the logs
can be used as a tool to improve the API.

## what do people actually want?

The ftdoi API has been running for a while now, maybe a year or so, and I've been 
collecting logs. Seems smart to look at the logs to determine what publishers 
users are requesting articles from that the ftdoi API does not currently support,
so that the API can hopefully add those publishers. For obvious reasons, I can't share
the log data.

Load packages and define file path. 

```{r warning = FALSE, message = FALSE}
library(rcrossref)
library(dplyr)
library(rex)
logs <- "~/pubpatterns_api_calls.log"
```

Use the awesome [rex][] package from Kevin Ushey et al. to parse the logs, pulling out
just the Crossref member ID in the API request, as well as the HTTP status code. There
are of course other API requests in the logs, but we're only interested here in the ones
to the `/api/doi/{doi}/` route.

```{r}
df <- tbl_df(scan(logs, what = "character", sep = "\n") %>%
  re_matches(
    rex(
      "/api/members/",
        capture(name = "route",
          one_or_more(numbers)
        ),
      "/",

      space, space, "HTTP/", numbers, ".", numbers, space,

      capture(name = "status_code",
        one_or_more(numbers)
      )
    )
  ))
df$route <- as.numeric(df$route)
df
```

Filter to those requests that resulted in a 400 HTTP status code, that is, they 
resulted in no returned data, indicating that we likely do not have a mapping for that 
Crossref member.


```{r}
res <- df %>%
  filter(status_code == 400) %>%
  select(route) %>% 
  group_by(route) %>%
  summarize(count = n()) %>% 
  arrange(desc(count))
res
```

Add crossref metadata

```{r cache=TRUE}
(members <- cr_members(res$route))
```

Add Crossref member names to the log data.

```{r}
alldat <- left_join(res, select(members$data, id, primary_name),
  by = c("route" = "id"))
alldat
```

Theres **A LOT** of requests to the American Medical Association. Coming in
a distant second is FapUNIFESP (SciELO), then the Oxford University Press,
Ovid Technologies (Wolters Kluwer Health), BMJ, and AME Publishing Company, 
all with greater than 1000 requests.

These are some clear leads for publishers to work into the ftdoi API, working
my way down the data.frame to less frequently requested publishers.


## more work to do

I've got a good list of publishers which I know users want URLs for, so 
I can get started implementing rules/etc. for those publishers. And I can 
repeat this process from time to time to add more publishers in high demand.



[fulltext]: https://github.com/ropensci/fulltext/
[ftdoi]: https://ftdoi.org/
[rex]: https://github.com/kevinushey/rex/
