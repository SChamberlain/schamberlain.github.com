tt <- getURL(url2)
out <- xmlParse(tt)
out
#' Get full text eLife papers via XML.
#'
#' @import XML
#' @param doi DOI to get full text for.
#' @examples \dontrun{
#' elife_paper(doi="10.7554/eLife.00160")
#' }
#' @export
elife_paper <- function(doi = NULL)
{
url <- "http://elife.elifesciences.org/elife-source-xml/"
url2 <- paste0(url, doi)
tt <- getURL(url2)
out <- xmlParse(tt)
out
}
elife_paper(doi="10.7554/eLife.00160")
out
xpathApply(out, "//abstract")
xpathSApply(out, "//abstract")
getNodeSet(out, "//abstract")
abstracts <- getNodeSet(out, "//abstract")
xpathApply(abstracts, "//p hwp:id")
getNodeSet(out, "//abstract", xmlValue)
getNodeSet(out, "//abstract/p hwp:id")
getNodeSet(out, "//abstract/p")
getNodeSet(out, "//abstract/p", xmlValue)
abstracts <- getNodeSet(out, "//abstract/p")
xpathApply(abstracts, "//p")
out
out
getNamespace(out)
getNamespaceName(out)
abstracts <- getNodeSet(out, "//abstract/p", namespaces="http://schema.highwire.org/Publishing")
abstracts <- getNodeSet(out, "//abstract/p", namespaces="org.highwire.hpp")
abstracts <- getNodeSet(out, "//abstract/p", namespaces="http://www.w3.org/2001/XMLSchema-instance")
abstracts <- getNodeSet(out, "//abstract/p", namespaces="http://schema.highwire.org/Journal")
library(XML)
help(package="XML")
getDefaultNamespace(out)
xmlNamespace(out)
xmlNamespace(abstracts)
abstracts[[1]]
xmlNamespace(abstracts[[1]])
out
xpathApply(abstracts, namespaces="p hwp")
getNodeSet(out, "//abstract")
getNodeSet(out, "//abstract", "p hwp")
getNodeSet(out, "//abstract", "p")
getNodeSet(out, "//abstract", "hwp")
abstracts <-
getNodeSet(out, "//abstract", "hwp")
sapply(abstracts, xmlValue)
abstracts <-
getNodeSet(out, "//abstract/p")
abstracts
getNodeSet(out, "//abstract/p", xmlValue)
getNodeSet(out, "//abstract/p", fun=xmlValue)
abstracts <- getNodeSet(out, "//abstract/p", fun=xmlValue)
grep(abstracts, "DOI")
grep("DOI", abstracts)
abstracts[grep("DOI", abstracts)]
abstracts[!grep("DOI", abstracts)]
abstracts[-grep("DOI", abstracts)]
paste0(abstracts[-grep("DOI", abstracts)])
paste0(abstracts[-grep("DOI", abstracts)], collapse=" ")
out
getNodeSet(out, "//body")
xpathApply(getNodeSet(out, "//body"), "//title='Introduction'")
getNodeSet(out, "//body[title='Introduction']")
getNodeSet(out, "//body//[title='Introduction']")
getNodeSet(out, "//body/[title='Introduction']")
out
getNodeSet(out, "//body")
getNodeSet(out, "//sec")
getNodeSet(out, "//sec-type")
getNodeSet(out, "//sec sec-type")
getNodeSet(out, "//sec/sec-type")
xpathApply(out, "//sec-type='intro'")
xpathSApply(out, "//sec-type='intro'")
xpathSApply(out, "//sec")
xpathSApply(out, "//sec:sec-type")
xpathApply(out, "//sec:sec-type")
getNodeSet(out, "//sec:sec-type")
getNodeSet(out, "//sec[sec-type]")
getNodeSet(out, "//sec[:sec-type]")
getNodeSet(out, "//sec:sec-type")
getNodeSet(out, "//title")
getNodeSet(out, "//sec[sec-type='intro']")
xpathApply(out, "//sec[sec-type='intro']")
getNodeSet(out, "//body/sec")
getNodeSet(out, "//body/sec")
library(RCurl XML RJSONIO plyr)
req
library(sacbox)
help(package=:)
help(package="sacbox")
req(list("XML", "RJSONIO", "plyr", "RCurl", "stringr"))
sacbox::req(list("XML", "RJSONIO", "plyr", "RCurl", "stringr"))
#' Easy require/library.
#'
#' @param x Vector of quoted package names to load.
#' @examples
#' req(list("XML", "doMC", "plyr", "RCurl", "stringr"))
#' @export
req <- function(x) {
lapply(x, FUN = function(X) {
do.call("require", list(X))
})
}
req(list("XML", "RJSONIO", "plyr", "RCurl", "stringr"))
library(roxygen2)
sacbox <- "/Users/scottmac2/github/sac/sacbox"
roxygenise(sacbox)
doi=c("10.7554/eLife.00160","10.7554/eLife.00248")
dois=c("10.7554/eLife.00160","10.7554/eLife.00248")
url <- "https://fluiddb.fluidinfo.com/values"
query <- paste0("elifesciences.org/api_v1/article/doi=", '"', doi, '"')
query
dois
foo <- function(doi){
query <- paste0("elifesciences.org/api_v1/article/doi=", '"', doi, '"')
args <- compact(list(query = query, tag = "*"))
tt <- getForm(url, .params=args)
fromJSON(tt)
}
llply(dois, foo)
terms="cell biology"
paste(terms, collapse="+")
paste(terms, sep="+")
paste(terms, sep="", collapse="+")
paste0(terms, collapse="+")
paste0(terms, collapse="\\+")
paste0(terms, collapse="\+")
paste0(terms, sep="+")
gsub("\\s", "+", terms)
searchin="subject_area"
paste0("elifesciences.org/api_v1/article/", searchin, '"', gsub("\\s", "+", terms), '"')
if(give=="doi"){give2 <- "elifesciences.org/api_v1/article/doi"} else{NULL}
give = "doi"
if(give=="doi"){give2 <- "elifesciences.org/api_v1/article/doi"} else{NULL}
give2
compact(list(query = query, tag = give2))
query <- paste0("elifesciences.org/api_v1/article/", searchin, '"', gsub("\\s", "+", terms), '"')
query
args <- compact(list(query = query, tag = give2))
args
url <- "https://fluiddb.fluidinfo.com/objects"
tt <- getForm(url, .params=args)
searchin
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains", '"', gsub("\\s", "+", terms), '"')
query
args <- compact(list(query = query, tag = give2))
tt <- getForm(url, .params=args)
args
terms="Cell biology"
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains", '"', gsub("\\s", "+", terms), '"')
args <- compact(list(query = query, tag = give2))
tt <- getForm(url, .params=args)
fromJSON(getURL(
'https://fluiddb.fluidinfo.com/objects?query=elifesciences.org/api_v1/article/subject_area+contains+"Cell+biology"&tag=elifesciences.org/api_v1/article/doi'
))
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains+", '"', gsub("\\s", "+", terms), '"')
args <- compact(list(query = query, tag = give2))
tt <- getForm(url, .params=args)
args
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains+", gsub("\\s", "+", terms))
query
if(give=="doi"){give2 <- "elifesciences.org/api_v1/article/doi"} else{NULL}
args <- compact(list(query = query, tag = give2))
tt <- getForm(url, .params=args)
args
paste0(url, "?query=", query, "&tag=", tag)
tag = give2
tag
paste0(url, "?query=", query, "&tag=", tag)
getURL(paste0(url, "?query=", query, "&tag=", tag))
getURL("https://fluiddb.fluidinfo.com/objects?query=elifesciences.org/api_v1/article/subject_area+contains+'Cell+biology'&tag=elifesciences.org/api_v1/article/doi")
getURL('https://fluiddb.fluidinfo.com/objects?query=elifesciences.org/api_v1/article/subject_area+contains+"Cell+biology"&tag=elifesciences.org/api_v1/article/doi')
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains+", '"', gsub("\\s", "+", terms), '"')
query
dois="10.7554/eLife.00160"
url <- "https://fluiddb.fluidinfo.com/values"
query <- paste0("elifesciences.org/api_v1/article/doi=", '"', doi, '"')
query
doi<-dois
doi
query <- paste0("elifesciences.org/api_v1/article/doi=", '"', doi, '"')
query
args <- compact(list(query = query, tag = "*"))
args
tt <- getForm(url, .params=args)
tt
fromJSON(tt)
url <- "https://fluiddb.fluidinfo.com/objects"
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains+", '"', gsub("\\s", "+", terms), '"')
query
paste0(url, "?query=", query, "&tag=", tag)
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
getURL(fullurl)
if(give=="doi"){tag <- "elifesciences.org/api_v1/article/doi"} else{NULL}
tag
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
getURL(fullurl)
fromJSON(getURL(fullurl))
getURL(fullurl)
url <- "https://fluiddb.fluidinfo.com/values"
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
fromJSON(getURL(fullurl))
out <- fromJSON(getURL(fullurl))
llply(out, "[[")
llply(out, "[")
llply(out[[1]], )
out$results
out$results$id
out$results$id[[1]]
llply(out$results$id, "value")
llply(out$results$id, "//value")
llply(out$results$id, function(x) x$value)
out$results$id[["value"]]
out$results$id["value"]
out$results$id[[1]]
out$results$id[[1]]["value"]
out$results$id[[1]][["value"]]
out$results$id[[1]][,4]
llply(out$results$id, function(x) x[[1]]$value)
llply(out$results$id, function(x) x[[1]])
llply(out$results$id, function(x) x[[1]][,"value"])
llply(out$results$id, function(x) x[[1]][,4])
llply(out$results$id, function(x) x[[1]][,4])
llply(out$results$id, function(x) x[[1]][[1]][,4])
llply(out$results$id, function(x) x[,4])
llply(out$results$id, function(x) x)
llply(out$results$id, function(x) x[[1]])
llply(out$results$id, function(x) x[[1]][[1]])
llply(out$results$id, function(x) x[[1]][[4]])
llply(out$results$id, function(x) x[[1]][[3]])
llply(out$results$id, function(x) x[[1]]["value"])
llply(out$results$id, function(x) x[[1]][["value"]])
laply(out$results$id, function(x) x[[1]][["value"]])
#' Make an eLife API call.
#'
#' @import RCurl XML RJSONIO plyr
#' @param terms Search terms.
#' @param
#' @examples \dontrun{
#' searchelife(terms="Cell biology", searchin="subject_area")
#' }
#' @export
searchelife <- function(terms, searchin = NULL, give = "doi")
{
url <- "https://fluiddb.fluidinfo.com/values"
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+contains+", '"', gsub("\\s", "+", terms), '"')
if(give=="doi"){tag <- "elifesciences.org/api_v1/article/doi"} else{NULL}
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
out <- fromJSON(getURL(fullurl))
laply(out$results$id, function(x) x[[1]][["value"]])
# 	args <- compact(list(query = query, tag = give2))
# 	tt <- getForm(url, .params=args)
}
# fromJSON(getURL("https://fluiddb.fluidinfo.com/values?query=has%20elifesciences.org/api_v1/article/accepted_date_month"))
# fromJSON(getURL(
# 	'https://fluiddb.fluidinfo.com/objects?query=elifesciences.org/api_v1/article/subject_area+contains+"Cell+biology"&tag=elifesciences.org/api_v1/article/doi'
# 	))
# fromJSON(
# 	getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/doi="10.7554/eLife.00013"&tag=*')
# 	)
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/component/article_doi="10.7554/eLife.00013"&tag=*'))
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/component/type="fig"&tag=elifesciences.org/api_v1/component/doi_url'))
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/author=""&tag=*'))
searchelife(terms="Cell biology", searchin="subject_area")
searchelife(terms="Cell biology", searchin="abstract")
searchelife(terms="drosophila", searchin="research_organism")
#' searchelife(terms="*", searchin="research_organism")
searchelife(terms="*", searchin="research_organism")
searchelife <- function(terms, searchin = NULL, boolean, give = "doi")
{
url <- "https://fluiddb.fluidinfo.com/values"
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+", boolean, "+", '"', gsub("\\s", "+", terms), '"')
if(give=="doi"){tag <- "elifesciences.org/api_v1/article/doi"} else{NULL}
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
out <- fromJSON(getURL(fullurl))
laply(out$results$id, function(x) x[[1]][["value"]])
# 	args <- compact(list(query = query, tag = give2))
# 	tt <- getForm(url, .params=args)
}
# fromJSON(getURL("https://fluiddb.fluidinfo.com/values?query=has%20elifesciences.org/api_v1/article/accepted_date_month"))
# fromJSON(getURL(
# 	'https://fluiddb.fluidinfo.com/objects?query=elifesciences.org/api_v1/article/subject_area+contains+"Cell+biology"&tag=elifesciences.org/api_v1/article/doi'
# 	))
# fromJSON(
# 	getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/doi="10.7554/eLife.00013"&tag=*')
# 	)
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/component/article_doi="10.7554/eLife.00013"&tag=*'))
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/component/type="fig"&tag=elifesciences.org/api_v1/component/doi_url'))
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/author=""&tag=*'))
searchelife(terms="Cell biology", searchin="abstract", "contains")
searchelife(terms="Cell biology", searchin="abstract", boolean="contains")
searchelife(terms="Cell biology", searchin="subject_area", boolean="contains")
searchelife(terms="hormone", searchin="article_title", boolean="matches")
searchelife(terms="hormone", searchin="abstract", boolean="matches")
query
terms="hormone"
term2="or"
searchin=c("article_title","abstract")
boolean="matches"
searchin
getquery <- function(x) paste0("elifesciences.org/api_v1/article/", searchin, "+", boolean, "+", '"', gsub("\\s", "+", terms), '"')
getquery
getquery <- function(x) paste0("elifesciences.org/api_v1/article/", x, "+", boolean, "+", '"', gsub("\\s", "+", terms), '"')
getquery(searchin[[1]])
getquery(searchin[[2]])
laply(searchin, getquery)
paste0(laply(searchin, getquery), "")
paste(laply(searchin, getquery), "")
paste(laply(searchin, getquery))
paste(laply(searchin, getquery), sep="")
paste(laply(searchin, getquery), sep="", collapse="")
paste(laply(searchin, getquery), sep="", collapse="+", term2, "+")
paste0("+", term2, "+")
paste(laply(searchin, getquery), sep="", collapse=paste0("+", term2, "+"))
query <- paste(laply(searchin, getquery), sep="", collapse=paste0("+", term2, "+"))
query
if(give=="doi"){tag <- "elifesciences.org/api_v1/article/doi"} else{NULL}
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
fullurl
out <- fromJSON(getURL(fullurl))
out
laply(out$results$id, function(x) x[[1]][["value"]])
#' Make an eLife API call.
#'
#' @import RCurl XML RJSONIO plyr
#' @param terms Search terms.
#' @param
#' @examples \dontrun{
#' # Simpler queries
#' searchelife(terms="Cell biology", searchin="subject_area", boolean="contains")
#' searchelife(terms="hormone", searchin="article_title", boolean="matches")
#' searchelife(terms="hormone", searchin="abstract", boolean="matches")
#' searchelife(terms="hormone", searchin="article_title", boolean="matches")
#'
#' # more complicated queries
#' searchelife(terms="hormone", term2="or", searchin=c("article_title","abstract"), boolean="matches")
#' }
#' @export
searchelife <- function(terms, term2 = "or", searchin = NULL, boolean, give = "doi")
{
url <- "https://fluiddb.fluidinfo.com/values"
if(length(searchin)==1){
query <- paste0("elifesciences.org/api_v1/article/", searchin, "+", boolean, "+", '"', gsub("\\s", "+", terms), '"')
if(give=="doi"){tag <- "elifesciences.org/api_v1/article/doi"} else{NULL}
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
out <- fromJSON(getURL(fullurl))
laply(out$results$id, function(x) x[[1]][["value"]])
} else
{
getquery <- function(x) paste0("elifesciences.org/api_v1/article/", x, "+", boolean, "+", '"', gsub("\\s", "+", terms), '"')
query <- paste(laply(searchin, getquery), sep="", collapse=paste0("+", term2, "+"))
if(give=="doi"){tag <- "elifesciences.org/api_v1/article/doi"} else{NULL}
fullurl <- paste0(url, "?query=", query, "&tag=", tag)
out <- fromJSON(getURL(fullurl))
laply(out$results$id, function(x) x[[1]][["value"]])
}
}
# fromJSON(getURL("https://fluiddb.fluidinfo.com/values?query=has%20elifesciences.org/api_v1/article/accepted_date_month"))
# fromJSON(getURL(
# 	'https://fluiddb.fluidinfo.com/objects?query=elifesciences.org/api_v1/article/subject_area+contains+"Cell+biology"&tag=elifesciences.org/api_v1/article/doi'
# 	))
# fromJSON(
# 	getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/doi="10.7554/eLife.00013"&tag=*')
# 	)
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/component/article_doi="10.7554/eLife.00013"&tag=*'))
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/component/type="fig"&tag=elifesciences.org/api_v1/component/doi_url'))
# fromJSON(getURL('https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/author=""&tag=*'))
https://fluiddb.fluidinfo.com/values?query=elifesciences.org/api_v1/article/article_title+matches+%22hormone%22+or+elifesciences.org/api_v1/article/abstract+matches+%22little%22&tag=elifesciences.org/api_v1/article/doi_url
searchelife(terms="Cell biology", searchin="subject_area", boolean="contains")
searchelife(terms="hormone", searchin="article_title", boolean="matches")
searchelife(terms="hormone", term2="or", searchin=c("article_title","abstract"), boolean="matches")
searchelife(terms="hormone", term2="and", searchin=c("article_title","abstract"), boolean="matches")
elife <- "/Users/scottmac2/github/sac/elife"
elife
elife <- "/Users/scottmac2/github/ropensci/elife"
library(roxygen2)
roxygenise(elife)
roxygenise(elife)
roxygenise(elife)
roxygenise(elife)
setwd("/Users/scottmac2/github/sac/scott/_posts")
knitpost("/Users/scottmac2/github/sac/scott/_drafts/2013-01-25-where-to-publish.Rmd")
knitpost("/Users/scottmac2/github/sac/scott/_drafts/2013-01-25-where-to-publish.Rmd")
alm(doi="10.1371/journal.pone.0029797")
library(rplos)
install_github("rplos", "ropensci", ref="almv3")
library(rplos)
alm(doi="10.1371/journal.pone.0029797")
dois <- searchplos(terms='evolution', fields='id', limit = 3)
out <- alm(doi=as.character(dois[,1]))
lapply(out, head)
out <- alm(doi='10.1371/journal.pone.0001543', info='detail')
almplot(out, type='totalmetrics') # just totalmetrics data
almplot(dat=out, type='history') # just historical data
almplot(dat=out, type='history') # just historical data
almtitle(doi='10.1371/journal.pbio.0000012')
out <- almevents(doi="10.1371/journal.pone.0029797")
out[["pmc"]] # get the results for PubMed Central
out[["twitter"]] # get the results for twitter (boo, there aren't any)
out[c("scienceseeker","crossref")] # get the results for two sources
out[c("pmc","crossref")] # get the results for two sources
out[c("scienceseeker","pmc")] # get the results for two sources
crossref(doi="10.1371/journal.pone.0042793")
print(crossref("10.3998/3336451.0009.101"), style="Bibtex")
searchplos(terms='ecology', fields='id', limit = 1200)
searchplos('ecology', 'id,publication_date', limit = 2)
searchplos('ecology', 'id,title', limit = 2)
searchplos(terms="*:*", 'id', toquery='doc_type:full', start=0, limit=250) #
searchplos(terms="*:*", 'id', toquery=list('cross_published_journal_key:PLoSONE',year=2010), start=0, limit=50) # query a specific journal
head(searchplos(terms="*:*", 'id', toquery=list('cross_published_journal_key:PLoSONE',year=2010), start=0, limit=50)) # query a specific journal
searchplos('ecology', 'id,publication_date', limit = 2) # specifiy fields to return in the output
head(searchplos(terms='ecology', fields='id', limit = 600)) # you can search alot of articles
plostitle(terms='drosophila', fields='title', limit=10) # and return the titles only
plostitle(terms='drosophila', fields='journal', limit=10) # return just journal name
plot_throughtime(list('drosophila','monkey'), 100)
plosword(list('monkey','Helianthus','sunflower','protein','whale'), vis = 'TRUE')
setwd("~/")
knit2html("/Users/scottmac2/github/rplos_staticdocs.Rmd")
# Get total metrics across dates
alm(doi="10.1371/journal.pone.0029797")
library(rplos)
# Get total metrics across dates
alm(doi="10.1371/journal.pone.0029797")
install_github("rplos", "ropensci", ref="almv3")
library(rplos)
alm(doi="10.1371/journal.pone.0029797")
# Search for DOI's, then feed into alm
dois <- searchplos(terms='evolution', fields='id', limit = 3)
out <- alm(doi=as.character(dois[,1]))
lapply(out, head)
out <- alm(doi='10.1371/journal.pone.0001543', info='detail')
almplot(out, type='totalmetrics') # just totalmetrics data
almplot(dat=out, type='history') # just historical data
out <- almevents(doi="10.1371/journal.pone.0029797")
out[["pmc"]] # get the results for PubMed Central
out[["twitter"]] # get the results for twitter (boo, there aren't any)
out[c("scienceseeker","pmc")] # get the results for two sources
crossref(doi="10.1371/journal.pone.0042793")
print(crossref("10.3998/3336451.0009.101"), style="Bibtex")
head(searchplos(terms='ecology', fields='id', limit = 600)) # you can search alot of articles
searchplos('ecology', 'id,publication_date', limit = 2) # specifiy fields to return in the output
head(searchplos(terms="*:*", 'id', toquery=list('cross_published_journal_key:PLoSONE',year=2010), start=0, limit=50)) # query a specific journal
plostitle(terms='drosophila', fields='title', limit=10) # and return the titles only
plostitle(terms='drosophila', fields='journal', limit=10) # return just journal name
plot_throughtime(list('drosophila','monkey'), 100)
plosword(list('monkey','Helianthus','sunflower','protein','whale'), vis = 'TRUE')
library(taxize)
ubio_namebank(searchName = 'elephant', sci = 1, vern = 0)
gni_search(search_term = "ani*")
gni_search(search_term = "*")
gni_search(search_term = "**")
gni_search(search_term = "a")
gni_search(search_term = "a", per_page=2)
gni_search(search_term = "a*", per_page=2, page=1)
gni_search(search_term = "a*", per_page=2)
gni_search(search_term = "a", per_page=2)
gni_search(search_term = "a", per_page=3)
tpl_get(dir_ = "~/foo2", family = "Scrophulariaceae")
dat <- read.csv("~/Scrophulariaceae.csv")
dat <- read.csv("~/foo2/Scrophulariaceae.csv")
head(dat)
str(dat)
dat[,c("Genus","Species")]
paste(dat[,c("Genus","Species")],sep=" ")
ddply(dat[,c("Genus","Species")], .(), transform, gen_sp = as.factor(paste(Genus,Species,sep=" ")))[,-1]
ddply(dat[,c("Genus","Species")], .(), transform, gen_sp = as.factor(paste(Genus,Species,sep=" ")))[,4]
as.character(ddply(dat[,c("Genus","Species")], .(), transform, gen_sp = as.factor(paste(Genus,Species,sep=" ")))[,4])
species <- as.character(ddply(dat[,c("Genus","Species")], .(), transform, gen_sp = as.factor(paste(Genus,Species,sep=" ")))[,4])
setwd("/Users/scottmac2/github/sac/schamberlain.github.com/_posts")
knitpost("/Users/scottmac2/github/sac/schamberlain.github.com/_drafts/2013-01-25-tnrs-use-case.Rmd")
knitpost("/Users/scottmac2/github/sac/schamberlain.github.com/_drafts/2013-01-25-tnrs-use-case.Rmd")
species
tpl_get(dir_ = "~/foo2", family = "Scrophulariaceae")
dat <- read.csv("~/foo2/Scrophulariaceae.csv")
dat
species <- as.character(ddply(dat[,c("Genus","Species")], .(), transform, gen_sp = as.factor(paste(Genus,Species,sep=" ")))[,4])
species
library(taxize)
slice <- function(input, by=2){
starts <- seq(1,length(input),by)
tt <- lapply(starts, function(y) input[y:(y+(by-1))])
laply(tt, function(x) x[!is.na(x)])
}
species_split <- slice(species, by=100)
species_split
slice(species, by=100)
input <- species
by=100
starts <- seq(1,length(input),by)
starts
lapply(starts, function(y) input[y:(y+(by-1))])
tt <- lapply(starts, function(y) input[y:(y+(by-1))])
laply(tt, function(x) x[!is.na(x)])
llply(tt, function(x) x[!is.na(x)])
species_split <- slice(species, by=100)
slice <- function(input, by=2){
starts <- seq(1,length(input),by)
tt <- lapply(starts, function(y) input[y:(y+(by-1))])
llply(tt, function(x) x[!is.na(x)])
}
species_split <- slice(species, by=100)
knitpost("/Users/scottmac2/github/sac/schamberlain.github.com/_drafts/2013-01-25-tnrs-use-case.Rmd")
